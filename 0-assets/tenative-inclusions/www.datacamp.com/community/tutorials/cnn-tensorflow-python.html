<!DOCTYPE html>
<html>
  <!-- Mirrored from www.datacamp.com/community/tutorials/cnn-tensorflow-python by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 18 Feb 2021 21:27:23 GMT -->
  <!-- Added by HTTrack -->
  <meta
    http-equiv="content-type"
    content="text/html;charset=utf-8"
  /><!-- /Added by HTTrack -->

  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, user-scalable=no"
    />
    <meta
      name="google-site-verification"
      content="NXbTi7gyLQESV4NIskeE9Ka0Am8KjAtzg5gm8g38HbU"
    />
    <meta name="keywords" content="python, tensorflow, cnn, neural network" />
    <meta
      name="description"
      content="In this tutorial, you&#x27;ll learn how to construct and implement Convolutional Neural Networks (CNNs) in Python with the TensorFlow framework."
    />
    <title>
      (Tutorial) Convolutional Neural Networks with TensorFlow - DataCamp
    </title>
    <link rel="canonical" href="cnn-tensorflow-python.html" />
    <link rel="author" href="https://plus.google.com/u/0/+Datacamp/" />
    <link
      rel="shortcut icon"
      type="image/x-icon"
      href="https://learn.datacamp.com/favicon.ico"
    />
    <link
      rel="chrome-webstore-item"
      href="https://chrome.google.com/webstore/detail/lbbhbkehmgbndgfdbncbmikooblghdbi"
    />
    <meta
      property="og:title"
      content="(Tutorial) Convolutional Neural Networks with TensorFlow"
    />
    <meta
      property="og:image"
      content="https://cdn.datacamp.com/community/logos/medium/placeholder.jpg"
    />
    <meta property="og:url" content="cnn-tensorflow-python.html" />
    <meta property="og:type" content="article" />
    <meta property="og:published_time" content="2020-06-08T16:00:00.000Z" />
    <meta property="og:author" content="Aditya Sharma" />
    <meta
      property="og:description"
      content="In this tutorial, you&#x27;ll learn how to construct and implement Convolutional Neural Networks (CNNs) in Python with the TensorFlow framework."
    />
    <meta property="og:site_name" content="DataCamp Community" />
    <meta
      name="twitter:title"
      content="(Tutorial) Convolutional Neural Networks with TensorFlow"
    />
    <meta
      name="twitter:description"
      content="In this tutorial, you&#x27;ll learn how to construct and implement Convolutional Neural Networks (CNNs) in Python with the TensorFlow framework."
    />
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="@DataCamp" />
    <meta name="twitter:creator" content="@DataCamp" />
    <meta name="twitter:domain" content="www.datacamp.com" />
    <meta
      name="twitter:image"
      content="https://cdn.datacamp.com/community/logos/medium/placeholder.jpg"
    />
    <meta name="twitter:image:width" content="1200" />
    <meta name="twitter:image:height" content="628" />
    <meta
      name="twitter:image:alt"
      content="(Tutorial) Convolutional Neural Networks with TensorFlow"
    />
    <meta
      name="article:publisher"
      content="https://www.facebook.com/DataCamp-726282547396228"
    />
    <meta name="fb:app_id" content="726282547396228" />
    <script async="" src="https://promo.datacamp.com/banner.js"></script>
    <script async="" src="https://compliance.datacamp.com/base.js"></script>
    <meta name="next-head-count" content="33" />
    <link
      rel="preload"
      href="../_next/static/chunks/main-f0b5624c3014889b3762.js"
      as="script"
    />
    <link
      rel="preload"
      href="../_next/static/chunks/webpack-8da9cabe58c6d2aa3bdd.js"
      as="script"
    />
    <link
      rel="preload"
      href="../_next/static/chunks/framework.ad7002b3a551c278f5f0.js"
      as="script"
    />
    <link
      rel="preload"
      href="../_next/static/chunks/commons.aeba1b501ee5096455a7.js"
      as="script"
    />
    <link
      rel="preload"
      href="../_next/static/chunks/pages/_app-87c1170f38154f9c4d0d.js"
      as="script"
    />
    <link
      rel="preload"
      href="../_next/static/chunks/ac46388c.70f445eb5557aa9dec72.js"
      as="script"
    />
    <link
      rel="preload"
      href="../_next/static/chunks/68f504b0.bba56c4f391a1db0815a.js"
      as="script"
    />
    <link
      rel="preload"
      href="../_next/static/chunks/78537f27.8279c7505c61cb8fea60.js"
      as="script"
    />
    <link
      rel="preload"
      href="../_next/static/chunks/adf1b103b565b7865a0f98ea967b51b3e3660c18.095266edfcdd9f646efd.js"
      as="script"
    />
    <link
      rel="preload"
      href="../_next/static/chunks/8ffc5858bf71a15f52361e7319c3ad5ceec7c325.576033b2f3878dbe8c10.js"
      as="script"
    />
    <link
      rel="preload"
      href="../_next/static/chunks/1ea862844a52e07ea30f2afd84cc366b3979d6b4.e27afa73a93d521c36e1.js"
      as="script"
    />
    <link
      rel="preload"
      href="../_next/static/chunks/de2bd6607f0d09174f6b5f6ccd214f4f3c8d6fb9.e2051ed48ad4791c2328.js"
      as="script"
    />
    <link
      rel="preload"
      href="../_next/static/chunks/0f6d78b899a1ada8a069577d0a0f2d60f5d91117.4019acfb783e806a75cb.js"
      as="script"
    />
    <link
      rel="preload"
      href="../_next/static/chunks/pages/community/tutorial-ab49573adb58f7e29045.js"
      as="script"
    />
    <style id="__jsx-3673784144">
      .logoContainer.jsx-3673784144 {
        padding-left: 10px;
      }

      .SidebarMenu.jsx-3673784144 {
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-flex-direction: row;
        -ms-flex-direction: row;
        flex-direction: row;
        -webkit-box-pack: justify;
        -webkit-justify-content: space-between;
        -ms-flex-pack: justify;
        justify-content: space-between;
        -webkit-align-items: center;
        -webkit-box-align: center;
        -ms-flex-align: center;
        align-items: center;
        position: fixed;
        z-index: 300;
        width: 100vw;
        height: 50px;
        background-color: #05192d;
      }

      .Layout--banner .SidebarMenu.jsx-3673784144 {
        top: 55px;
      }

      .icon.jsx-3673784144 {
        text-align: right;
      }

      .icon.jsx-3673784144 svg {
        margin-right: 9px;
        width: 20px;
        height: 20px;
        fill: #ffffff;
      }

      @media (min-width: 800px) {
        .SidebarMenu.jsx-3673784144 {
          z-index: 200;
          width: 220px;
          -webkit-align-items: stretch;
          -webkit-box-align: stretch;
          -ms-flex-align: stretch;
          align-items: stretch;
          top: 0;
          left: 0;
          background-image: none;
        }

        .logoContainer.jsx-3673784144 {
          padding-left: 20px;
          padding-top: 16px;
        }

        .Layout--banner .SidebarMenu.jsx-3673784144 {
          top: 80px;
        }
      }
    </style>
    <style id="__jsx-2576226853">
      .Menu.jsx-2576226853 {
        -webkit-flex: 1 1 auto;
        -ms-flex: 1 1 auto;
        flex: 1 1 auto;
        padding-top: 13px;
        background-color: #05192d;
      }

      .Layout--banner .Menu.jsx-2576226853 {
        margin-top: 105px;
      }

      .Layout--openMenu .Menu.jsx-2576226853 {
        min-height: calc(100vh - 50px - 134px);
      }

      .Layout--openMenu.Layout--banner .Menu.jsx-2576226853 {
        min-height: calc(100vh - 105px - 134px);
      }

      .section.jsx-2576226853 {
        margin-bottom: 20px;
      }

      .section.jsx-2576226853 h5.jsx-2576226853 {
        margin: 0;
        font-size: 13px;
        line-height: 36px;
        text-align: left;
        text-transform: uppercase;
        padding-left: 16px;
        background-color: #05192d;
      }

      .section.jsx-2576226853 h5.jsx-2576226853 .title.jsx-2576226853 {
        opacity: 0.7;
        -webkit-letter-spacing: 3px;
        -moz-letter-spacing: 3px;
        -ms-letter-spacing: 3px;
        letter-spacing: 3px;
        color: #ffffff;
      }

      .item.jsx-2576226853 {
        margin-bottom: 1px;
        padding-left: 12px;
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-flex-direction: row;
        -ms-flex-direction: row;
        flex-direction: row;
        -webkit-align-content: center;
        -ms-flex-line-pack: center;
        align-content: center;
        font-size: 15px;
        -webkit-letter-spacing: 0.2px;
        -moz-letter-spacing: 0.2px;
        -ms-letter-spacing: 0.2px;
        letter-spacing: 0.2px;
        line-height: 40px;
        -webkit-text-decoration: none;
        text-decoration: none;
        color: #ffffff;
      }

      .statusIcon.jsx-2576226853 {
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-align-self: center;
        -ms-flex-item-align: center;
        align-self: center;
        padding: 10px;
      }

      .active.jsx-2576226853 {
        background-color: #213147;
      }

      a.jsx-2576226853:hover {
        background-color: #213147;
      }

      .image.jsx-2576226853 {
        margin-top: 2px;
        margin-right: 3px;
        -webkit-flex: 0 0 auto;
        -ms-flex: 0 0 auto;
        flex: 0 0 auto;
        width: 30px;
        height: 30px;
        text-align: center;
      }

      .image.jsx-2576226853 svg {
        color: #ffffff;
        fill: #ffffff;
      }

      .active.jsx-2576226853 svg,
      a.jsx-2576226853:hover svg {
        color: #03ef62;
        fill: #03ef62;
      }

      .text.jsx-2576226853 {
        -webkit-flex: 1 1 auto;
        -ms-flex: 1 1 auto;
        flex: 1 1 auto;
      }

      .subMenu.jsx-2576226853 {
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
      }

      @media (min-width: 800px) {
        .Menu.jsx-2576226853 {
          position: fixed;
          width: 220px;
          margin-top: 50px;
        }

        .Layout--banner .Menu.jsx-2576226853 {
          margin-top: 130px;
        }

        .section.jsx-2576226853 h5.jsx-2576226853 {
          padding-left: 16;
        }
      }
    </style>
    <style id="__jsx-1169100422">
      .Button.jsx-1169100422 {
        display: -webkit-inline-box;
        display: -webkit-inline-flex;
        display: -ms-inline-flexbox;
        display: inline-flex;
        -webkit-box-pack: center;
        -webkit-justify-content: center;
        -ms-flex-pack: center;
        justify-content: center;
        -webkit-align-items: center;
        -webkit-box-align: center;
        -ms-flex-align: center;
        align-items: center;
        -webkit-flex: 0 0 auto;
        -ms-flex: 0 0 auto;
        flex: 0 0 auto;
        height: 33px;
        margin: auto 5px;
        padding: 0 15px;
        font-size: 13px;
        font-weight: bold;
        -webkit-letter-spacing: 0.2px;
        -moz-letter-spacing: 0.2px;
        -ms-letter-spacing: 0.2px;
        letter-spacing: 0.2px;
        white-space: nowrap;
        color: #3a3a3a;
        border: 1px solid transparent;
        border-radius: 4px;
        background-color: transparent;
        cursor: pointer;
        outline: none;
      }

      .Button.jsx-1169100422::before,
      .Button.jsx-1169100422::after {
        content: "";
        -webkit-flex: 1 0 auto;
        -ms-flex: 1 0 auto;
        flex: 1 0 auto;
      }

      .icon.jsx-1169100422 {
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        min-height: 20px;
      }

      .icon.jsx-1169100422 svg {
        -webkit-flex: 1 1 0;
        -ms-flex: 1 1 0;
        flex: 1 1 0;
        height: 20px;
        -webkit-align-self: center;
        -ms-flex-item-align: center;
        align-self: center;
        fill: #06bdfc;
      }

      .greyIcon.jsx-1169100422 .icon.jsx-1169100422 svg {
        min-width: 16px;
        min-height: 16px;
        fill: #3a3a3a;
      }

      .same.jsx-1169100422 .icon.jsx-1169100422 {
        min-width: 13px;
        height: 13px;
        margin-right: 5px;
      }

      .same.jsx-1169100422 .icon.jsx-1169100422 svg {
        height: 13px;
      }

      .Button.jsx-1169100422:disabled,
      .Button.jsx-1169100422:hover.jsx-1169100422:disabled {
        color: #d1d3d8;
        background-color: #e6eaeb;
      }

      .primary.jsx-1169100422 {
        background-color: #ffc844;
      }

      .primary.jsx-1169100422:hover {
        background-color: #fbe28d;
      }

      .secondary.jsx-1169100422 {
        color: #ffffff;
        background-color: #06bdfc;
      }

      .secondary.jsx-1169100422:hover {
        background-color: #7ecce2;
      }

      .red.jsx-1169100422 {
        color: #ffffff;
        background-color: #fe5c5c;
      }

      .green.jsx-1169100422 {
        height: 35px;
        color: #ffffff;
        background-color: #ffffff;
      }

      .green.jsx-1169100422 .icon.jsx-1169100422 svg {
        fill: #36d57d;
        width: 35px;
        height: 35px;
      }

      .grey.jsx-1169100422 {
        color: #3d4251;
        background-color: #d1d3d8;
      }

      .grey.jsx-1169100422:hover {
        color: #3d4251;
        background-color: #e6eaeb;
      }

      .big.jsx-1169100422 {
        font-size: 15px;
        height: 42px;
      }

      .extra.jsx-1169100422 {
        font-size: 17px;
        height: 45px;
      }

      .border.jsx-1169100422 {
        border: 1px solid #e3e7e8;
      }

      .border.jsx-1169100422:hover {
        border: 1px solid #06bdfc;
      }

      .seeAll.jsx-1169100422 {
        border: 1px solid #06bdfc;
      }

      .seeAll.jsx-1169100422:hover {
        border: 1px solid #ffc844;
      }

      .iconButton.jsx-1169100422:hover {
        color: #06bdfc;
      }

      .minWidth.jsx-1169100422 {
        min-width: 85px;
      }

      .noPadding.jsx-1169100422 {
        padding: 0;
      }

      @media (min-width: 800px) {
        .icon.jsx-1169100422 {
          min-width: 13px;
          height: 13px;
          margin-right: 5px;
        }

        .icon.jsx-1169100422 svg {
          height: 13px;
        }

        .big.jsx-1169100422 .icon.jsx-1169100422 {
          min-width: 15px;
          height: 15px;
        }

        .big.jsx-1169100422 svg {
          height: 15px;
        }

        .extra.jsx-1169100422 .icon.jsx-1169100422,
        .extraIcon.jsx-1169100422 {
          min-width: 17px;
          height: 17px;
        }

        .extra.jsx-1169100422 svg,
        .extraIcon.jsx-1169100422 svg {
          height: 17px;
        }

        .green.jsx-1169100422 {
          padding: 0 15px;
          color: #ffffff;
          background-color: #36d57d;
        }

        .green.jsx-1169100422 .icon.jsx-1169100422 svg {
          width: 13px;
          height: 13px;
          fill: #ffffff;
        }

        .forcePadding.jsx-1169100422 {
          padding: 0 15px;
        }
      }
    </style>
    <style id="__jsx-2098455175">
      .ActionBarSearch.jsx-2098455175 {
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-flex-direction: row;
        -ms-flex-direction: row;
        flex-direction: row;
        -webkit-align-items: center;
        -webkit-box-align: center;
        -ms-flex-align: center;
        align-items: center;
        -webkit-box-pack: start;
        -webkit-justify-content: flex-start;
        -ms-flex-pack: start;
        justify-content: flex-start;
      }
    </style>
    <style id="__jsx-3698045554">
      .ActionBarAuth.jsx-3698045554 {
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-flex-direction: row;
        -ms-flex-direction: row;
        flex-direction: row;
        -webkit-align-items: center;
        -webkit-box-align: center;
        -ms-flex-align: center;
        align-items: center;
        -webkit-box-pack: end;
        -webkit-justify-content: flex-end;
        -ms-flex-pack: end;
        justify-content: flex-end;
      }

      .wrapper.jsx-3698045554 {
        display: -webkit-inline-box;
        display: -webkit-inline-flex;
        display: -ms-inline-flexbox;
        display: inline-flex;
        -webkit-flex-direction: row;
        -ms-flex-direction: row;
        flex-direction: row;
        -webkit-align-items: center;
        -webkit-box-align: center;
        -ms-flex-align: center;
        align-items: center;
        cursor: pointer;
      }

      .name.jsx-3698045554 {
        margin-right: 9px;
        font-size: 13px;
        font-weight: bold;
        color: #3d4251;
        -webkit-text-decoration: none;
        text-decoration: none;
      }

      .name.jsx-3698045554:hover {
        color: #06bdfc;
      }

      .logout.jsx-3698045554 {
        font-size: 15px;
        padding: 10px;
        color: #3d4251;
        display: inline-block;
        min-width: 100px;
      }

      .logout.jsx-3698045554:hover {
        background-color: #f0f4f5;
        border-bottom: solid 1px #e3e7e8;
      }

      .menuList.jsx-3698045554 a.jsx-3698045554 {
        display: block;
      }
    </style>
    <style id="__jsx-588888136">
      .ActionBar.jsx-588888136 {
        height: 50px;
        margin-top: 50px;
        padding: 0 5px;
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-flex-direction: row;
        -ms-flex-direction: row;
        flex-direction: row;
        -webkit-box-pack: justify;
        -webkit-justify-content: space-between;
        -ms-flex-pack: justify;
        justify-content: space-between;
        -webkit-align-items: center;
        -webkit-box-align: center;
        -ms-flex-align: center;
        align-items: center;
        background-color: #ffffff;
        border-bottom: 1px solid #e3e7e8;
      }

      .authBlock.jsx-588888136 {
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-fles-direction: row;
        -ms-fles-direction: row;
        fles-direction: row;
      }

      .Page.content .ActionBar {
        margin-bottom: 10px;
      }

      @media (min-width: 800px) {
        .ActionBar.jsx-588888136 {
          width: calc(100% - 220px);
          height: 50px;
          margin-top: 0;
          margin-bottom: 0;
          padding: 0 25px;
          position: fixed;
          z-index: 300;
        }
      }
    </style>
    <style id="__jsx-1582297868">
      .Title.jsx-1582297868 {
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-flex-direction: row;
        -ms-flex-direction: row;
        flex-direction: row;
        -webkit-box-pack: center;
        -webkit-justify-content: center;
        -ms-flex-pack: center;
        justify-content: center;
        -webkit-align-items: center;
        -webkit-box-align: center;
        -ms-flex-align: center;
        align-items: center;
      }

      .Title.jsx-1582297868 .icon.jsx-1582297868 {
        -webkit-flex: 0 0 auto;
        -ms-flex: 0 0 auto;
        flex: 0 0 auto;
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        min-width: 18px;
        height: 18px;
      }

      .icon.jsx-1582297868 svg {
        -webkit-flex: 1 1 0;
        -ms-flex: 1 1 0;
        flex: 1 1 0;
        height: 18px;
        -webkit-align-self: center;
        -ms-flex-item-align: center;
        align-self: center;
        fill: #06bdfc;
      }

      .Title.jsx-1582297868 .h1.jsx-1582297868,
      .Title.jsx-1582297868 h1.jsx-1582297868 {
        -webkit-flex: 0 0 auto;
        -ms-flex: 0 0 auto;
        flex: 0 0 auto;
        margin: auto 0 auto 9px;
        font-size: 22px;
        text-transform: capitalize;
      }

      .Title.jsx-1582297868 .status.jsx-1582297868 {
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-align-self: center;
        -ms-flex-item-align: center;
        align-self: center;
        padding: 10px;
      }
    </style>
    <style id="__jsx-1615743782">
      .TitleBar.jsx-1615743782 {
        height: 50px;
        padding: 0 5px;
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-flex-direction: row;
        -ms-flex-direction: row;
        flex-direction: row;
        -webkit-flex-wrap: wrap;
        -ms-flex-wrap: wrap;
        flex-wrap: wrap;
        -webkit-box-pack: justify;
        -webkit-justify-content: space-between;
        -ms-flex-pack: justify;
        justify-content: space-between;
        -webkit-align-items: center;
        -webkit-box-align: center;
        -ms-flex-align: center;
        align-items: center;
        line-height: 50px;
        background-color: #ffffff;
        border-bottom: 1px solid #e3e7e8;
        margin-bottom: 65px;
      }

      .filter.jsx-1615743782 {
        -webkit-flex: 1 1 auto;
        -ms-flex: 1 1 auto;
        flex: 1 1 auto;
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-align-items: center;
        -webkit-box-align: center;
        -ms-flex-align: center;
        align-items: center;
        height: 48px;
      }

      .action.jsx-1615743782 {
        -webkit-flex: 0 0 auto;
        -ms-flex: 0 0 auto;
        flex: 0 0 auto;
        line-height: normal;
      }

      .title.jsx-1615743782 {
        height: 65px;
        line-height: 65px;
        -webkit-flex: 0 0 100%;
        -ms-flex: 0 0 100%;
        flex: 0 0 100%;
        -webkit-order: 1;
        -ms-flex-order: 1;
        order: 1;
        text-align: center;
      }

      h1.jsx-1615743782 {
        margin: 0 0;
      }

      .Page.content .TitleBar {
        display: none;
      }

      @media (min-width: 800px) {
        .TitleBar.jsx-1615743782 {
          height: 50px;
          padding: 0 25px;
          display: -webkit-box !important;
          display: -webkit-flex !important;
          display: -ms-flexbox !important;
          display: flex !important;
          -webkit-flex-direction: row;
          -ms-flex-direction: row;
          flex-direction: row;
          -webkit-flex-wrap: nowrap;
          -ms-flex-wrap: nowrap;
          flex-wrap: nowrap;
          -webkit-box-pack: justify;
          -webkit-justify-content: space-between;
          -ms-flex-pack: justify;
          justify-content: space-between;
          -webkit-align-items: center;
          -webkit-box-align: center;
          -ms-flex-align: center;
          align-items: center;
          margin-bottom: 29px;
          margin-top: 50px;
          background-color: #ffffff;
          border-bottom: 1px solid #e3e7e8;
        }

        .filter.jsx-1615743782 {
          -webkit-flex: 0 0 33%;
          -ms-flex: 0 0 33%;
          flex: 0 0 33%;
          line-height: normal;
        }

        .action.jsx-1615743782 {
          display: -webkit-box;
          display: -webkit-flex;
          display: -ms-flexbox;
          display: flex;
          -webkit-box-pack: end;
          -webkit-justify-content: flex-end;
          -ms-flex-pack: end;
          justify-content: flex-end;
          line-height: normal;
          -webkit-flex: 0 0 33%;
          -ms-flex: 0 0 33%;
          flex: 0 0 33%;
        }

        .action.jsx-1615743782 a {
          line-height: 0;
        }

        .title.jsx-1615743782 {
          -webkit-order: 0;
          -ms-flex-order: 0;
          order: 0;
          -webkit-flex: 1 1 auto;
          -ms-flex: 1 1 auto;
          flex: 1 1 auto;
          text-align: center;
        }

        h1.jsx-1615743782 {
          margin: 0 0;
        }
      }
    </style>
    <style id="__jsx-765279523">
      .CommentCounter.jsx-765279523 {
        width: 54px;
        height: 54px;
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-flex-direction: column;
        -ms-flex-direction: column;
        flex-direction: column;
        -webkit-box-pack: center;
        -webkit-justify-content: center;
        -ms-flex-pack: center;
        justify-content: center;
        -webkit-align-items: center;
        -webkit-box-align: center;
        -ms-flex-align: center;
        align-items: center;
        border-radius: 4px;
        border: 1px solid #e6eaeb;
        background-color: #f0f4f5;
        cursor: pointer;
      }

      .CommentCounter.jsx-765279523 .icon.jsx-765279523 {
        font-size: 13px;
        line-height: 0;
        color: #06bdfc;
      }

      .CommentCounter.jsx-765279523 .icon.jsx-765279523 svg {
        width: 16px;
        height: 16px;
        fill: #06bdfc;
      }

      .CommentCounter.jsx-765279523 .count.jsx-765279523 {
        font-size: 13px;
        font-weight: bold;
        -webkit-letter-spacing: 0.2px;
        -moz-letter-spacing: 0.2px;
        -ms-letter-spacing: 0.2px;
        letter-spacing: 0.2px;
        color: #686f75;
      }

      .CommentCounter.jsx-765279523:hover {
        background-color: #ffffff;
      }
    </style>
    <style id="__jsx-1727309017">
      .Upvote.jsx-1727309017 {
        position: relative;
        width: 54px;
        height: 54px;
        overflow: hidden;
        border-radius: 4px;
        border: 1px solid #e6eaeb;
        background-color: #f0f4f5;
        cursor: pointer;
      }

      .Upvote.news.jsx-1727309017 {
        width: 45px;
        height: 35px;
        border: none;
        background-color: transparent;
      }

      .Upvote.comment.jsx-1727309017 {
        width: 45px;
        height: 25px;
        border: none;
        background-color: transparent;
      }

      .Upvote.jsx-1727309017 > div.jsx-1727309017 {
        position: absolute;
        top: 0;
        -webkit-transition: all 0.15s ease-in-out;
        transition: all 0.15s ease-in-out;
      }

      .Upvote.upvoted.jsx-1727309017 > div.jsx-1727309017 {
        top: -54px;
      }

      .Upvote.upvoted.news.jsx-1727309017 > div.jsx-1727309017 {
        top: -35px;
      }

      .Upvote.upvoted.comment.jsx-1727309017 > div.jsx-1727309017 {
        top: -25px;
      }

      .Upvote.jsx-1727309017 > div.jsx-1727309017 > div.jsx-1727309017 {
        display: -webkit-inline-box;
        display: -webkit-inline-flex;
        display: -ms-inline-flexbox;
        display: inline-flex;
        -webkit-flex-direction: column;
        -ms-flex-direction: column;
        flex-direction: column;
        -webkit-align-items: center;
        -webkit-box-align: center;
        -ms-flex-align: center;
        align-items: center;
        -webkit-box-pack: center;
        -webkit-justify-content: center;
        -ms-flex-pack: center;
        justify-content: center;
        width: 54px;
        height: 54px;
      }

      .Upvote.news.jsx-1727309017 > div.jsx-1727309017 > div.jsx-1727309017 {
        width: 45px;
        height: 35px;
      }

      .Upvote.comment.jsx-1727309017 > div.jsx-1727309017 > div.jsx-1727309017 {
        width: 45px;
        height: 25px;
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-flex-direction: row;
        -ms-flex-direction: row;
        flex-direction: row;
      }

      .Upvote.jsx-1727309017 .icon.jsx-1727309017 {
        font-size: 13px;
        line-height: 0;
        color: #06bdfc;
      }

      .Upvote.comment.jsx-1727309017 .icon.jsx-1727309017 {
        padding-right: 5px;
      }

      .Upvote.comment.jsx-1727309017 .count.jsx-1727309017 {
        padding-bottom: 1px;
      }

      .Upvote.jsx-1727309017 .icon.jsx-1727309017 svg {
        width: 12px;
        height: 12px;
        fill: #06bdfc;
      }

      .Upvote.jsx-1727309017 .count.jsx-1727309017 {
        font-size: 13px;
        font-weight: bold;
        -webkit-letter-spacing: 0.2px;
        -moz-letter-spacing: 0.2px;
        -ms-letter-spacing: 0.2px;
        letter-spacing: 0.2px;
        color: #686f75;
      }

      .Upvote.jsx-1727309017:hover {
        background-color: #ffffff;
      }

      .Upvote.news.jsx-1727309017:hover,
      .Upvote.comment.jsx-1727309017:hover {
        background-color: #ebf4f7;
      }

      .Upvote.news.jsx-1727309017:hover .count.jsx-1727309017,
      .Upvote.comment.jsx-1727309017:hover .count.jsx-1727309017 {
        color: #06bdfc;
      }

      .Upvote.jsx-1727309017 .voted.jsx-1727309017 {
        background-color: #06bdfc;
        border-color: #06bdfc;
      }

      .Upvote.news.jsx-1727309017 .voted.jsx-1727309017,
      .Upvote.comment.jsx-1727309017 .voted.jsx-1727309017 {
        background-color: #ffffff;
        border: none;
      }

      .Upvote.jsx-1727309017 .voted.jsx-1727309017 .icon.jsx-1727309017 svg {
        fill: undefined;
      }

      .Upvote.news.jsx-1727309017
        .voted.jsx-1727309017
        .icon.jsx-1727309017
        svg,
      .Upvote.comment.jsx-1727309017
        .voted.jsx-1727309017
        .icon.jsx-1727309017
        svg {
        fill: #36d57d;
      }

      .Upvote.jsx-1727309017 .voted.jsx-1727309017 .count.jsx-1727309017 {
        color: #ffffff;
      }

      .Upvote.news.jsx-1727309017 .voted.jsx-1727309017 .count.jsx-1727309017,
      .Upvote.comment.jsx-1727309017
        .voted.jsx-1727309017
        .count.jsx-1727309017 {
        color: #36d57d;
      }

      @media (min-width: 800px) {
        .Upvote.news.jsx-1727309017 {
          height: 45px;
        }

        .Upvote.comment.jsx-1727309017 {
          height: 25px;
        }

        .Upvote.upvoted.news.jsx-1727309017 > div.jsx-1727309017 {
          top: -45px;
        }

        .Upvote.upvoted.comment.jsx-1727309017 > div.jsx-1727309017 {
          top: -25px;
        }

        .Upvote.news.jsx-1727309017 > div.jsx-1727309017 > div.jsx-1727309017 {
          height: 45px;
        }

        .Upvote.comment.jsx-1727309017
          > div.jsx-1727309017
          > div.jsx-1727309017 {
          height: 25px;
        }
      }
    </style>
    <style id="__jsx-2027482828">
      .Social.jsx-2027482828 {
        -webkit-flex: 0 0 auto;
        -ms-flex: 0 0 auto;
        flex: 0 0 auto;
      }

      .icons.jsx-2027482828 {
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-flex-direction: row;
        -ms-flex-direction: row;
        flex-direction: row;
        -webkit-box-pack: center;
        -webkit-justify-content: center;
        -ms-flex-pack: center;
        justify-content: center;
        -webkit-align-items: center;
        -webkit-box-align: center;
        -ms-flex-align: center;
        align-items: center;
      }

      .icon.jsx-2027482828 {
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-box-pack: center;
        -webkit-justify-content: center;
        -ms-flex-pack: center;
        justify-content: center;
        -webkit-align-items: center;
        -webkit-box-align: center;
        -ms-flex-align: center;
        align-items: center;
        width: 28px;
        height: 28px;
        border: 1px solid #e3e7e8;
        background-color: #ffffff;
        border-radius: 50%;
      }

      .icon.jsx-2027482828:hover {
        background-color: #f0f4f5;
      }

      .centerIcon.jsx-2027482828 {
        margin: 0 10px;
      }

      .icon.jsx-2027482828 svg {
        fill: #686f75;
        -webkit-align-self: center;
        -ms-flex-item-align: center;
        align-self: center;
        -webkit-flex: 1 1 auto;
        -ms-flex: 1 1 auto;
        flex: 1 1 auto;
      }

      @media (min-width: 800px) {
        .Social.jsx-2027482828 {
          margin-top: 18px;
        }

        .vertical.jsx-2027482828 {
          margin-top: 10px;
        }

        .vertical.jsx-2027482828 .icons.jsx-2027482828 {
          -webkit-flex-direction: column;
          -ms-flex-direction: column;
          flex-direction: column;
        }

        .vertical.jsx-2027482828 .centerIcon.jsx-2027482828 {
          margin: 10px 0;
        }
      }
    </style>
    <style id="__jsx-1670412252">
      .Avatar.jsx-1670412252 {
        display: inline-block;
        background-size: cover;
        background-color: #e6eaeb;
        background-repeat: no-repeat;
      }
    </style>
    <style id="__jsx-886169423">
      .Author.jsx-886169423 a.jsx-886169423 {
        display: -webkit-inline-box;
        display: -webkit-inline-flex;
        display: -ms-inline-flexbox;
        display: inline-flex;
        -webkit-flex-direction: row;
        -ms-flex-direction: row;
        flex-direction: row;
        -webkit-align-items: center;
        -webkit-box-align: center;
        -ms-flex-align: center;
        align-items: center;
        cursor: pointer;
      }

      .info.jsx-886169423 {
        margin-left: 9px;
        white-space: nowrap;
      }

      .mirrored.jsx-886169423 .info.jsx-886169423 {
        margin: 0 9px 0 0;
        -webkit-order: -1;
        -ms-flex-order: -1;
        order: -1;
      }

      .name.jsx-886169423 {
        font-size: 13px;
        font-weight: bold;
        -webkit-letter-spacing: 0.2px;
        -moz-letter-spacing: 0.2px;
        -ms-letter-spacing: 0.2px;
        letter-spacing: 0.2px;
        color: #3d4251;
        -webkit-text-decoration: none;
        text-decoration: none;
      }

      .name.jsx-886169423:hover {
        color: #06bdfc;
      }

      .date.jsx-886169423 {
        font-size: 11px;
        -webkit-letter-spacing: 0.2px;
        -moz-letter-spacing: 0.2px;
        -ms-letter-spacing: 0.2px;
        letter-spacing: 0.2px;
        color: #686f75;
      }
    </style>
    <style id="__jsx-3073944448">
      .Tag.jsx-3073944448 {
        display: inline-block;
        border-radius: 4px;
        background-color: #f0f4f5;
        border: solid 1px #e6eaeb;
        -webkit-letter-spacing: 0.2px;
        -moz-letter-spacing: 0.2px;
        -ms-letter-spacing: 0.2px;
        letter-spacing: 0.2px;
        color: #686f75;
        cursor: default;
      }

      .title.jsx-3073944448 {
        line-height: 20px;
        padding: 0 8px;
        font-size: 11px;
        text-transform: uppercase;
      }

      .Tag.mustRead.jsx-3073944448 {
        background-color: #ad86ce;
        border-color: #ad86ce;
        color: #ffffff;
        font-weight: bold;
      }

      .Tag.jsx-3073944448:hover {
        background-color: #ffffff;
      }

      .Tag.mustRead.jsx-3073944448:hover {
        background-color: #ceabec;
        border-color: #ceabec;
      }
    </style>
    <style id="__jsx-3814997685">
      .more.jsx-3814997685 {
        font-size: 11px;
        cursor: default;
        color: #686f75;
      }

      .more.jsx-3814997685:hover {
        -webkit-text-decoration: underline;
        text-decoration: underline;
      }
    </style>
    <style id="__jsx-253207151">
      .tooltipInner .Tag {
        margin: 4px;
      }

      .rc-tooltip {
        position: absolute;
        z-index: 200;
        display: block;
        visibility: visible;
        line-height: 1.5;
        font-size: 12px;
        border-radius: 4px;
      }

      .rc-tooltip-hidden {
        display: none;
      }

      .rc-tooltip-inner {
        padding: 12px;
        color: #333333;
        text-align: left;
        -webkit-text-decoration: none;
        text-decoration: none;
        background-color: #ffffff;
        border-radius: 4px;
        min-height: 34px;
        border: 1px solid #e3e7e8;
      }

      .rc-tooltip-arrow,
      .rc-tooltip-arrow-inner {
        position: absolute;
        width: 0;
        height: 0;
        border-color: transparent;
        border-style: solid;
      }

      .rc-tooltip-placement-top .rc-tooltip-arrow,
      .rc-tooltip-placement-topLeft .rc-tooltip-arrow,
      .rc-tooltip-placement-topRight .rc-tooltip-arrow {
        bottom: -5px;
        margin-left: -6px;
        border-width: 6px 6px 0;
        border-top-color: #e3e7e8;
      }

      .rc-tooltip-placement-top .rc-tooltip-arrow-inner,
      .rc-tooltip-placement-topLeft .rc-tooltip-arrow-inner,
      .rc-tooltip-placement-topRight .rc-tooltip-arrow-inner {
        bottom: 1px;
        margin-left: -6px;
        border-width: 6px 6px 0;
        border-top-color: #ffffff;
      }

      .rc-tooltip-placement-top .rc-tooltip-arrow {
        left: 50%;
      }

      .rc-tooltip-placement-topLeft .rc-tooltip-arrow {
        left: 15%;
      }

      .rc-tooltip-placement-topRight .rc-tooltip-arrow {
        right: 15%;
      }

      .rc-tooltip-placement-right .rc-tooltip-arrow,
      .rc-tooltip-placement-rightTop .rc-tooltip-arrow,
      .rc-tooltip-placement-rightBottom .rc-tooltip-arrow {
        left: -5px;
        margin-top: -6px;
        border-width: 6px 6px 6px 0;
        border-right-color: #e3e7e8;
      }

      .rc-tooltip-placement-right .rc-tooltip-arrow-inner,
      .rc-tooltip-placement-rightTop .rc-tooltip-arrow-inner,
      .rc-tooltip-placement-rightBottom .rc-tooltip-arrow-inner {
        left: 1px;
        margin-top: -6px;
        border-width: 6px 6px 6px 0;
        border-right-color: #ffffff;
      }

      .rc-tooltip-placement-right .rc-tooltip-arrow {
        top: 50%;
      }

      .rc-tooltip-placement-rightTop .rc-tooltip-arrow {
        top: 15%;
        margin-top: 0;
      }

      .rc-tooltip-placement-rightBottom .rc-tooltip-arrow {
        bottom: 15%;
      }

      .rc-tooltip-placement-left .rc-tooltip-arrow,
      .rc-tooltip-placement-leftTop .rc-tooltip-arrow,
      .rc-tooltip-placement-leftBottom .rc-tooltip-arrow {
        right: -5px;
        margin-top: -6px;
        border-width: 6px 0 6px 6px;
        border-left-color: #e3e7e8;
      }

      .rc-tooltip-placement-left .rc-tooltip-arrow-inner,
      .rc-tooltip-placement-leftTop .rc-tooltip-arrow-inner,
      .rc-tooltip-placement-leftBottom .rc-tooltip-arrow-inner {
        right: 1px;
        margin-top: -6px;
        border-width: 6px 0 6px 6px;
        border-left-color: #ffffff;
      }

      .rc-tooltip-placement-left .rc-tooltip-arrow {
        top: 50%;
      }

      .rc-tooltip-placement-leftTop .rc-tooltip-arrow {
        top: 15%;
        margin-top: 0;
      }

      .rc-tooltip-placement-leftBottom .rc-tooltip-arrow {
        bottom: 15%;
      }

      .rc-tooltip-placement-bottom .rc-tooltip-arrow,
      .rc-tooltip-placement-bottomLeft .rc-tooltip-arrow,
      .rc-tooltip-placement-bottomRight .rc-tooltip-arrow {
        top: -5px;
        margin-left: -6px;
        border-width: 0 6px 6px;
        border-bottom-color: #e3e7e8;
      }

      .rc-tooltip-placement-bottom .rc-tooltip-arrow-inner,
      .rc-tooltip-placement-bottomLeft .rc-tooltip-arrow-inner,
      .rc-tooltip-placement-bottomRight .rc-tooltip-arrow-inner {
        top: 1px;
        margin-left: -6px;
        border-width: 0 6px 6px;
        border-bottom-color: #ffffff;
      }

      .rc-tooltip-placement-bottom .rc-tooltip-arrow {
        left: 50%;
      }

      .rc-tooltip-placement-bottomLeft .rc-tooltip-arrow {
        left: 15%;
      }

      .rc-tooltip-placement-bottomRight .rc-tooltip-arrow {
        right: 15%;
      }

      .rc-tooltip.rc-tooltip-zoom-enter,
      .rc-tooltip.rc-tooltip-zoom-leave {
        display: block;
      }

      .rc-tooltip-zoom-enter,
      .rc-tooltip-zoom-appear {
        opacity: 0;
        -webkit-animation-duration: 0.3s;
        -webkit-animation-duration: 0.3s;
        animation-duration: 0.3s;
        -webkit-animation-fill-mode: both;
        -webkit-animation-fill-mode: both;
        animation-fill-mode: both;
        -webkit-animation-timing-function: cubic-bezier(0.18, 0.89, 0.32, 1.28);
        -webkit-animation-timing-function: cubic-bezier(0.18, 0.89, 0.32, 1.28);
        animation-timing-function: cubic-bezier(0.18, 0.89, 0.32, 1.28);
        -webkit-animation-play-state: paused;
        -webkit-animation-play-state: paused;
        animation-play-state: paused;
      }

      .rc-tooltip-zoom-leave {
        -webkit-animation-duration: 0.3s;
        -webkit-animation-duration: 0.3s;
        animation-duration: 0.3s;
        -webkit-animation-fill-mode: both;
        -webkit-animation-fill-mode: both;
        animation-fill-mode: both;
        -webkit-animation-timing-function: cubic-bezier(0.6, -0.3, 0.74, 0.05);
        -webkit-animation-timing-function: cubic-bezier(0.6, -0.3, 0.74, 0.05);
        animation-timing-function: cubic-bezier(0.6, -0.3, 0.74, 0.05);
        -webkit-animation-play-state: paused;
        -webkit-animation-play-state: paused;
        animation-play-state: paused;
      }

      .rc-tooltip-zoom-enter.rc-tooltip-zoom-enter-active,
      .rc-tooltip-zoom-appear.rc-tooltip-zoom-appear-active {
        -webkit-animation-name: rcToolTipZoomIn;
        -webkit-animation-name: rcToolTipZoomIn;
        animation-name: rcToolTipZoomIn;
        -webkit-animation-play-state: running;
        -webkit-animation-play-state: running;
        animation-play-state: running;
      }

      .rc-tooltip-zoom-leave.rc-tooltip-zoom-leave-active {
        -webkit-animation-name: rcToolTipZoomOut;
        -webkit-animation-name: rcToolTipZoomOut;
        animation-name: rcToolTipZoomOut;
        -webkit-animation-play-state: running;
        -webkit-animation-play-state: running;
        animation-play-state: running;
      }

      @-webkit-keyframes rcToolTipZoomIn {
        0% {
          opacity: 0;
          -webkit-transform-origin: 50% 50%;
          -webkit-transform-origin: 50% 50%;
          -ms-transform-origin: 50% 50%;
          transform-origin: 50% 50%;
          -webkit-transform: scale(0, 0);
          -webkit-transform: scale(0, 0);
          -ms-transform: scale(0, 0);
          transform: scale(0, 0);
        }

        100% {
          opacity: 1;
          -webkit-transform-origin: 50% 50%;
          -webkit-transform-origin: 50% 50%;
          -ms-transform-origin: 50% 50%;
          transform-origin: 50% 50%;
          -webkit-transform: scale(1, 1);
          -webkit-transform: scale(1, 1);
          -ms-transform: scale(1, 1);
          transform: scale(1, 1);
        }
      }

      @-webkit-keyframes rcToolTipZoomIn {
        0% {
          opacity: 0;
          -webkit-transform-origin: 50% 50%;
          -webkit-transform-origin: 50% 50%;
          -ms-transform-origin: 50% 50%;
          transform-origin: 50% 50%;
          -webkit-transform: scale(0, 0);
          -webkit-transform: scale(0, 0);
          -ms-transform: scale(0, 0);
          transform: scale(0, 0);
        }

        100% {
          opacity: 1;
          -webkit-transform-origin: 50% 50%;
          -webkit-transform-origin: 50% 50%;
          -ms-transform-origin: 50% 50%;
          transform-origin: 50% 50%;
          -webkit-transform: scale(1, 1);
          -webkit-transform: scale(1, 1);
          -ms-transform: scale(1, 1);
          transform: scale(1, 1);
        }
      }

      @keyframes rcToolTipZoomIn {
        0% {
          opacity: 0;
          -webkit-transform-origin: 50% 50%;
          -webkit-transform-origin: 50% 50%;
          -ms-transform-origin: 50% 50%;
          transform-origin: 50% 50%;
          -webkit-transform: scale(0, 0);
          -webkit-transform: scale(0, 0);
          -ms-transform: scale(0, 0);
          transform: scale(0, 0);
        }

        100% {
          opacity: 1;
          -webkit-transform-origin: 50% 50%;
          -webkit-transform-origin: 50% 50%;
          -ms-transform-origin: 50% 50%;
          transform-origin: 50% 50%;
          -webkit-transform: scale(1, 1);
          -webkit-transform: scale(1, 1);
          -ms-transform: scale(1, 1);
          transform: scale(1, 1);
        }
      }

      @-webkit-keyframes rcToolTipZoomOut {
        0% {
          opacity: 1;
          -webkit-transform-origin: 50% 50%;
          -webkit-transform-origin: 50% 50%;
          -ms-transform-origin: 50% 50%;
          transform-origin: 50% 50%;
          -webkit-transform: scale(1, 1);
          -webkit-transform: scale(1, 1);
          -ms-transform: scale(1, 1);
          transform: scale(1, 1);
        }

        100% {
          opacity: 0;
          -webkit-transform-origin: 50% 50%;
          -webkit-transform-origin: 50% 50%;
          -ms-transform-origin: 50% 50%;
          transform-origin: 50% 50%;
          -webkit-transform: scale(0, 0);
          -webkit-transform: scale(0, 0);
          -ms-transform: scale(0, 0);
          transform: scale(0, 0);
        }
      }

      @-webkit-keyframes rcToolTipZoomOut {
        0% {
          opacity: 1;
          -webkit-transform-origin: 50% 50%;
          -webkit-transform-origin: 50% 50%;
          -ms-transform-origin: 50% 50%;
          transform-origin: 50% 50%;
          -webkit-transform: scale(1, 1);
          -webkit-transform: scale(1, 1);
          -ms-transform: scale(1, 1);
          transform: scale(1, 1);
        }

        100% {
          opacity: 0;
          -webkit-transform-origin: 50% 50%;
          -webkit-transform-origin: 50% 50%;
          -ms-transform-origin: 50% 50%;
          transform-origin: 50% 50%;
          -webkit-transform: scale(0, 0);
          -webkit-transform: scale(0, 0);
          -ms-transform: scale(0, 0);
          transform: scale(0, 0);
        }
      }

      @keyframes rcToolTipZoomOut {
        0% {
          opacity: 1;
          -webkit-transform-origin: 50% 50%;
          -webkit-transform-origin: 50% 50%;
          -ms-transform-origin: 50% 50%;
          transform-origin: 50% 50%;
          -webkit-transform: scale(1, 1);
          -webkit-transform: scale(1, 1);
          -ms-transform: scale(1, 1);
          transform: scale(1, 1);
        }

        100% {
          opacity: 0;
          -webkit-transform-origin: 50% 50%;
          -webkit-transform-origin: 50% 50%;
          -ms-transform-origin: 50% 50%;
          transform-origin: 50% 50%;
          -webkit-transform: scale(0, 0);
          -webkit-transform: scale(0, 0);
          -ms-transform: scale(0, 0);
          transform: scale(0, 0);
        }
      }
    </style>
    <style id="__jsx-3846131586">
      .TagLine.jsx-3846131586 {
        display: inline-block;
        white-space: nowrap;
      }

      .TagLine.jsx-3846131586 > .Tag {
        margin-right: 10px;
      }

      .more.jsx-3846131586 {
        font-size: 11px;
      }
    </style>
    <style id="__jsx-1575957864">
      .markdown {
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-flex-direction: column;
        -ms-flex-direction: column;
        flex-direction: column;
        -webkit-box-pack: start;
        -webkit-justify-content: flex-start;
        -ms-flex-pack: start;
        justify-content: flex-start;
        -webkit-align-items: flex-start;
        -webkit-box-align: flex-start;
        -ms-flex-align: flex-start;
        align-items: flex-start;
        font-family: "Lora", serif;
        font-size: 16px;
        padding: 1.5em;
      }

      .markdown [data-datacamp-exercise] {
        overflow: hidden;
      }

      .markdown > div {
        width: 100%;
      }

      .markdown div {
        outline: none;
      }

      .markdown hr {
        border: 0;
        border-bottom: 1px solid #e6eaeb;
        margin: 3em 0;
      }

      .markdown div[data-type="mathjax"] {
        margin: 1.5em 0;
      }

      .markdown p {
        font-family: "Lora", serif;
        font-size: 1em;
        line-height: 1.8em;
        color: #3d4251;
      }

      .markdown .powered-by-datacamp + p {
        margin-top: 1.5em !important;
      }

      .markdown p + p,
      .markdown p + img,
      .markdown p + div,
      .markdown p + table,
      .markdown p + ol,
      .markdown p + ul,
      .markdown p + nav,
      .markdown div + p,
      .markdown p + iframe,
      .markdown iframe + p,
      .markdown pre + img,
      .markdown pre + p {
        margin-top: 1.5em !important;
      }

      .markdown h2 {
        font-family: "Lato", sans-serif;
        font-size: 1.5em;
        font-weight: 700;
        color: #3d4251;
        line-height: 1.3em;
        margin: 1.5em 0 0.5em;
      }

      .markdown h3 {
        font-family: "Lato", sans-serif;
        font-size: 1.1em;
        font-weight: 700;
        color: #3d4251;
        line-height: 1.2em;
        margin: 1.5em 0 0.5em;
      }

      .markdown h4 {
        font-family: "Lato", sans-serif;
        font-size: 1em;
        font-weight: 700;
        color: #3d4251;
        line-height: 1.2em;
        margin: 1.5em 0 0.5em;
      }

      .markdown .videoWrapper {
        position: relative;
        padding-bottom: 47.25%;
        padding-top: 25px;
        height: 0;
        margin-bottom: 1.5em;
      }

      .markdown .videoWrapper iframe {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
      }

      .markdown p code,
      .markdown li code {
        display: inline-block;
        padding: 0 5px;
        border-radius: 4px;
        font-family: "Roboto Mono", monospace;
        font-size: 0.9em;
        line-height: 1.6em;
        color: #3d4251;
        background-color: #e6eaeb;
      }

      .markdown a code {
        color: #009bd8;
      }

      .markdown pre {
        padding: 1em 1.5em;
        font-family: "Roboto Mono", monospace;
        font-size: 0.9em;
        background-color: #002b36 !important;
        border-radius: 4px;
        overflow-x: auto;
        -webkit-overflow-scrolling: touch;
      }

      .markdown pre code {
        padding: 0;
        font-size: 0.9em;
        line-height: 2em;
        background-color: #002b36;
        overflow-x: visible;
        -webkit-overflow-scrolling: touch;
      }

      .markdown img {
        display: block;
        -webkit-flex: 1 1 auto;
        -ms-flex: 1 1 auto;
        flex: 1 1 auto;
        margin: auto;
        max-width: 100%;
        height: auto !important;
      }

      .markdown img + p {
        margin-top: 1.5em !important;
      }

      .markdown nav + p,
      .markdown nav + div {
        margin-top: 1.5em !important;
      }

      .markdown iframe {
        width: 100%;
      }

      .markdown ul,
      .markdown ol {
        font-family: "Lora", serif;
        color: #3d4251;
        padding: 0 0 0 1em;
      }

      .markdown ul + p,
      .markdown ol + p,
      .markdown ul + div,
      .markdown ol + div,
      .markdown ul + pre,
      .markdown ol + pre {
        margin-top: 1.5em !important;
      }

      .markdown div.datacamp-exercise ul,
      .markdown div.datacamp-exercise ol {
        background-color: initial;
        margin: initial;
        padding: initial;
        width: initial;
        list-style-position: initial;
        line-height: initial;
      }

      .markdown ul.oneliner,
      .markdown ol.oneliner {
        padding: 0;
        list-style-position: inside;
        line-height: 1.5em;
      }

      .markdown li {
        line-height: 1.8em;
      }

      .markdown li + li {
        margin-top: 1em;
      }

      .markdown div.datacamp-exercise li {
        padding-left: initial;
        background-color: initial;
        font-size: initial;
        font-weight: initial;
        line-height: initial;
      }

      .markdown li p {
        margin: 0;
        font-size: 1em;
        line-height: 1.8em;
      }

      .markdown ul.oneliner li,
      .markdown ol.oneliner li {
        padding-left: 0;
        white-space: nowrap;
        text-overflow: ellipsis;
        overflow: hidden;
      }

      .markdown ul ul {
        margin: 0;
        list-style: circle;
      }

      .markdown ol ol {
        margin: 0;
      }

      .markdown ol ul,
      .markdown ul ol {
        margin: 0;
      }

      .markdown a {
        font-weight: 400;
        -webkit-text-decoration: none;
        text-decoration: none;
        color: #009bd8;
      }

      .markdown a:hover {
        -webkit-text-decoration: underline;
        text-decoration: underline;
      }

      .markdown div.datacamp-exercise a {
        font-weight: initial;
        -webkit-text-decoration: initial;
        text-decoration: initial;
      }

      .markdown div.datacamp-exercise a:hover {
        -webkit-text-decoration: initial;
        text-decoration: initial;
      }

      .markdown div.datacamp-exercise li + li {
        margin-top: unset;
      }

      .markdown blockquote {
        margin: 1.5em 0;
        font-family: "Lato", sans-serif;
        color: #3d4251;
        font-weight: 300;
        font-size: 1.5em;
        font-style: italic;
        line-height: 2em;
      }

      .markdown blockquote::before {
        display: block;
        margin-bottom: 15px;
        width: 35px;
        height: 35px;
        font-family: "Lora", serif;
        font-size: 36px;
        font-weight: bold;
        font-style: normal;
        line-height: 60px;
        text-align: center;
        color: #06bdfc;
        content: "“";
        border: 1px solid #e3e7e8;
        border-radius: 50%;
      }

      .markdown table {
        width: 100% !important;
        border: 1px solid #e3e7e8;
        border-radius: 4px;
        overflow: hidden;
        border-collapse: separate;
        border-spacing: 0;
      }

      .markdown table th,
      .markdown table td {
        padding: 0.75em;
      }

      .markdown table tr th,
      .markdown table tr td {
        border: 1px solid #e3e7e8;
        vertical-align: middle;
      }

      .markdown table thead tr th {
        font-family: "Lato", sans-serif;
        background-color: #f0f4f5;
      }

      .markdown table tr:nth-child(even) {
        background-color: #f0f4f5;
      }

      .markdown table + p {
        margin-top: 1.5em !important;
      }

      .markdown table + img {
        margin-top: 1.5em !important;
      }

      .markdown table + div {
        margin-top: 1.5em !important;
      }

      .markdown .dcl-content--tab-body {
        margin-top: 0 !important;
      }

      @media (min-width: 800px) {
        .markdown {
          font-size: 20px;
          padding: 0;
        }

        .markdown h2,
        .markdown h3,
        .markdown h4 {
          margin: 1.5em 0 0.5em;
        }

        .markdown p {
          margin: 0;
        }

        .markdown li p {
          font-size: inherit;
          line-height: inherit;
        }

        .markdown ul + p,
        .markdown ol + p,
        .markdown ul + div,
        .markdown ol + div,
        .markdown ul + pre,
        .markdown ol + pre {
          margin-top: 1.5em !important;
        }

        .markdown blockquote {
          position: relative;
          margin: 50px 55px;
        }

        .markdown blockquote::before {
          position: absolute;
          left: -55px;
        }
      }

      .output_wrapper {
        overflow-x: auto;
        -webkit-overflow-scrolling: touch;
      }
    </style>
    <style id="__jsx-2326038540">
      .SidebarSocial.jsx-2326038540 {
        -webkit-flex: 1 1 auto;
        -ms-flex: 1 1 auto;
        flex: 1 1 auto;
        font-size: 11px;
        font-weight: bold;
      }

      .rss.jsx-2326038540 {
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-flex-direction: row;
        -ms-flex-direction: row;
        flex-direction: row;
        -webkit-box-pack: center;
        -webkit-justify-content: center;
        -ms-flex-pack: center;
        justify-content: center;
        -webkit-align-items: center;
        -webkit-box-align: center;
        -ms-flex-align: center;
        align-items: center;
        line-height: 50px;
      }

      .rss.jsx-2326038540 svg {
        padding-right: 7px;
        fill: #ffc844;
      }

      .rss.jsx-2326038540 a.jsx-2326038540 {
        -webkit-text-decoration: none;
        text-decoration: none;
        color: #ffffff;
      }

      .rss.jsx-2326038540 a.jsx-2326038540:hover {
        -webkit-text-decoration: none;
        text-decoration: none;
        color: #ffc844;
      }

      .icons.jsx-2326038540 {
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-flex-direction: row;
        -ms-flex-direction: row;
        flex-direction: row;
        -webkit-box-pack: center;
        -webkit-justify-content: center;
        -ms-flex-pack: center;
        justify-content: center;
        -webkit-align-items: center;
        -webkit-box-align: center;
        -ms-flex-align: center;
        align-items: center;
      }

      .icon.jsx-2326038540 {
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-box-pack: center;
        -webkit-justify-content: center;
        -ms-flex-pack: center;
        justify-content: center;
        -webkit-align-items: center;
        -webkit-box-align: center;
        -ms-flex-align: center;
        align-items: center;
        margin: 0 7px;
        width: 30px;
        height: 30px;
      }

      .icon.jsx-2326038540 svg {
        fill: #ffffff;
        -webkit-align-self: center;
        -ms-flex-item-align: center;
        align-self: center;
        -webkit-flex: 1 1 0;
        -ms-flex: 1 1 0;
        flex: 1 1 0;
      }

      .icon.jsx-2326038540:hover svg {
        fill: #03ef62;
      }

      .menu.jsx-2326038540 {
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-flex-direction: row;
        -ms-flex-direction: row;
        flex-direction: row;
        -webkit-box-pack: center;
        -webkit-justify-content: center;
        -ms-flex-pack: center;
        justify-content: center;
        -webkit-align-items: center;
        -webkit-box-align: center;
        -ms-flex-align: center;
        align-items: center;
        line-height: 44px;
      }

      .menuItem.jsx-2326038540 {
        -webkit-flex: 0 0 auto;
        -ms-flex: 0 0 auto;
        flex: 0 0 auto;
        padding: 5px;
        -webkit-text-decoration: none;
        text-decoration: none;
        color: #ffffff;
      }

      .menuItem--active.jsx-2326038540,
      a.jsx-2326038540:hover {
        color: #03ef62;
      }

      @media (min-width: 800px) and (min-height: 585px) {
        .SidebarSocial.jsx-2326038540 {
          width: 220px;
          position: fixed;
          bottom: 0;
          left: 0;
        }
      }
    </style>
    <style id="__jsx-1777972523">
      .Layout.jsx-1777972523 {
        -webkit-flex: 1 1 auto;
        -ms-flex: 1 1 auto;
        flex: 1 1 auto;
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-flex-direction: column;
        -ms-flex-direction: column;
        flex-direction: column;
        -webkit-box-pack: start;
        -webkit-justify-content: flex-start;
        -ms-flex-pack: start;
        justify-content: flex-start;
        -webkit-align-items: stretch;
        -webkit-box-align: stretch;
        -ms-flex-align: stretch;
        align-items: stretch;
        background-color: #05192d;
      }

      .Layout--openMenu.jsx-1777972523 {
        min-height: 100vh;
      }

      .Main.jsx-1777972523 {
        min-height: 100vh;
        -webkit-flex: 1 1 auto;
        -ms-flex: 1 1 auto;
        flex: 1 1 auto;
        background-color: #f7f3eb;
      }

      .Layout--banner.jsx-1777972523 .Main.jsx-1777972523 {
        margin-top: 55px;
        min-height: calc(100vh - 55px);
      }

      .Layout.bar.jsx-1777972523:not(.Layout--openMenu) .SidebarSocial {
        margin-bottom: 90px;
      }

      .Layout.editor.jsx-1777972523:not(.Layout--openMenu) .SidebarSocial {
        margin-bottom: 300px;
      }

      @media (min-width: 800px) {
        .Main.jsx-1777972523 {
          margin-left: 220px;
        }

        .Layout--banner.jsx-1777972523 .Main.jsx-1777972523 {
          margin-top: 80px;
          min-height: calc(100vh - 80px);
        }

        .Layout.bar.jsx-1777972523:not(.Layout--openMenu) .SidebarSocial,
        .Layout.editor.jsx-1777972523:not(.Layout--openMenu) .SidebarSocial {
          margin-bottom: 0;
        }

        .Layout.bar.jsx-1777972523 .Main > div:last-child {
          margin-bottom: 90px;
        }

        .Layout.editor.jsx-1777972523 .Main > div:last-child {
          margin-bottom: 300px;
        }
      }
    </style>
    <style id="__jsx-1851940463">
      .Tutorial.jsx-1851940463 {
        margin: 0 0px 30px;
        padding: 20px 0 0;
        background-color: #ffffff;
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-flex-direction: column;
        -ms-flex-direction: column;
        flex-direction: column;
      }

      .preface.jsx-1851940463 {
        margin: 0 20px;
      }

      .author.jsx-1851940463 {
        margin-bottom: 10px;
      }

      h1.jsx-1851940463 {
        margin-top: 20px;
      }

      .illustration.jsx-1851940463 {
        margin-bottom: 30px;
      }

      .illustration.jsx-1851940463 img.jsx-1851940463 {
        max-width: 100%;
      }

      .Tutorial.jsx-1851940463 .social__top .voteAndSocial,
      .social__bottom.jsx-1851940463 .voteAndSocial.jsx-1851940463 {
        padding: 40px 20px 20px;
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-flex-direction: row;
        -ms-flex-direction: row;
        flex-direction: row;
        -webkit-box-pack: justify;
        -webkit-justify-content: space-between;
        -ms-flex-pack: justify;
        justify-content: space-between;
        -webkit-align-items: flex-end;
        -webkit-box-align: flex-end;
        -ms-flex-align: flex-end;
        align-items: flex-end;
      }

      .Tutorial.jsx-1851940463
        .voteAndSocial.jsx-1851940463
        > div.jsx-1851940463 {
        display: -webkit-box;
        display: -webkit-flex;
        display: -ms-flexbox;
        display: flex;
        -webkit-flex-direction: row;
        -ms-flex-direction: row;
        flex-direction: row;
        -webkit-box-pack: start;
        -webkit-justify-content: flex-start;
        -ms-flex-pack: start;
        justify-content: flex-start;
        -webkit-align-items: center;
        -webkit-box-align: center;
        -ms-flex-align: center;
        align-items: center;
      }

      .Tutorial.jsx-1851940463
        .voteAndSocial.jsx-1851940463
        > div.jsx-1851940463
        .CommentCounter {
        margin-left: 10px;
      }

      @media (min-width: 800px) {
        .Tutorial.jsx-1851940463 {
          margin: 0 auto 30px;
          padding: 30px 100px 100px;
          max-width: 1120px;
          border-radius: 4px;
          border: 1px solid #e3e7e8;
        }

        .preface.jsx-1851940463 {
          margin: 0;
        }

        .Tutorial.jsx-1851940463 .social__top {
          position: absolute;
          margin-top: 220px;
        }

        .Tutorial.jsx-1851940463 .social__top .voteAndSocial {
          position: absolute;
          left: -100px;
          top: 0;
          width: 100px;
          height: auto;
          -webkit-flex-direction: column;
          -ms-flex-direction: column;
          flex-direction: column;
          -webkit-box-pack: start;
          -webkit-justify-content: flex-start;
          -ms-flex-pack: start;
          justify-content: flex-start;
          -webkit-align-items: center;
          -webkit-box-align: center;
          -ms-flex-align: center;
          align-items: center;
        }

        .Tutorial.jsx-1851940463
          .voteAndSocial.jsx-1851940463
          > div.jsx-1851940463 {
          -webkit-flex-direction: column;
          -ms-flex-direction: column;
          flex-direction: column;
        }

        .Tutorial.jsx-1851940463 .voteAndSocial > div .CommentCounter {
          margin-left: 0;
          margin-bottom: 10px;
        }
      }
    </style>
    <style id="__jsx-2124067752">
      * {
        box-sizing: border-box;
      }

      html,
      body {
        min-height: 100vh;
        margin: 0;
        padding: 0;
        background-image: linear-gradient(207deg, #2388b0, #33aacc);
        background-size: 100vw 100vh;
        background-attachment: fixed;
        background-repeat: no-repeat;
      }

      body.ReactModal__Body--open {
        overflow: hidden;
      }

      .ReactModal__Content {
        width: 100%;
      }

      img {
        margin: auto;
      }

      .mobileOnlyShow {
        display: block !important;
      }

      .mobileOnlyHide {
        display: none !important;
      }

      .mobileOnly {
        display: block !important;
      }

      .desktopOnly {
        display: none !important;
      }

      @media (min-width: 800px) {
        body {
          background-image: linear-gradient(207deg, #2388b0, #33aacc);
          background-size: 220px 100vh;
          background-attachment: fixed;
          background-repeat: no-repeat;
        }

        .ReactModal__Content {
          width: auto;
        }

        .mobileOnlyShow {
          display: block !important;
        }

        .mobileOnlyHide {
          display: block !important;
        }

        .mobileOnly {
          display: none !important;
        }

        .desktopOnly {
          display: block !important;
        }
      }
    </style>
    <style id="__jsx-3928222068">
      body,
      input,
      button,
      select,
      textarea {
        font-family: "Lato", sans-serif;
        color: #686f75;
        font-size: 15px;
      }

      h1,
      .h1,
      h2,
      h3,
      h4,
      h5 {
        font-family: "Lato", sans-serif;
      }

      .pageTitle {
        font-family: "Lato", sans-serif;
        font-size: 32px;
        font-weight: bold;
        line-height: 1.3em;
        margin-bottom: 0.5em;
      }

      .pageDescription {
        font-family: "Lora", serif;
        font-size: 20.8px;
        line-height: 1.5em;
        margin-bottom: 1.4em;
        color: #3d4251;
      }

      h1,
      .h1 {
        font-size: 29px;
        color: #3d4251;
        font-weight: bold;
      }

      h2 {
        font-size: 20px;
        -webkit-letter-spacing: 0.3px;
        -moz-letter-spacing: 0.3px;
        -ms-letter-spacing: 0.3px;
        letter-spacing: 0.3px;
        line-height: 1.33;
        font-weight: bold;
        margin: 18px 0px;
        color: #3d4251;
      }

      h2.blue {
        color: #06bdfc;
      }

      a {
        color: #06bdfc;
        -webkit-text-decoration: none;
        text-decoration: none;
      }

      .blocText {
        font-size: 15px;
        -webkit-letter-spacing: 0.2px;
        -moz-letter-spacing: 0.2px;
        -ms-letter-spacing: 0.2px;
        letter-spacing: 0.2px;
        line-height: 1.47;
        color: #686f75;
      }

      label {
        display: block;
        width: 100%;
        margin-bottom: 8px;
        font-size: 13px;
      }

      label span {
        float: right;
        font-weight: 300;
      }

      input,
      textarea {
        padding: 15px;
        font-weight: 300;
        color: #3d4251;
        background-color: #f0f4f5;
        border: 1px solid transparent;
        border-radius: 4px;
        outline-style: none;
      }

      input:disabled,
      textarea:disabled {
        color: #686f75;
        background-color: #e6eaeb;
      }

      input.error,
      textarea.error {
        border: 1px solid #fe5c5c;
      }

      input:focus,
      textarea:focus {
        border: 1px solid #06bdfc;
        -webkit-transition: border 150ms ease-out;
        transition: border 150ms ease-out;
      }

      input::-webkit-input-placeholder,
      textarea::-webkit-input-placeholder {
        color: #06bdfc;
        -webkit-transition: color 150ms ease-out;
        transition: color 150ms ease-out;
      }

      input::-moz-placeholder,
      textarea::-moz-placeholder {
        color: #06bdfc;
        -webkit-transition: color 150ms ease-out;
        transition: color 150ms ease-out;
      }

      input:-ms-input-placeholder,
      textarea:-ms-input-placeholder {
        color: #06bdfc;
        -webkit-transition: color 150ms ease-out;
        transition: color 150ms ease-out;
      }

      input::placeholder,
      textarea::placeholder {
        color: #06bdfc;
        -webkit-transition: color 150ms ease-out;
        transition: color 150ms ease-out;
      }

      input:focus::-webkit-input-placeholder,
      textarea:focus::-webkit-input-placeholder {
        color: transparent;
      }

      input:focus::-moz-placeholder,
      textarea:focus::-moz-placeholder {
        color: transparent;
      }

      input:focus:-ms-input-placeholder,
      textarea:focus:-ms-input-placeholder {
        color: transparent;
      }

      input:focus::placeholder,
      textarea:focus::placeholder {
        color: transparent;
      }

      input.small,
      textarea.small {
        margin-bottom: 19px;
        padding: 8px 10px;
        font-size: 13px;
      }

      textarea.small {
        min-height: 55px;
      }

      @media (min-width: 800px) {
        h1,
        .h1 {
          font-size: 36px;
        }

        h2 {
          font-size: 32px;
        }

        .pageTitle {
          font-size: 40px;
        }

        .pageDescription {
          font-size: 26px;
        }
      }
    </style>
    <script
      async
      src="https://www.datacamp.com/cdn-cgi/bm/cv/669835187/api.js"
    ></script>
  </head>

  <body>
    <div id="__next">
      <div class="Page content">
        <div class="jsx-1777972523 Layout">
          <div class="jsx-3673784144 SidebarMenu">
            <div class="jsx-3673784144 logoContainer">
              <div css="[object Object]">
                <a href="https://www.datacamp.com/"
                  ><svg
                    height="29"
                    viewBox="0 0 173 36"
                    width="139.36111111111111"
                  >
                    <g fill="none" fill-rule="evenodd">
                      <path
                        d="M42.56 27.1a5.694 5.694 0 110-11.39 5.694 5.694 0 010 11.39m5.704-20.623v8.853a8.334 8.334 0 100 12.148v1.836h2.632V6.477h-2.632zm73.28 20.622a5.694 5.694 0 110-11.389 5.694 5.694 0 010 11.39m8.333-5.695v-8.247h-2.63v2.172a8.334 8.334 0 100 12.148v1.836h2.631v-7.91h-.001zm20.987-7.634a1.296 1.296 0 011.109-.622h.507c1.075 0 1.947.872 1.947 1.947v14.218h-2.686V17.269c-1.239 2-5.674 9.25-7.003 11.424a1.296 1.296 0 01-1.108.62h-.548a1.298 1.298 0 01-1.298-1.297V17.238a1909.582 1909.582 0 00-7.31 11.954l-.074.122h-2.574v-16.16h2.684v.033l-.062 11.147 6.438-10.56a1.3 1.3 0 011.11-.622h.51c1.073 0 1.944.869 1.947 1.942 0 2.972.014 8.383.014 9.17l6.397-10.493zm-37.92 12.541a8.331 8.331 0 11.21-9.502l-2.524 1.312a5.533 5.533 0 10-.379 6.88l2.693 1.31zm51.542.8a5.693 5.693 0 01-5.68-5.352v-.682a5.694 5.694 0 115.684 6.036m0-14.028a8.298 8.298 0 00-5.684 2.24v-2.168h-2.632V35.91h2.632v-8.4a8.333 8.333 0 105.684-14.425M75.277 15.68v9.938c0 .589.478 1.067 1.067 1.067h3.064v2.629h-3.062a3.7 3.7 0 01-3.696-3.696l-.01-9.938h-2.838v-2.56h2.838V8.702h2.635v4.428h4.672v2.55h-4.67zm12.757 11.418a5.694 5.694 0 110-11.39 5.694 5.694 0 010 11.39m5.702-13.941v2.173a8.334 8.334 0 100 12.148v1.836h2.632v-16.16l-2.632.003zM60.285 27.099a5.694 5.694 0 110-11.389 5.694 5.694 0 010 11.39m5.702-13.942v2.172a8.334 8.334 0 100 12.148v1.836h2.63v-16.16l-2.63.004z"
                        fill="#ffffff"
                      ></path>
                      <path
                        d="M11.7 8.514v8.332L2.857 21.89V3.44l8.841 5.074zm2.86 17.507v-7.51l11.84-6.757-2.88-1.65-8.96 5.112V7.68a1.44 1.44 0 00-.718-1.242L3.056.256A2.066 2.066 0 000 2.07v21.184a2.067 2.067 0 002.971 1.866l.082-.042 8.64-4.932v6.72c.002.513.276.987.721 1.243L23.502 34.4l2.88-1.651L14.56 26.02z"
                        fill="#03EF62"
                      ></path>
                    </g></svg
                ></a>
              </div>
            </div>
            <div class="jsx-3673784144 icon mobileOnly">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 70">
                <path
                  d="M6 1a6 6 0 1 0 0 12h68a6 6 0 0 0 0-12H6zm0 28a6 6 0 1 0 0 12h68a6 6 0 0 0 0-12H6zm0 28a6 6 0 1 0 0 12h68a6 6 0 0 0 0-12H6z"
                ></path>
              </svg>
            </div>
          </div>
          <div class="jsx-2576226853 Menu mobileOnlyHide">
            <div class="jsx-2576226853 section">
              <h5 class="jsx-2576226853">
                <div class="jsx-2576226853 title">community</div>
              </h5>
              <nav class="jsx-2576226853">
                <div>
                  <a
                    target="_self"
                    href="https://www.datacamp.com/community"
                    class="jsx-2576226853 item"
                  >
                    <div class="jsx-2576226853 image">
                      <svg
                        height="14"
                        xmlns="http://www.w3.org/2000/svg"
                        viewBox="0 0 23 23"
                      >
                        <path
                          id="a"
                          d="M2 4.5h20a1.5 1.5 0 0 1 0 3H2a1.5 1.5 0 0 1 0-3zm0 6h14a1.5 1.5 0 0 1 0 3H2a1.5 1.5 0 0 1 0-3zm0 6h20a1.5 1.5 0 0 1 0 3H2a1.5 1.5 0 0 1 0-3z"
                        ></path>
                      </svg>
                    </div>
                    <div class="jsx-2576226853 text">News</div>
                  </a>
                </div>
                <div>
                  <a
                    target="_self"
                    href="../tutorials.html"
                    class="jsx-2576226853 item active"
                  >
                    <div class="jsx-2576226853 image">
                      <svg
                        height="14"
                        xmlns="http://www.w3.org/2000/svg"
                        viewBox="0 0 32 24.22"
                      >
                        <path
                          d="M16.23 24.22a2 2 0 0 1-.73-.14L7 20.79a2 2 0 0 1-1.29-1.88v-4a1.5 1.5 0 0 1 3 0v3.36l7.54 2.92 7.54-2.92v-3.45a1.5 1.5 0 0 1 3 0v4.09a2 2 0 0 1-1.29 1.88L17 24.08a2 2 0 0 1-.77.14zm-.35-2.94zm.7 0z"
                        ></path>
                        <path
                          d="M16.23 13.35a2 2 0 0 1-.62-.1C9.17 11.16 2.36 9 1.61 8.76a2 2 0 0 1-.25-3.87l14-4.78a2 2 0 0 1 1.3 0l14 4.78a2 2 0 0 1 0 3.81l-13.8 4.55a2 2 0 0 1-.63.1zm-.31-3zM5.21 6.74c3.49 1.11 9.07 2.92 11 3.56l10.68-3.53L16 3.05z"
                        ></path>
                      </svg>
                    </div>
                    <div class="jsx-2576226853 text">Tutorials</div>
                  </a>
                </div>
                <div>
                  <a
                    target="_self"
                    href="../data-science-cheatsheets.html"
                    class="jsx-2576226853 item"
                  >
                    <div class="jsx-2576226853 image">
                      <svg
                        height="14"
                        xmlns="http://www.w3.org/2000/svg"
                        viewBox="0 0 24 26"
                      >
                        <path
                          d="M18.5 26h-13A5.51 5.51 0 0 1 0 20.5v-15A5.51 5.51 0 0 1 5.5 0h13A5.51 5.51 0 0 1 24 5.5v15a5.51 5.51 0 0 1-5.5 5.5zM5.5 3A2.5 2.5 0 0 0 3 5.5v15A2.5 2.5 0 0 0 5.5 23h13a2.5 2.5 0 0 0 2.5-2.5v-15A2.5 2.5 0 0 0 18.5 3z"
                        ></path>
                        <path
                          d="M16 11H8a1.5 1.5 0 0 1 0-3h8a1.5 1.5 0 0 1 0 3zM16 18H8a1.5 1.5 0 0 1 0-3h8a1.5 1.5 0 0 1 0 3z"
                        ></path>
                      </svg>
                    </div>
                    <div class="jsx-2576226853 text">Cheat Sheets</div>
                  </a>
                </div>
                <div>
                  <a
                    target="_self"
                    href="../open-courses.html"
                    class="jsx-2576226853 item"
                  >
                    <div class="jsx-2576226853 image">
                      <svg
                        height="14"
                        xmlns="http://www.w3.org/2000/svg"
                        viewBox="0 0 34 26"
                      >
                        <path
                          d="M28.5 26h-23A5.51 5.51 0 0 1 0 20.5v-15A5.51 5.51 0 0 1 5.5 0h23A5.51 5.51 0 0 1 34 5.5v15a5.51 5.51 0 0 1-5.5 5.5zM5.5 3A2.5 2.5 0 0 0 3 5.5v15A2.5 2.5 0 0 0 5.5 23h23a2.5 2.5 0 0 0 2.5-2.5v-15A2.5 2.5 0 0 0 28.5 3z"
                        ></path>
                        <path
                          d="M13.5 26a1.5 1.5 0 0 1-1.5-1.5v-22a1.5 1.5 0 0 1 3 0v22a1.5 1.5 0 0 1-1.5 1.5zM27 11h-8a1.5 1.5 0 0 1 0-3h8a1.5 1.5 0 0 1 0 3zM27 18h-8a1.5 1.5 0 0 1 0-3h8a1.5 1.5 0 0 1 0 3z"
                        ></path>
                      </svg>
                    </div>
                    <div class="jsx-2576226853 text">Open Courses</div>
                  </a>
                </div>
                <div>
                  <a
                    target="_self"
                    href="../podcast.html"
                    class="jsx-2576226853 item"
                  >
                    <div class="jsx-2576226853 image">
                      <svg
                        height="14"
                        xmlns="http://www.w3.org/2000/svg"
                        width="18"
                        viewBox="0 0 18 18"
                      >
                        <path
                          d="M9.415 11.077h-.369a2.777 2.777 0 0 1-2.769-2.77V2.77A2.777 2.777 0 0 1 9.047 0h.368a2.777 2.777 0 0 1 2.77 2.77v5.538a2.777 2.777 0 0 1-2.77 2.769zm5.008-7.615c.573 0 1.039.464 1.039 1.038v3.462c0 3.08-2.25 5.64-5.193 6.136v1.825h2.077a1.038 1.038 0 1 1 0 2.077h-6.23a1.038 1.038 0 1 1 0-2.077h2.076v-1.825C5.25 13.602 3 11.042 3 7.962V4.5a1.038 1.038 0 1 1 2.077 0v3.462a4.158 4.158 0 0 0 4.154 4.153 4.158 4.158 0 0 0 4.154-4.153V4.5c0-.574.465-1.038 1.038-1.038z"
                        ></path>
                      </svg>
                    </div>
                    <div class="jsx-2576226853 text">Podcast - DataFramed</div>
                  </a>
                </div>
                <div>
                  <a
                    target="_self"
                    href="../chat.html"
                    class="jsx-2576226853 item"
                  >
                    <div class="jsx-2576226853 image">
                      <svg
                        height="14"
                        xmlns="http://www.w3.org/2000/svg"
                        viewBox="0 0 18 16"
                      >
                        <g transform="translate(0 -1)">
                          <path
                            id="path-1"
                            d="M12.595 13.364c.01-.111.02-.197.028-.251.058-.405.372-.702.74-.702h1.257c1.035-.002 1.875-.934 1.88-2.082V4.764c-.001-1.155-.842-2.092-1.878-2.094H3.38c-1.034.002-1.873.931-1.88 2.076v5.565c.001 1.156.842 2.092 1.878 2.094h6.626c.292 0 .557.189.68.484.408.977 1.07 1.576 1.94 1.85a6.004 6.004 0 0 1-.03-1.375h.001zm1.51 1.119c.048.314.136.521.235.606.566.487.258 1.497-.458 1.497-1.87 0-3.423-.785-4.33-2.51H3.376C1.513 14.07.004 12.39 0 10.311V4.74C.014 2.673 1.52 1.004 3.378 1h11.245c1.864.004 3.373 1.686 3.377 3.763v5.57c-.01 2.07-1.518 3.744-3.378 3.748h-.551c.004.138.016.273.035.402h-.001zm-8.423-5.81a1.115 1.115 0 1 0 0-2.229 1.115 1.115 0 0 0 0 2.229zm3.268 0a1.115 1.115 0 1 0 0-2.229 1.115 1.115 0 0 0 0 2.229zm3.318 0a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229z"
                          ></path>
                        </g>
                      </svg>
                    </div>
                    <div class="jsx-2576226853 text">Chat</div>
                  </a>
                </div>
              </nav>
            </div>
            <div class="jsx-2576226853 section">
              <h5 class="jsx-2576226853">
                <div class="jsx-2576226853 title">datacamp</div>
              </h5>
              <nav class="jsx-2576226853">
                <div>
                  <a
                    target="_self"
                    href="../blog.html"
                    class="jsx-2576226853 item"
                  >
                    <div class="jsx-2576226853 image">
                      <svg
                        viewBox="0 0 18 18"
                        aria-hidden="false"
                        height="18"
                        role="img"
                        width="18"
                      >
                        <title>DataCamp</title>
                        <path
                          d="M8.122 4.454v4.36l-4.627 2.64V1.799l4.627 2.655zm1.497 9.161v-3.93l6.195-3.535-1.507-.863L9.62 7.96V4.018a.754.754 0 00-.376-.65L3.599.133a1.08 1.08 0 00-1.6.95l.001 11.085a1.082 1.082 0 001.555.976l.042-.022 4.522-2.581v3.516c0 .27.144.517.377.651L14.298 18l1.507-.864-6.186-3.52z"
                          fill="currentColor"
                          fill-rule="evenodd"
                        ></path>
                      </svg>
                    </div>
                    <div class="jsx-2576226853 text">Official Blog</div>
                  </a>
                </div>
                <div>
                  <a
                    target="_self"
                    href="https://www.datacamp.com/resources"
                    class="jsx-2576226853 item"
                  >
                    <div class="jsx-2576226853 image">
                      <svg
                        height="14"
                        xmlns="http://www.w3.org/2000/svg"
                        viewBox="0 0 24 26"
                      >
                        <path
                          d="M18.5 26h-13A5.51 5.51 0 0 1 0 20.5v-15A5.51 5.51 0 0 1 5.5 0h13A5.51 5.51 0 0 1 24 5.5v15a5.51 5.51 0 0 1-5.5 5.5zM5.5 3A2.5 2.5 0 0 0 3 5.5v15A2.5 2.5 0 0 0 5.5 23h13a2.5 2.5 0 0 0 2.5-2.5v-15A2.5 2.5 0 0 0 18.5 3z"
                        ></path>
                        <path
                          d="M16 11H8a1.5 1.5 0 0 1 0-3h8a1.5 1.5 0 0 1 0 3zM16 18H8a1.5 1.5 0 0 1 0-3h8a1.5 1.5 0 0 1 0 3z"
                        ></path>
                      </svg>
                    </div>
                    <div class="jsx-2576226853 text">Resource Center</div>
                  </a>
                </div>
                <div>
                  <a
                    target="_self"
                    href="https://www.datacamp.com/discover/webinars"
                    class="jsx-2576226853 item"
                  >
                    <div class="jsx-2576226853 image">
                      <svg
                        height="14"
                        xmlns="http://www.w3.org/2000/svg"
                        viewBox="0 0 21.75 29.76"
                      >
                        <path
                          d="M15.56 22.57a1.5 1.5 0 0 1-1.5-1.5 7.66 7.66 0 0 1 2.47-5.29 7.38 7.38 0 0 0 2.21-5.31A7.48 7.48 0 0 0 11.28 3h-.82a7.47 7.47 0 0 0-5.2 12.83 7.63 7.63 0 0 1 2.42 5.23 1.5 1.5 0 0 1-3 0 4.65 4.65 0 0 0-1.45-3A10.47 10.47 0 0 1 10.47 0h.82a10.47 10.47 0 0 1 7.28 18 4.68 4.68 0 0 0-1.5 3.08 1.5 1.5 0 0 1-1.51 1.49zM16.5 24.26a5.5 5.5 0 0 1-11 0"
                        ></path>
                        <path
                          d="M10.89 22.56a1.5 1.5 0 0 1-1.5-1.5v-8.84a1.5 1.5 0 0 1 3 0v8.84a1.5 1.5 0 0 1-1.5 1.5z"
                        ></path>
                      </svg>
                    </div>
                    <div class="jsx-2576226853 text">Upcoming Events</div>
                  </a>
                </div>
              </nav>
            </div>
          </div>
          <main class="jsx-1777972523 Main">
            <div class="jsx-588888136 ActionBar">
              <div>
                <div class="jsx-2098455175 ActionBarSearch">
                  <button
                    style="font-weight: normal"
                    class="jsx-1169100422 Button extra noPadding"
                  >
                    <div class="jsx-1169100422 icon">
                      <svg
                        xmlns="http://www.w3.org/2000/svg"
                        viewBox="0 0 20.03 23"
                      >
                        <path
                          d="M10.39 19.29A9.65 9.65 0 1 1 20 9.65a9.66 9.66 0 0 1-9.61 9.64zm0-17.06a7.42 7.42 0 1 0 7.42 7.42 7.43 7.43 0 0 0-7.42-7.42z"
                        ></path>
                        <path
                          d="M1.11 23a1.11 1.11 0 0 1-.89-1.78l4.1-5.47a1.11 1.11 0 1 1 1.78 1.34L2 22.56a1.11 1.11 0 0 1-.89.44z"
                        ></path>
                      </svg>
                    </div>
                    <div class="jsx-1169100422 desktopOnly">Search</div>
                  </button>
                </div>
              </div>
              <div class="jsx-588888136 authBlock">
                <div></div>
                <div class="jsx-3698045554 ActionBarAuth">
                  <div class="jsx-3698045554">
                    <a
                      href="https://www.datacamp.com/users/sign_in?redirect=https://www.datacamp.com/community"
                      class="jsx-3698045554"
                      ><button class="jsx-1169100422 Button border minWidth">
                        <div class="jsx-1169100422">Log in</div>
                      </button></a
                    ><button class="jsx-1169100422 Button primary">
                      <div class="jsx-1169100422">Create Free Account</div>
                    </button>
                  </div>
                </div>
              </div>
            </div>
            <div class="jsx-1615743782 TitleBar">
              <div class="jsx-1615743782 filter">
                <button class="jsx-1169100422 Button iconButton noPadding">
                  <div class="jsx-1169100422 icon">
                    <svg
                      id="Réteg_1"
                      xmlns="http://www.w3.org/2000/svg"
                      viewBox="0 0 7 12"
                    >
                      <path
                        id="path-1_1_"
                        d="M5.9 0c.4 0 .9.3 1 .7s.1.9-.2 1.2L2.7 6l4 4.1c.3.5.3 1.1-.1 1.6s-1.1.4-1.5.1l-4.8-5c-.4-.4-.4-1.2 0-1.6L5.1.3c.2-.2.5-.3.8-.3z"
                      ></path>
                    </svg>
                  </div>
                  <div class="jsx-1169100422 desktopOnly">
                    Back to Tutorials
                  </div>
                </button>
              </div>
              <div class="jsx-1615743782 title">
                <div class="jsx-1582297868 Title">
                  <div class="jsx-1582297868 icon">
                    <svg
                      xmlns="http://www.w3.org/2000/svg"
                      viewBox="0 0 32 24.22"
                    >
                      <path
                        d="M16.23 24.22a2 2 0 0 1-.73-.14L7 20.79a2 2 0 0 1-1.29-1.88v-4a1.5 1.5 0 0 1 3 0v3.36l7.54 2.92 7.54-2.92v-3.45a1.5 1.5 0 0 1 3 0v4.09a2 2 0 0 1-1.29 1.88L17 24.08a2 2 0 0 1-.77.14zm-.35-2.94zm.7 0z"
                      ></path>
                      <path
                        d="M16.23 13.35a2 2 0 0 1-.62-.1C9.17 11.16 2.36 9 1.61 8.76a2 2 0 0 1-.25-3.87l14-4.78a2 2 0 0 1 1.3 0l14 4.78a2 2 0 0 1 0 3.81l-13.8 4.55a2 2 0 0 1-.63.1zm-.31-3zM5.21 6.74c3.49 1.11 9.07 2.92 11 3.56l10.68-3.53L16 3.05z"
                      ></path>
                    </svg>
                  </div>
                  <div class="jsx-1582297868 h1">Tutorials</div>
                </div>
              </div>
              <div class="jsx-1615743782 action"></div>
            </div>
            <div class="jsx-1851940463 Tutorial">
              <div>
                <div>
                  <div></div>
                  <div class="jsx-undefined social__top desktopOnly">
                    <div class="jsx-undefined voteAndSocial">
                      <div class="jsx-undefined">
                        <a href="#comments" class="jsx-765279523 CommentCounter"
                          ><span class="jsx-765279523 icon"
                            ><svg
                              xmlns="http://www.w3.org/2000/svg"
                              viewBox="0 0 18 18"
                            >
                              <path
                                d="M12.595 13.364c.01-.111.02-.197.028-.251.058-.405.372-.702.74-.702h1.257c1.035-.002 1.875-.934 1.88-2.082V4.764c-.001-1.155-.842-2.092-1.878-2.094H3.38c-1.034.002-1.873.931-1.88 2.076v5.565c.001 1.156.842 2.092 1.878 2.094h6.626c.292 0 .557.189.68.484.408.977 1.07 1.576 1.94 1.85a6.004 6.004 0 0 1-.03-1.375zm1.51 1.119c.048.314.136.521.235.606.566.487.258 1.497-.458 1.497-1.87 0-3.423-.785-4.33-2.51H3.376C1.513 14.07.004 12.39 0 10.311V4.74C.014 2.673 1.52 1.004 3.378 1h11.245c1.864.004 3.373 1.686 3.377 3.763v5.57c-.01 2.07-1.518 3.744-3.378 3.748h-.551c.004.138.016.273.035.402zm-8.423-5.81a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229zm3.268 0a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229zm3.318 0a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229z"
                              ></path></svg></span
                          ><span class="jsx-765279523 count">0</span></a
                        >
                        <div class="jsx-1727309017 Upvote">
                          <div class="jsx-1727309017">
                            <div class="jsx-1727309017 normal">
                              <span class="jsx-1727309017 icon"
                                ><svg
                                  xmlns="http://www.w3.org/2000/svg"
                                  width="12"
                                  height="12"
                                  viewBox="0 0 12 12"
                                >
                                  <path d="M1 10L6 0l5 10z"></path></svg></span
                              ><span class="jsx-1727309017 count">22</span>
                            </div>
                            <div class="jsx-1727309017 voted">
                              <span class="jsx-1727309017 icon"
                                ><svg
                                  xmlns="http://www.w3.org/2000/svg"
                                  width="12"
                                  height="12"
                                  viewBox="0 0 12 12"
                                >
                                  <path d="M1 10L6 0l5 10z"></path></svg></span
                              ><span class="jsx-1727309017 count">22</span>
                            </div>
                          </div>
                        </div>
                      </div>
                      <div class="jsx-2027482828 Social vertical">
                        <div class="jsx-2027482828 icons">
                          <a
                            href="https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/cnn-tensorflow-python"
                            target="_blank"
                            rel="noopener noreferrer"
                            class="jsx-2027482828 icon"
                            ><svg
                              height="12"
                              xmlns="http://www.w3.org/2000/svg"
                              viewBox="0 0 11.73 22.58"
                            >
                              <path
                                d="M7.61 22.58v-10.3h3.46l.52-4h-4V5.7c0-1.16.32-2 2-2h2.13V.16A28.47 28.47 0 0 0 8.63 0C5.56 0 3.47 1.87 3.47 5.31v3H0v4h3.47v10.3h4.14z"
                              ></path></svg></a
                          ><a
                            href="https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/cnn-tensorflow-python"
                            target="_blank"
                            rel="noopener noreferrer"
                            class="jsx-2027482828 icon centerIcon"
                            ><svg
                              height="10"
                              xmlns="http://www.w3.org/2000/svg"
                              viewBox="0 0 20.42 16.67"
                            >
                              <path
                                d="M10 5.18c0-.28-.06-.53-.07-.78a4 4 0 0 1 .73-2.57A4.08 4.08 0 0 1 13.93 0 4 4 0 0 1 17 1.15a.43.43 0 0 0 .46.12 8.68 8.68 0 0 0 2.2-.84l.2-.1a4.36 4.36 0 0 1-1.75 2.28A9 9 0 0 0 20.42 2l-.21.3a3.83 3.83 0 0 1-.23.3A8.45 8.45 0 0 1 18.5 4a.28.28 0 0 0-.13.27A12 12 0 0 1 17 10.18a11.8 11.8 0 0 1-3.37 4.11 11.17 11.17 0 0 1-4.39 2.06 12.53 12.53 0 0 1-4.44.22 11.87 11.87 0 0 1-4.74-1.73L0 14.79a8.6 8.6 0 0 0 6.16-1.74 4.28 4.28 0 0 1-3.91-2.91h.95a6.18 6.18 0 0 0 .89-.12A4.2 4.2 0 0 1 .8 5.88a4 4 0 0 0 1.81.49 4.23 4.23 0 0 1-1.78-3A4.07 4.07 0 0 1 1.38.79 12.06 12.06 0 0 0 10 5.18z"
                                id="iOjKBC.tif"
                              ></path></svg></a
                          ><a
                            href="https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/cnn-tensorflow-python"
                            target="_blank"
                            rel="noopener noreferrer"
                            class="jsx-2027482828 icon"
                            ><svg
                              height="10"
                              xmlns="http://www.w3.org/2000/svg"
                              viewBox="0 0 16.99 17"
                            >
                              <path
                                d="M3.85 17H.34V5.67h3.51zM2.07 4.18a2.09 2.09 0 1 1 2.08-2.09 2.08 2.08 0 0 1-2.08 2.09zM17 17h-3.5v-5.95c0-1.63-.62-2.54-1.91-2.54s-2.14.95-2.14 2.54V17H6.09V5.67h3.36v1.52a4 4 0 0 1 3.42-1.87c2.4 0 4.12 1.47 4.12 4.5V17z"
                              ></path></svg
                          ></a>
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
                <div class="jsx-1851940463 preface">
                  <div class="jsx-1851940463 author">
                    <div class="jsx-886169423 Author">
                      <a
                        href="https://www.datacamp.com/profile/adityasharma101993"
                        target="_blank"
                        rel="noopener noreferrer"
                        class="jsx-886169423"
                      >
                        <div
                          style="
                            background-image: url(https://res.cloudinary.com/dyd911kmh/image/fetch/t_avatar_thumbnail/https://assets.datacamp.com/users/avatars/000/701/074/square/29872933_10210392357919457_2437555204118650647_o.jpg?1544687015);
                            border-radius: 20px;
                            min-width: 40px;
                            min-height: 40px;
                          "
                          class="jsx-1670412252 Avatar"
                        ></div>
                        <div class="jsx-886169423 info">
                          <div class="jsx-886169423 name">Aditya Sharma</div>
                          <div class="jsx-886169423 date">
                            <span>June 8th, 2020</span>
                          </div>
                        </div>
                      </a>
                    </div>
                  </div>
                  <div class="jsx-1851940463 tags">
                    <div class="jsx-3846131586 TagLine">
                      <div class="jsx-3073944448 Tag mustRead">
                        <span class="jsx-3073944448 title">must read</span>
                      </div>
                      <div class="jsx-3073944448 Tag">
                        <span class="jsx-3073944448 title">python</span>
                      </div>
                      <a class="jsx-3814997685 more"
                        >+
                        <!-- -->2
                      </a>
                    </div>
                  </div>
                  <h1 class="jsx-1851940463 pageTitle">
                    Convolutional Neural Networks with TensorFlow
                  </h1>
                  <div class="jsx-1851940463 description pageDescription">
                    In this tutorial, you&#x27;ll learn how to construct and
                    implement Convolutional Neural Networks (CNNs) in Python
                    with the TensorFlow framework.
                  </div>
                </div>
                <div class="markdown">
                  <div>
                    <center>
                      <a
                        target="“_blank”"
                        href="https://www.datacamp.com/learn/python/"
                        ><img
                          src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1583330891/python_2_wyjjwk.png"
                      /></a>
                    </center>
                    <br />

                    <p>
                      TensorFlow is a popular deep learning framework. In this
                      tutorial, you will learn the basics of this Python library
                      and understand how to implement these deep, feed-forward
                      artificial neural networks with it.
                    </p>

                    <p>
                      To be precise, you'll be introduced to the following
                      topics in today's tutorial:
                    </p>

                    <p>
                      - You'll be first introduced to
                      <a href="#tensors">tensors</a> and how they differ from
                      matrices; Once you understand what tensors are then,
                      you'll be introduced to the
                      <a href="#Tensorflow">Tensorflow</a> Framework, within
                      this you will also see that how even a single line of code
                      is implemented via a
                      <a href="#computational">computational graph</a> in
                      TensorFlow, then you will learn about some of the
                      package's concepts that play a major role in you to do
                      deep learning like
                      <a href="#constants"
                        >constants, variables, and placeholders</a
                      >.
                    </p>

                    <p>
                      - Then, you'll be headed to the most interesting part of
                      this tutorial. That is the implementation of the
                      <a href="#cnn">Convolutional Neural Network</a>: first,
                      you will try to understand the data. You'll use Python and
                      its libraries to load, explore, and analyze your data.
                      You'll also preprocess your data: you’ll learn how to
                      visualize your images as a matrix, reshape your data and
                      rescale the images between 0 and 1 if required.
                    </p>

                    <p>
                      - With all of this done, you are ready to
                      <a href="#construct"
                        >construct the deep neural network model</a
                      >. You'll start by defining the network parameters, then
                      learn how to create wrappers to increase the simplicity of
                      your code, define weights and biases, model the network,
                      define loss and optimizer nodes. Once you have all this in
                      place, you are ready for
                      <a href="#training">training and testing your model</a>.
                    </p>

                    <p>
                      - Finally, you will learn to work with your
                      <a href="#own">own dataset</a>. In this section, you would
                      download the CIFAR-10 dataset from Kaggle, load the images
                      and labels using Python modules like
                      <code>glob</code> &amp; <code>pandas</code>. You will read
                      the images using <code>OpenCV</code>, one-hot the class
                      labels, visualize the images with labels, normalize the
                      images, and finally split the dataset into train and test
                      set.
                    </p>

                    <h2 id="tensors">Tensors</h2>

                    <a id="tensors"></a>

                    <p>
                      In layman's terms, a tensor is a way of representing the
                      data in deep learning. A tensor can be a 1-dimensional, a
                      2-dimensional, a 3-dimensional array, etc. You can think
                      of a tensor as a multidimensional array. In machine
                      learning and deep learning, you have datasets that are
                      high dimensional, in which each dimension represents a
                      different feature of that dataset.
                    </p>

                    <p>
                      Consider the following example of a dog versus cat
                      classification problem, where the dataset you're working
                      with has multiple varieties of both cats and dogs images.
                      Now, in order to correctly classify a dog or a cat when
                      given an image, the network has to learn discriminative
                      features like color, face structure, ears, eyes, the shape
                      of the tail, etc.
                    </p>

                    <p>These features are incorporated by the tensors.</p>

                    <center>
                      <img
                        src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1591576676/Screen_Shot_2020-06-07_at_8.36.33_PM_yqpppc.png"
                      />
                    </center>

                    <p>
                      <strong>Tip</strong>: if you want to learn more about
                      tensors, check out DataCamp's
                      <a href="tensorflow-tutorial.html"
                        >TensorFlow Tutorial for Beginners</a
                      >.
                    </p>
                    <p>
                      But how are tensors then any different from matrices?
                      You'll find out in the next section!
                    </p>
                    <h3 id="tensors-versus-matrices-differences">
                      Tensors versus Matrices: Differences
                    </h3>
                    <p>
                      A matrix is a two-dimensional grid of size $n×m$ that
                      contains numbers: you can add and subtract matrices of the
                      same size, multiply one matrix with another as long as the
                      sizes are compatible $((n×m)×(m×p)=n×p)$, and multiply an
                      entire matrix by a constant.
                    </p>
                    <p>
                      A vector is a matrix with just one row or column (but see
                      below).
                    </p>
                    <p>
                      A tensor is often thought of as a generalized matrix. That
                      is, it could be
                    </p>
                    <ul>
                      <li>
                        <p>
                          a 1-D matrix, like a vector, which is actually such a
                          tensor,
                        </p>
                      </li>
                      <li>
                        <p>a 3-D matrix (something like a cube of numbers),</p>
                      </li>
                      <li>
                        <p>a 0-D matrix (a single number), or</p>
                      </li>
                      <li>
                        <p>
                          a higher dimensional structure that is harder to
                          visualize.
                        </p>
                      </li>
                    </ul>
                    <p>The dimension of the tensor is called its rank.</p>
                    <p>
                      Any rank-2 tensor can be represented as a matrix, but not
                      every matrix is a rank-2 tensor. The numerical values of a
                      tensor’s matrix representation depend on what
                      transformation rules have been applied to the entire
                      system.
                    </p>
                    <h2 id="tensorflow-constants-variables-and-placeholders">
                      TensorFlow: Constants, Variables, and Placeholders
                    </h2>
                    <p><a id="Tensorflow"></a></p>
                    <p>
                      TensorFlow is a framework developed by Google on 9th
                      November 2015. It is written in Python, C++, and Cuda. It
                      supports platforms like Linux, Microsoft Windows, macOS,
                      and Android. TensorFlow provides multiple APIs in Python,
                      C++, Java, etc. It is the most widely used API in Python,
                      and you will implement a convolutional neural network
                      using Python API in this tutorial.
                    </p>
                    <p>
                      The name TensorFlow is derived from the operations, such
                      as adding or multiplying, that artificial neural networks
                      perform on multidimensional data arrays. These arrays are
                      called tensors in this framework, which is slightly
                      different from what you saw earlier.
                    </p>
                    <p>
                      So why is there a mention of a flow when you're talking
                      about operations?
                    </p>
                    <p>
                      Let's consider a simple equation and its diagram,
                      represented as a computational graph. Note: don't worry if
                      you don't get this equation straight away, this is just to
                      help you to understand how the flow takes place while
                      using the TensorFlow framework.
                    </p>
                    <p>
                      <a id="computational"></a> prediction =
                      tf.nn.softmax(tf.matmul(W,x) + b)
                    </p>
                    <center>
                      <img
                        src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1591576676/Screen_Shot_2020-06-07_at_8.36.57_PM_asaftw.png"
                      />
                    </center>

                    <p>
                      In TensorFlow, every line of code that you write has to go
                      through a computational graph. As in the above figure, you
                      can see that first $W$ and $x$ get multiplied. Then comes
                      b, which is added to the output of $W$ and $x$. After
                      adding the output of $W$ and $x$ with $b$, a softmax
                      function is applied, and the final output is generated.
                    </p>
                    <p><a id="constants"></a></p>
                    <p>
                      You'll find that when you're working with TensorFlow,
                      constants, variables, and placeholders come handy to
                      define the input data, class labels, weights, and biases.
                    </p>
                    <ul>
                      <li>
                        <strong>Constant</strong> takes no input, you use them
                        to store constant values. They produce a constant output
                        that it stores.
                      </li>
                    </ul>
                    <pre><code class="lang-python">import tensorflow as tf
a = tf.constant(2.0)
b = tf.constant(3.0)
c = a * b
</code></pre>
                    <p>
                      Here, nodes <code>a</code> and <code>b</code> are
                      constants that store values <code>2.0</code> and
                      <code>3.0</code>. Node <code>c</code> stores the operation
                      that multiplies the nodes <code>a</code> and
                      <code>b</code>, respectively. When you initialize a
                      session and run <code>c</code>, you'll see that the output
                      that you get back is <code>6.0</code>:
                    </p>
                    <pre><code class="lang-python">sess = tf.Session()
sess.run(c)
</code></pre>
                    <pre><code>6.0
</code></pre>
                    <ul>
                      <li>
                        <strong>Placeholders</strong> allow you to feed input on
                        the run. Because of this flexibility, placeholders are
                        used, which allows your computational graph to take
                        inputs as parameters. Defining a node as a placeholder
                        assures that node, that it is expected to receive a
                        value later or during runtime. Here, "runtime" means
                        that the input is fed to the placeholder when you run
                        your computational graph.
                      </li>
                    </ul>
                    <pre><code class="lang-python"># Creating placeholders
a = tf.placeholder(tf.float32)
b = tf.placeholder(tf.float32)

# Assigning addition operation w.r.t. a and b to node add
add = a + b

# Create session object
sess = tf.Session()

# Executing add by passing the values [1, 3] [2, 4] for a and b respectively
output = sess.run(add, {a: [1,3], b: [2, 4]})
print('Adding a and b:', output)
print('Datatype:', output.dtype)
</code></pre>
                    <pre><code>Adding a and b: [3. 7.]
Datatype: float32
</code></pre>
                    <p>
                      In this case, you have explicitly provided the data type
                      with <code>tf.float32</code>. Note that this data type is,
                      therefore, a single-precision, which is stored in 32 bits
                      form. However, in cases where you do not do this, just
                      like in the first example, TensorFlow will infer the type
                      of the constant/variable from the initialized value.
                    </p>
                    <center>
                      <img
                        src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1591576676/Screen_Shot_2020-06-07_at_8.37.08_PM_fjym5c.png"
                      />
                    </center>

                    <p>
                      <strong>Variables</strong> allow you to modify the graph
                      such that it can produce new outputs with respect to the
                      same inputs. A variable allows you to add such parameters
                      or nodes to the graph that are trainable. That is, the
                      value can be modified throughout time.
                    </p>
                    <pre><code class="lang-python">#Variables are defined by providing their initial value and type
variable = tf.Variable([0.9,0.7], dtype = tf.float32)

#variable must be initialized before a graph is used for the first time.
init = tf.global_variables_initializer()
sess.run(init)
</code></pre>
                    <p>
                      Constants are initialized when you call
                      <code>tf.constant</code>, and their value can never
                      change. But, variables are not initialized when you call
                      <code>tf.Variable</code>. To initialize all the variables
                      in TensorFlow, you need to explicitly call the global
                      variable initializer
                      <code>global_variables_initializer()</code>, which
                      initializes all the existing variables in your TensorFlow
                      code, as you can see in the above code chunk.
                    </p>
                    <p>
                      Variables survive across multiple executions of a graph,
                      unlike normal tensors that are only instantiated when a
                      graph is run and are immediately deleted afterward.
                    </p>
                    <p>
                      In this section, you have seen that placeholders are used
                      for holding the input data and class labels, whereas
                      variables are used for weights and biases. Don't worry if
                      you have still not been able to develop proper intuition
                      about how a computational graph works or for what
                      placeholders and variables typically used for in deep
                      learning. You will address all these topics later on in
                      this tutorial.
                    </p>
                    <h2 id="convolutional-neural-network-cnn-in-tensorflow">
                      Convolutional Neural Network (CNN) in TensorFlow
                    </h2>
                    <p><a id="cnn"></a></p>
                    <h3 id="fashion-mnist-dataset">Fashion-MNIST Dataset</h3>
                    <p>
                      Before you go ahead and load in the data, it's good to
                      take a look at what you'll exactly be working with! The
                      <a href="https://arxiv.org/abs/1708.07747"
                        >Fashion-MNIST</a
                      >
                      dataset contains Zalando's article images, with 28x28
                      grayscale images of 65,000 fashion products from 10
                      categories, and 6,500 images per category. The training
                      set has 55,000 images, and the test set has 10,000 images.
                      You can double-check this later when you have loaded in
                      your data! ;)
                    </p>
                    <p>
                      Fashion-MNIST is similar to the MNIST dataset that you
                      might already know, which you use to classify handwritten
                      digits. That means that the image dimensions, training,
                      and test splits are similar.
                    </p>
                    <p>
                      Tip: if you want to learn how to implement a Multi-Layer
                      Perceptron (MLP) for classification tasks with this latter
                      dataset,
                      <a href="deep-learning-python.html">go to this tutorial</a
                      >, or if you want to learn about convolutional neural
                      networks and its implementation in a Keras framework,
                      check out
                      <a href="convolutional-neural-networks-python.html"
                        >this tutorial</a
                      >.
                    </p>
                    <p>
                      You can find the Fashion-MNIST dataset
                      <a href="https://github.com/zalandoresearch/fashion-mnist"
                        >here</a
                      >. Unlike the Keras or Scikit-Learn packages, TensorFlow
                      has no predefined module to load the Fashion MNIST
                      dataset, though it has an MNIST dataset by default. To
                      load the data, you first need to download the data from
                      the above link and then structure the data in a particular
                      folder format, as shown below, to be able to work with it.
                      Otherwise, Tensorflow will download and use the original
                      MNIST.
                    </p>
                    <h3 id="load-the-data">Load the data</h3>
                    <p>
                      You first start with importing all the required modules
                      like NumPy, matplotlib, and, most importantly, Tensorflow.
                    </p>
                    <pre><code class="lang-python"># Import libraries
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
%matplotlib inline
import os
# os.environ["CUDA_VISIBLE_DEVICES"]="0" #for training on gpu
</code></pre>
                    <p>
                      After importing all the modules, you will now learn how to
                      load data in TensorFlow, which should be pretty
                      straightforward. The only thing that you should take into
                      account is the <code>one_hot=True</code> argument, which
                      you'll also find in the line of code below: it converts
                      the categorical class labels to binary vectors.
                    </p>
                    <p>
                      In one-hot encoding, you convert the categorical data into
                      a vector of numbers. You do this because machine learning
                      algorithms can't work with categorical data directly.
                      Instead, you generate one boolean column for each category
                      or class. Only one of these columns could take on the
                      value 1 for each sample. That explains the term "one-hot
                      encoding".
                    </p>
                    <p>
                      But what does such a one-hot encoded data column look
                      like?
                    </p>
                    <p>
                      For your problem statement, the one-hot encoding will be a
                      row vector, and for each image, it will have a dimension
                      of 1 x 10. It's important to note here that the vector
                      consists of all zeros except for the class that it
                      represents. There, you'll find a 1. For example, the ankle
                      boot image that you plotted above has a label of 9, so for
                      all the ankle boot images, the one-hot encoding vector
                      would be [0 0 0 0 0 0 0 0 0 1].
                    </p>
                    <p>
                      Now that all of this is clear, it's time to import the
                      data!
                    </p>
                    <pre><code class="lang-python">data = input_data.read_data_sets('data/fashion',one_hot=True,\
                                 source_url='http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/')
</code></pre>
                    <pre><code>WARNING:tensorflow:From /Users/adityasharma/Library/Python/3.7/lib/python/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.&lt;locals&gt;.wrap.&lt;locals&gt;.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Please use urllib or similar directly.
Successfully downloaded train-images-idx3-ubyte.gz 26421880 bytes.
Extracting data/fashion1/train-images-idx3-ubyte.gz
Successfully downloaded train-labels-idx1-ubyte.gz 29515 bytes.
Extracting data/fashion1/train-labels-idx1-ubyte.gz
Successfully downloaded t10k-images-idx3-ubyte.gz 4422102 bytes.
Extracting data/fashion1/t10k-images-idx3-ubyte.gz
Successfully downloaded t10k-labels-idx1-ubyte.gz 5148 bytes.
Extracting data/fashion1/t10k-labels-idx1-ubyte.gz
</code></pre>
                    <p>
                      <strong>Note</strong>: If you have trouble loading the
                      <code>Fashion-MNIST</code> dataset with the above method,
                      kindly refer to
                      <a href="https://github.com/zalandoresearch/fashion-mnist"
                        >this</a
                      >
                      repository, which shows a handful of ways in which you can
                      load your dataset.
                    </p>
                    <p>
                      Once you have the training and testing data loaded, you're
                      all set to analyze the data to get some intuition about
                      the dataset that you are going to work with for this
                      tutorial!
                    </p>
                    <h3 id="analyze-the-data">Analyze the Data</h3>
                    <p>
                      Before you start any heavy lifting, it's always a good
                      idea to check out what the images in the dataset look
                      like. First, you can take a programmatical approach and
                      check out their dimensions. Also, take into account that
                      if you want to explore your images, these have already
                      been rescaled between 0 and 1. That means that you would
                      not need to rescale the image pixels again!
                    </p>
                    <pre><code class="lang-python"># Shapes of training set
print("Training set (images) shape: {shape}".format(shape=data.train.images.shape))
print("Training set (labels) shape: {shape}".format(shape=data.train.labels.shape))

# Shapes of test set
print("Test set (images) shape: {shape}".format(shape=data.test.images.shape))
print("Test set (labels) shape: {shape}".format(shape=data.test.labels.shape))
</code></pre>
                    <pre><code>Training set (images) shape: (55000, 784)
Training set (labels) shape: (55000, 10)
Test set (images) shape: (10000, 784)
Test set (labels) shape: (10000, 10)
</code></pre>
                    <p>
                      From the above output, you can see that the training data
                      has a shape of 55000 x 784: there are 55,000 training
                      samples each of the 784-dimensional vector. Similarly, the
                      test data has a shape of 10000 x 784, since there are
                      10,000 testing samples.
                    </p>
                    <p>
                      The 784-dimensional vector is nothing but a 28 x
                      28-dimensional matrix. That's why you will be reshaping
                      each training and testing sample from a 784-dimensional
                      vector to a 28 x 28 x 1-dimensional matrix in order to
                      feed the samples into the CNN model.
                    </p>
                    <p>
                      For simplicity, let's create a dictionary that will have
                      class names with their corresponding categorical class
                      labels.
                    </p>
                    <pre><code class="lang-python"># Create dictionary of target classes
label_dict = {
 0: 'T-shirt/top',
 1: 'Trouser',
 2: 'Pullover',
 3: 'Dress',
 4: 'Coat',
 5: 'Sandal',
 6: 'Shirt',
 7: 'Sneaker',
 8: 'Bag',
 9: 'Ankle boot',
}
</code></pre>
                    <p>
                      Also, let's take a look at a couple of images in the
                      dataset:
                    </p>
                    <pre><code class="lang-python">plt.figure(figsize=[5,5])

# Display the first image in training data
plt.subplot(121)
curr_img = np.reshape(data.train.images[0], (28,28))
curr_lbl = np.argmax(data.train.labels[0,:])
plt.imshow(curr_img, cmap='gray')
plt.title("(Label: " + str(label_dict[curr_lbl]) + ")")

# Display the first image in testing data
plt.subplot(122)
curr_img = np.reshape(data.test.images[0], (28,28))
curr_lbl = np.argmax(data.test.labels[0,:])
plt.imshow(curr_img, cmap='gray')
plt.title("(Label: " + str(label_dict[curr_lbl]) + ")")
</code></pre>
                    <pre><code>Text(0.5, 1.0, '(Label: Ankle boot)')
</code></pre>
                    <center>
                      <img
                        src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1591576245/output_22_1_ylcf7u.png"
                      />
                    </center>

                    <p>
                      The output of the above two plots is one of the sample
                      images from both training and testing data, and these
                      images are assigned a class label of 4 (Coat) and 9 (Ankle
                      boot). Similarly, other fashion products will have
                      different labels, but similar products will have the same
                      labels. This means that all the 6,500 ankle boot images
                      will have a class label of 9.
                    </p>
                    <h3 id="data-preprocessing">Data Preprocessing</h3>
                    <p>
                      The images are of size 28 x 28 (or a 784-dimensional
                      vector).
                    </p>
                    <p>
                      The images are already rescaled between 0 and 1, so you
                      don't need to rescale them again, but to be sure, let's
                      visualize an image from the training dataset as a matrix.
                      Along with that, let's also print the maximum and minimum
                      value of the matrix.
                    </p>
                    <pre><code class="lang-python">data.train.images[0][500:]
</code></pre>
                    <pre><code>array([0.40784317, 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.03921569, 0.9568628 ,
       0.8588236 , 0.9803922 , 0.80392164, 0.7803922 , 0.8196079 ,
       0.79215693, 0.8196079 , 0.82745105, 0.7411765 , 0.83921576,
       0.8078432 , 0.8235295 , 0.7843138 , 0.8313726 , 0.6039216 ,
       0.94117653, 0.81568635, 0.8588236 , 0.54901963, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.08235294, 1.        , 0.8705883 , 0.9333334 ,
       0.72156864, 0.8235295 , 0.75294125, 0.8078432 , 0.8196079 ,
       0.8235295 , 0.7411765 , 0.8352942 , 0.82745105, 0.8196079 ,
       0.75294125, 0.8941177 , 0.60784316, 0.8862746 , 0.9333334 ,
       0.9450981 , 0.6509804 , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.14509805,
       0.9607844 , 0.8862746 , 0.9450981 , 0.5882353 , 0.7725491 ,
       0.7411765 , 0.8000001 , 0.8196079 , 0.8235295 , 0.7176471 ,
       0.8352942 , 0.8352942 , 0.78823537, 0.72156864, 0.8431373 ,
       0.57254905, 0.8470589 , 0.92549026, 0.882353  , 0.6039216 ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.227451  , 0.93725497, 0.89019614,
       1.        , 0.61960787, 0.7568628 , 0.76470596, 0.8000001 ,
       0.8196079 , 0.8352942 , 0.7058824 , 0.8117648 , 0.85098046,
       0.7803922 , 0.7607844 , 0.82745105, 0.61960787, 0.8588236 ,
       0.92549026, 0.8470589 , 0.5921569 , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.26666668, 0.91372555, 0.8862746 , 0.95294124, 0.54509807,
       0.7843138 , 0.7568628 , 0.80392164, 0.8235295 , 0.81568635,
       0.7058824 , 0.80392164, 0.8313726 , 0.7960785 , 0.7686275 ,
       0.8470589 , 0.6156863 , 0.7019608 , 1.        , 0.8470589 ,
       0.60784316, 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.31764707, 0.882353  ,
       0.87843144, 0.82745105, 0.5411765 , 0.8588236 , 0.7254902 ,
       0.78823537, 0.8352942 , 0.8117648 , 0.7725491 , 0.8862746 ,
       0.8313726 , 0.7843138 , 0.74509805, 0.8431373 , 0.7176471 ,
       0.3529412 , 1.        , 0.82745105, 0.5764706 , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.35686275, 0.8235295 , 0.90196085, 0.61960787,
       0.44705886, 0.80392164, 0.73333335, 0.81568635, 0.8196079 ,
       0.8078432 , 0.7568628 , 0.8235295 , 0.82745105, 0.8000001 ,
       0.76470596, 0.8000001 , 0.70980394, 0.09019608, 1.        ,
       0.8352942 , 0.61960787, 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.34117648,
       0.80392164, 0.909804  , 0.427451  , 0.6431373 , 1.        ,
       0.83921576, 0.87843144, 0.8705883 , 0.8235295 , 0.7725491 ,
       0.83921576, 0.882353  , 0.8705883 , 0.82745105, 0.86274517,
       0.85098046, 0.        , 0.9176471 , 0.8470589 , 0.6627451 ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.36078432, 0.8352942 , 0.909804  ,
       0.57254905, 0.01960784, 0.5254902 , 0.5921569 , 0.63529414,
       0.6666667 , 0.7176471 , 0.7137255 , 0.6431373 , 0.6509804 ,
       0.69803923, 0.63529414, 0.6117647 , 0.38431376, 0.        ,
       0.94117653, 0.882353  , 0.8235295 , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.16862746, 0.6431373 , 0.8078432 , 0.5529412 , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.49803925, 0.4901961 ,
       0.29803923, 0.        , 0.        , 0.        ], dtype=float32)
</code></pre>
                    <pre><code class="lang-python">np.max(data.train.images[0])
</code></pre>
                    <pre><code>1.0
</code></pre>
                    <pre><code class="lang-python">np.min(data.train.images[0])
</code></pre>
                    <pre><code>0.0
</code></pre>
                    <p>
                      Let us reshape the images so that it's of size 28 x 28 x
                      1, and feed this as an input to the network.
                    </p>
                    <p>
                      The reason you need to reshape your data is that
                      Tensorflow expects a certain input shape for its Deep
                      Learning Model, i.e., in this case, a
                      <code>Convolution Neural Network</code>, specifically:
                    </p>
                    <p>
                      <code
                        >(&lt;number of images&gt;, &lt;image x_dim&gt;,
                        &lt;image y_dim&gt;, &lt;number of channels&gt;)</code
                      >
                    </p>
                    <p>
                      The dataset class used here apparently yields a flattened
                      (list-like) shape for these images, so the reshape command
                      puts the data structure into a type that the TF class can
                      work with.
                    </p>
                    <pre><code class="lang-python"># Reshape training and testing image
train_X = data.train.images.reshape(-1, 28, 28, 1)
test_X = data.test.images.reshape(-1,28,28,1)
</code></pre>
                    <pre><code class="lang-python">train_X.shape, test_X.shape
</code></pre>
                    <pre><code>((55000, 28, 28, 1), (10000, 28, 28, 1))
</code></pre>
                    <p>
                      You need not reshape the labels since they already have
                      the correct dimensions, but let us put the training and
                      testing labels in separate variables and also print their
                      respective shapes just to be on the safer side.
                    </p>
                    <pre><code class="lang-python">train_y = data.train.labels
test_y = data.test.labels
</code></pre>
                    <pre><code class="lang-python">train_y.shape, test_y.shape
</code></pre>
                    <pre><code>((55000, 10), (10000, 10))
</code></pre>
                    <h2 id="the-deep-neural-network">
                      The Deep Neural Network
                    </h2>
                    <p><a id="construct"></a></p>
                    <p>You'll use three convolutional layers:</p>
                    <ul>
                      <li>The first layer will have 32-3 x 3 filters,</li>
                    </ul>
                    <ul>
                      <li>The second layer will have 64-3 x 3 filters and</li>
                    </ul>
                    <ul>
                      <li>The third layer will have 128-3 x 3 filters.</li>
                    </ul>
                    <p>
                      In addition, there are three max-pooling layers, each of
                      the size 2 x 2.
                    </p>
                    <center>
                      <img
                        src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1591576676/Screen_Shot_2020-06-07_at_8.37.21_PM_kytyha.png"
                      />
                    </center>

                    <p>
                      You start with defining the training iterations
                      <code>training_iters</code>, the learning rate
                      <code>learning_rate</code>, and the batch size
                      <code>batch_size</code>. Keep in mind that all these are
                      hyperparameters and that these don't have fixed values, as
                      these differ for every problem statement.
                    </p>
                    <p>Nevertheless, here's what you usually can expect:</p>
                    <ul>
                      <li>
                        Training iterations indicate the number of times you
                        train your network,
                      </li>
                    </ul>
                    <ul>
                      <li>
                        It is a good practice to use a learning rate of 1e-3,
                        the learning rate is a factor that is multiplied with
                        the weights based on which the weights get updated, and
                        this indeed helps in reducing the
                        cost/loss/cross-entropy and ultimately in converging or
                        reaching the local optima. The learning rate should
                        neither be too high or too low it should be a balanced
                        rate.
                      </li>
                    </ul>
                    <ul>
                      <li>
                        The batch size means that your training images will be
                        divided into a fixed batch size, and at every batch, it
                        will take a fixed number of images and train them. It's
                        recommended to use a batch size in the power of 2. Since
                        the number of the physical processor is often a power of
                        2, using several virtual processors different from a
                        power of 2 leads to poor performance. Also, taking a
                        very large batch size can lead to memory errors, so you
                        have to make sure that the machine you run your code on
                        has sufficient RAM to handle specified batch size.
                      </li>
                    </ul>
                    <pre><code class="lang-python">training_iters = 10
learning_rate = 0.001
batch_size = 128
</code></pre>
                    <h2 id="network-parameters">Network Parameters</h2>
                    <p>
                      Next, you need to define the network parameters. Firstly,
                      you define the number of inputs. This is 784 since the
                      image is initially loaded as a 784-dimensional vector.
                      Later, you will see that how you will reshape the
                      784-dimensional vector to a 28 x 28 x 1 matrix. Secondly,
                      you'll also define the number of classes, which is nothing
                      else than the number of class labels.
                    </p>
                    <pre><code class="lang-python"># MNIST data input (img shape: 28*28)
n_input = 28

# MNIST total classes (0-9 digits)
n_classes = 10
</code></pre>
                    <p>
                      Now is the time to use those placeholders, about which you
                      read previously in this tutorial. You will define an input
                      placeholder <code>x</code>, which will have a dimension of
                      <code>None x 784</code> and the output placeholder with a
                      dimension of <code>None x 10</code>. To reiterate,
                      placeholders allow you to do operations and build your
                      computation graph without feeding in data.
                    </p>
                    <p>
                      Similarly, y will hold the label of the training images in
                      the form matrix, which will be a
                      <code>None*10 matrix</code>.
                    </p>
                    <p>
                      The row dimension is <code>None</code>. That's because you
                      have defined <code>batch_size</code>, which tells
                      placeholders that they will receive this dimension when
                      you feed in the data to them. Since you set the batch size
                      to 128, this will be the row dimension of the
                      placeholders.
                    </p>
                    <pre><code class="lang-python">#both placeholders are of type float
x = tf.placeholder("float", [None, 28,28,1])
y = tf.placeholder("float", [None, n_classes])
</code></pre>
                    <h3 id="creating-wrappers-for-simplicity">
                      Creating wrappers for simplicity
                    </h3>
                    <p>
                      In your network architecture model, you will have multiple
                      convolution and max-pooling layers. In such cases, it's
                      always a better idea to define convolution and max-pooling
                      functions, so that you can call them as many times you
                      want to use them in your network.
                    </p>
                    <ul>
                      <li>
                        In the <code>conv2d()</code> function, you pass 4
                        arguments: input <code>x</code>, weights <code>W</code>,
                        bias <code>b</code>, and <code>strides</code>. This last
                        argument is by default set to 1, but you can always play
                        with it to see how the network performs. The first and
                        last stride must always be 1 because the first is for
                        the image-number, and the last is for the input-channel
                        (since the image is a gray-scale image which has only
                        one channel). After applying the convolution, you will
                        add bias and apply an activation function called
                        Rectified Linear Unit (ReLU).
                      </li>
                    </ul>
                    <ul>
                      <li>
                        The max-pooling function is simple: it has the input
                        <code>x</code> and a kernel size <code>k</code>, which
                        is set to be 2. This means that the max-pooling filter
                        will be a square matrix with dimensions 2 x 2, and the
                        stride by which the filter will move in is also 2.
                      </li>
                    </ul>
                    <p>
                      You will be padding equal to the same which ensures that
                      while performing the convolution operations, the boundary
                      pixels of the image are not left out, so padding equal to
                      same will basically add zeros at the boundaries of the
                      input and allow the convolution filter to access the
                      boundary pixels as well.
                    </p>
                    <p>
                      Similarly, max-pooling operation padding equal to the same
                      will add zeros. Later, when you define the weights and the
                      biases, you will notice that input of size 28 x 28 is
                      downsampled to 4 x 4 after applying three max-pooling
                      layers.
                    </p>
                    <pre><code class="lang-python">def conv2d(x, W, b, strides=1):
    # Conv2D wrapper, with bias and relu activation
    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')
    x = tf.nn.bias_add(x, b)
    return tf.nn.relu(x)

def maxpool2d(x, k=2):
    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],padding='SAME')
</code></pre>
                    <p>
                      After you have defined the <code>conv2d</code> and
                      <code>maxpool2d</code> wrappers, now you can define your
                      weights and biases variables. So, let's get started!
                    </p>
                    <p>
                      But first, let's understand each weight and bias parameter
                      step by step. You will create two dictionaries, one for
                      weight and the second for the bias parameter.
                    </p>
                    <ul>
                      <li>
                        If you can recall from the above figure that the first
                        convolution layer has 32-3x3 filters, so the first key
                        (<code>wc1</code>) in the weight dictionary has an
                        argument <code>shape</code> that takes a tuple with 4
                        values: the first and second are the filter size, while
                        the third is the number of channels in the input image
                        and the last represents the number of convolution
                        filters you want in the first convolution layer. The
                        first key in the <code>biases</code> dictionary,
                        <code>bc1</code>, will have 32 bias parameters.
                      </li>
                    </ul>
                    <ul>
                      <li>
                        Similarly, the second key (<code>wc2</code>) of the
                        weight dictionary has a <code>shape</code> parameter
                        that will take a tuple with 4 values: the first and
                        second again refer to the filter size, and the third
                        represents the number of channels from the previous
                        output. Since you pass 32 convolution filters on the
                        input image, you will have 32 channels as an output from
                        the first convolution layer operation. The last
                        represents the number of filters you want in the second
                        convolution filter. Note that the second key in the
                        <code>biases</code> dictionary, <code>bc2</code>, will
                        have 64 parameters.
                      </li>
                    </ul>
                    <p>You will do the same for the third convolution layer.</p>
                    <ul>
                      <li>
                        Now, it's important to understand the fourth key
                        (<code>wd1</code>). After applying 3 convolution and
                        max-pooling operations, you are downsampling the input
                        image from 28 x 28 x 1 to 4 x 4 x 1, and now you need to
                        flatten this downsampled output to feed this as input to
                        the fully connected layer. That's why you do the
                        multiplication operation $44128$, which is the output of
                        the previous layer or number of channels that are
                        outputted by the convolution layer 3. The second element
                        of the tuple that you pass to <code>shape</code> has
                        number of neurons that you want in the fully connected
                        layer. Similarly, in the <code>biases</code> dictionary,
                        the fourth key <code>bd1</code> has 128 parameters.
                      </li>
                    </ul>
                    <p>
                      You will follow the same logic for the last fully
                      connected layer, in which the number of neurons will be
                      equivalent to the number of classes.
                    </p>
                    <pre><code class="lang-python">weights = {
    'wc1': tf.get_variable('W0', shape=(3,3,1,32), initializer=tf.contrib.layers.xavier_initializer()),
    'wc2': tf.get_variable('W1', shape=(3,3,32,64), initializer=tf.contrib.layers.xavier_initializer()),
    'wc3': tf.get_variable('W2', shape=(3,3,64,128), initializer=tf.contrib.layers.xavier_initializer()),
    'wd1': tf.get_variable('W3', shape=(4*4*128,128), initializer=tf.contrib.layers.xavier_initializer()),
    'out': tf.get_variable('W6', shape=(128,n_classes), initializer=tf.contrib.layers.xavier_initializer()),
}
biases = {
    'bc1': tf.get_variable('B0', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),
    'bc2': tf.get_variable('B1', shape=(64), initializer=tf.contrib.layers.xavier_initializer()),
    'bc3': tf.get_variable('B2', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),
    'bd1': tf.get_variable('B3', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),
    'out': tf.get_variable('B4', shape=(10), initializer=tf.contrib.layers.xavier_initializer()),
}
</code></pre>
                    <p>
                      Now, it's time to define the network architecture!
                      Unfortunately, this is not as simple as you do it in the
                      Keras framework!
                    </p>
                    <p>
                      The <code>conv_net()</code> function takes 3 arguments as
                      an input: the input <code>x</code> and the
                      <code>weights</code> and <code>biases</code>
                      dictionaries. Again, let's go through the construction of
                      the network step by step:
                    </p>
                    <ul>
                      <li>
                        Firstly, you reshape the 784-dimensional input vector to
                        a 28 x 28 x 1 matrix. As you had seen earlier, the
                        images are loaded as a 784-dimensional vector, but you
                        will feed the input to your model as a matrix of size 28
                        x 28 x 1. The <code>-1</code> in the
                        <code>reshape()</code> function means that it will infer
                        the first dimension on its own, but the rest of the
                        dimensions are fixed, that is, 28 x 28 x 1.
                      </li>
                    </ul>
                    <ul>
                      <li>
                        Next, as shown in the figure of the architecture of the
                        model, you will define <code>conv1</code>, which takes
                        input as an image, weights <code>wc1</code>, and biases
                        <code>bc1</code>. Next, you apply max-pooling on the
                        output of <code>conv1</code>, and you will perform a
                        process analogous to this until <code>conv3</code>.
                      </li>
                    </ul>
                    <ul>
                      <li>
                        Since your task is to classify, given an image, it
                        belongs to which class label. So, after you pass through
                        all the convolution and max-pooling layers, you will
                        flatten the output of <code>conv3</code>. Next, you'll
                        connect the flattened <code>conv3</code> neurons with
                        each and every neuron in the next layer. Then you will
                        apply activation function on the output of the fully
                        connected layer <code>fc1</code>.
                      </li>
                    </ul>
                    <p>
                      Finally, in the last layer, you will have 10 neurons since
                      you have to classify 10 labels. That means that you will
                      connect all the neurons of fc1 in the output layer with 10
                      neurons in the last layer.
                    </p>
                    <pre><code class="lang-python">def conv_net(x, weights, biases):  

    # here we call the conv2d function we had defined above and pass the input image x, weights wc1 and bias bc1.
    conv1 = conv2d(x, weights['wc1'], biases['bc1'])
    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 14*14 matrix.
    conv1 = maxpool2d(conv1, k=2)

    # Convolution Layer
    # here we call the conv2d function we had defined above and pass the input image x, weights wc2 and bias bc2.
    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])
    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 7*7 matrix.
    conv2 = maxpool2d(conv2, k=2)

    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])
    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 4*4.
    conv3 = maxpool2d(conv3, k=2)


    # Fully connected layer
    # Reshape conv2 output to fit fully connected layer input
    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])
    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])
    fc1 = tf.nn.relu(fc1)
    # Output, class prediction
    # finally we multiply the fully connected layer with the weights and add a bias term.
    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])
    return out
</code></pre>
                    <h3 id="loss-and-optimizer-nodes">
                      Loss and Optimizer Nodes
                    </h3>
                    <p>
                      You will start with constructing a model and call the
                      <code>conv_net()</code> function by passing in input
                      <code>x</code>, <code>weights</code>, and
                      <code>biases</code>. Since this is a multi-class
                      classification problem, you will use softmax activation on
                      the output layer. This will give you probabilities for
                      each class label. The loss function you use is
                      cross-entropy.
                    </p>
                    <p>
                      The reason you use cross-entropy as a loss function is
                      that the cross-entropy function's value is always
                      positive, and tends toward zero as the neuron gets better
                      at computing the desired output, y, for all training
                      inputs, x. These are both properties you would intuitively
                      expect for a cost function. It avoids the problem of
                      learning to slow down, which means that if the weights and
                      biases are initialized in a wrong fashion, it helps in
                      recovering faster and does not hamper much of the training
                      phase.
                    </p>
                    <p>
                      In TensorFlow, you define both the activation and the
                      cross-entropy loss functions in one line. You pass two
                      parameters, which are the predicted output and the ground
                      truth label <code>y</code>. You will then take the mean
                      (<code>reduce_mean</code>), which will compute the mean
                      loss over all instances in a single batch and not the
                      average over all the batches since you will be training
                      your model in a mini-batch fashion.
                    </p>
                    <p>
                      Next, you define one of the most popular optimization
                      algorithms: Adam optimizer. You can read more about the
                      optimizer from
                      <a
                        href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/"
                        >here</a
                      >, and you specify the learning rate by explicitly stating
                      how to minimize the cost you had calculated in the
                      previous step.
                    </p>
                    <pre><code class="lang-python">pred = conv_net(x, weights, biases)

cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))

optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)
</code></pre>
                    <pre><code>WARNING:tensorflow:From &lt;ipython-input-23-989f812044df&gt;:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.
</code></pre>
                    <h3 id="evaluate-model-node">Evaluate Model Node</h3>
                    <p>
                      To test your model, let's define two more nodes:
                      correct_prediction and accuracy. It will evaluate your
                      model after every training iteration, which will help you
                      keep track of your model's performance. After every
                      iteration, the model is tested on the 10,000 testing
                      images, which will not be seen in the training phase.
                    </p>
                    <p>
                      You can always save the graph and run the testing part
                      later as well. But for now, you will test within the
                      session.
                    </p>
                    <pre><code class="lang-python">#Here, you check whether the index of the maximum value of the predicted image is equal to the actual labeled image. And both will be a column vector.
correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))

#calculate accuracy across all the given images and average them out.
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
</code></pre>
                    <p>
                      <strong>Remember</strong> that your weights and biases are
                      variables and that you have to initialize them before you
                      can make use of them. So let's do that with the following
                      line of code:
                    </p>
                    <pre><code class="lang-python"># Initializing the variables
init = tf.global_variables_initializer()
</code></pre>
                    <p><a id="training"></a></p>
                    <h3 id="training-and-testing-the-model">
                      Training and Testing the Model
                    </h3>
                    <p>
                      When you train and test your model in TensorFlow, you go
                      through the following steps:
                    </p>
                    <ul>
                      <li>
                        You start by launching the graph. This is a class that
                        runs all the TensorFlow operations and launches the
                        graph in a session. All the operations have to be within
                        the indentation.
                      </li>
                    </ul>
                    <ul>
                      <li>
                        Then, you run the session, which will execute the
                        initialized variables in the previous step and evaluate
                        the tensor.
                      </li>
                    </ul>
                    <ul>
                      <li>
                        <p>
                          Next, you define a for loop that runs for the number
                          of training iterations you had specified in the
                          beginning.
                        </p>
                      </li>
                      <li>
                        <p>
                          Right after that, you'll initiate a second for loop,
                          which is for the number of batches that you will have
                          based on the batch size you chose, so you divide the
                          total number of images by the batch size.
                        </p>
                      </li>
                    </ul>
                    <ul>
                      <li>
                        You will then input the images based on the batch size
                        you pass in <code>batch_x</code> and their respective
                        labels in <code>batch_y</code>.
                      </li>
                    </ul>
                    <ul>
                      <li>
                        Now is the most important step. Just like you ran the
                        initializer after creating the graph, now you feed the
                        placeholders <code>x</code> and <code>y</code> the
                        actual data in a dictionary and run the session by
                        passing the cost and the accuracy that you had defined
                        earlier. It returns the loss (cost) and accuracy.
                      </li>
                    </ul>
                    <ul>
                      <li>
                        You can print the loss and training accuracy after each
                        epoch (training iteration) is completed.
                      </li>
                    </ul>
                    <ul>
                      <li>
                        After each training iteration is completed, you run only
                        the accuracy by passing all of the 10000 test images and
                        labels. This will give you an idea of how accurately
                        your model is performing while it is training.
                      </li>
                    </ul>
                    <p>
                      It's usually recommended to do the testing once your model
                      is trained completely and validate only while it is in the
                      training phase after each epoch. However, let's stick with
                      this approach for now.
                    </p>
                    <pre><code class="lang-python">with tf.Session() as sess:
    sess.run(init)
    train_loss = []
    test_loss = []
    train_accuracy = []
    test_accuracy = []
    summary_writer = tf.summary.FileWriter('./Output', sess.graph)
    for i in range(training_iters):
        for batch in range(len(train_X)//batch_size):
            batch_x = train_X[batch*batch_size:min((batch+1)*batch_size,len(train_X))]
            batch_y = train_y[batch*batch_size:min((batch+1)*batch_size,len(train_y))]    
            # Run optimization op (backprop).
                # Calculate batch loss and accuracy
            opt = sess.run(optimizer, feed_dict={x: batch_x,
                                                              y: batch_y})
            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,
                                                              y: batch_y})
        print("Iter " + str(i) + ", Loss= " + \
                      "{:.6f}".format(loss) + ", Training Accuracy= " + \
                      "{:.5f}".format(acc))
        print("Optimization Finished!")

        # Calculate accuracy for all 10000 mnist test images
        test_acc,valid_loss = sess.run([accuracy,cost], feed_dict={x: test_X,y : test_y})
        train_loss.append(loss)
        test_loss.append(valid_loss)
        train_accuracy.append(acc)
        test_accuracy.append(test_acc)
        print("Testing Accuracy:","{:.5f}".format(test_acc))
    summary_writer.close()
</code></pre>
                    <pre><code>Iter 0, Loss= 0.383201, Training Accuracy= 0.84375
Optimization Finished!
Testing Accuracy: 0.83500
Iter 1, Loss= 0.205107, Training Accuracy= 0.92969
Optimization Finished!
Testing Accuracy: 0.87630
Iter 2, Loss= 0.163720, Training Accuracy= 0.96094
Optimization Finished!
Testing Accuracy: 0.88950
Iter 3, Loss= 0.135824, Training Accuracy= 0.96875
Optimization Finished!
Testing Accuracy: 0.89450
Iter 4, Loss= 0.120255, Training Accuracy= 0.97656
Optimization Finished!
Testing Accuracy: 0.90190
Iter 5, Loss= 0.116372, Training Accuracy= 0.97656
Optimization Finished!
Testing Accuracy: 0.90210
Iter 6, Loss= 0.114322, Training Accuracy= 0.95312
Optimization Finished!
Testing Accuracy: 0.90260
Iter 7, Loss= 0.095541, Training Accuracy= 0.97656
Optimization Finished!
Testing Accuracy: 0.90110
Iter 8, Loss= 0.094024, Training Accuracy= 0.96875
Optimization Finished!
Testing Accuracy: 0.90060
Iter 9, Loss= 0.079477, Training Accuracy= 0.98438
Optimization Finished!
Testing Accuracy: 0.90130
</code></pre>
                    <p>
                      The test accuracy looks impressive. It turns out that your
                      classifier does better than the benchmark that was
                      reported
                      <a
                        href="http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/"
                        >here</a
                      >, which is an SVM classifier with a mean accuracy of
                      0.897. Also, the model does well compared to some of the
                      deep learning models mentioned on the
                      <a href="https://github.com/zalandoresearch/fashion-mnist"
                        >GitHub</a
                      >
                      profile of the creators of the fashion-MNIST dataset.
                    </p>
                    <p>
                      However, you saw that the model was overfitting since the
                      training accuracy is more than the testing accuracy. Are
                      these results all that good?
                    </p>
                    <p>
                      Let's put your model evaluation into perspective and plot
                      the accuracy and loss plots between training and
                      validation data:
                    </p>
                    <pre><code class="lang-python">plt.plot(range(len(train_loss)), train_loss, 'b', label='Training loss')
plt.plot(range(len(train_loss)), test_loss, 'r', label='Test loss')
plt.title('Training and Test loss')
plt.xlabel('Epochs ',fontsize=16)
plt.ylabel('Loss',fontsize=16)
plt.legend()
plt.figure()
plt.show()
</code></pre>
                    <center>
                      <img
                        src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1591576245/output_54_0_k2wmy0.png"
                      />
                    </center>

                    <pre><code>&lt;Figure size 432x288 with 0 Axes&gt;
</code></pre>
                    <pre><code class="lang-python">plt.plot(range(len(train_loss)), train_accuracy, 'b', label='Training Accuracy')
plt.plot(range(len(train_loss)), test_accuracy, 'r', label='Test Accuracy')
plt.title('Training and Test Accuracy')
plt.xlabel('Epochs ',fontsize=16)
plt.ylabel('Loss',fontsize=16)
plt.legend()
plt.figure()
plt.show()
</code></pre>
                    <center>
                      <img
                        src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1591576245/output_55_0_bwinbp.png"
                      />
                    </center>

                    <pre><code>&lt;Figure size 432x288 with 0 Axes&gt;
</code></pre>
                    <p>
                      From the above two plots, you can see that the test
                      accuracy almost became stagnant after 8 epochs and rarely
                      increased at certain epochs. In the beginning, the testing
                      accuracy was linearly increasing with loss, but then it
                      did not increase much.
                    </p>
                    <p>
                      The testing loss shows that this is the sign of
                      overfitting. Similar to training accuracy, it linearly
                      decreased, but after 5 epochs, it started to increase.
                      This means that the model tried to memorize the data and
                      succeeded.
                    </p>
                    <p>
                      This was it for this tutorial, but there is a task for you
                      all:
                    </p>
                    <ul>
                      <li>
                        Your task is to reduce the overfitting of the above
                        model by introducing the dropout technique. For
                        simplicity, you may like to follow along with the
                        tutorial
                        <a
                          href="convolutional-neural-networks-python.html#explore"
                          >Convolutional Neural Networks in Python</a
                        >
                        with Keras, even though it is in keras. However, still,
                        the accuracy and loss heuristics are pretty much the
                        same. So, following along with this tutorial will help
                        you to add dropout layers in your current model since
                        both of the tutorials have exactly similar architecture.
                      </li>
                    </ul>
                    <ul>
                      <li>
                        Secondly, try to improve the testing accuracy by
                        deepening the network a bit, adding learning rate decay
                        for faster convergence, or trying to play with the
                        optimizer and so on!
                      </li>
                    </ul>
                    <p><a id="own"></a></p>
                    <h3 id="processing-your-own-data">
                      Processing your own Data
                    </h3>
                    <p>
                      You would be working with the <code>CIFAR-10</code> data,
                      which consists of 60,000 32x32 color images in 10 classes,
                      with 6000 images per class. There are 50,000 training
                      images and 10,000 test images in the official data.
                    </p>
                    <p>
                      The dataset can be downloaded from the
                      <a href="https://www.kaggle.com/c/cifar-10/data"
                        >kaggle</a
                      >
                      website. It would be in <code>.7z</code> format, which you
                      would need to uncompress, and finally, you will have the
                      <code>.png</code> image files in the folder.
                    </p>
                    <p>
                      If you are using a Macbook, you can install
                      <code>p7zip</code> using <code>brew install p7zip</code>,
                      and once its installed, run <code>7z x train.7z</code>.
                      This will create a <code>train</code> folder which will
                      have 50,000 <code>.png</code> images.
                    </p>
                    <p>
                      The data was split in train/test from the original
                      dataset, hence, you can download the files accordingly.
                      For now, you will only download the
                      <code>train.7z</code> folder.
                    </p>
                    <p>
                      The dataset consists of following classes all being
                      mutually exclusive:
                    </p>
                    <ul>
                      <li>airplane</li>
                      <li>automobile</li>
                      <li>bird</li>
                      <li>cat</li>
                      <li>deer</li>
                      <li>dog</li>
                      <li>frog</li>
                      <li>horse</li>
                      <li>ship</li>
                      <li>truck</li>
                    </ul>
                    <p>
                      The first step is to load the <code>train</code> folder
                      using Python's built-in <code>glob</code> module and then
                      read the <code>labels.csv</code> using the
                      <code>Pandas</code> library.
                    </p>
                    <pre><code class="lang-python">import glob
import pandas as pd
</code></pre>
                    <pre><code class="lang-python">imgs = []
label = []
</code></pre>
                    <pre><code class="lang-python">data = glob.glob('train/*')
</code></pre>
                    <pre><code class="lang-python">len(data)
</code></pre>
                    <pre><code>50000
</code></pre>
                    <pre><code class="lang-python">labels_main = pd.read_csv('trainLabels.csv')
</code></pre>
                    <pre><code class="lang-python">labels_main.head(5)
</code></pre>
                    <div>
                      <style scoped>
                        .dataframe tbody tr th:only-of-type {
                          vertical-align: middle;
                        }

                        .dataframe tbody tr th {
                          vertical-align: top;
                        }

                        .dataframe thead th {
                          text-align: right;
                        }
                      </style>
                      <div class="output_wrapper">
                        <table border="1" class="dataframe">
                          <thead>
                            <tr style="text-align: right">
                              <th></th>
                              <th>id</th>
                              <th>label</th>
                            </tr>
                          </thead>
                          <tbody>
                            <tr>
                              <th>0</th>
                              <td>1</td>
                              <td>frog</td>
                            </tr>
                            <tr>
                              <th>1</th>
                              <td>2</td>
                              <td>truck</td>
                            </tr>
                            <tr>
                              <th>2</th>
                              <td>3</td>
                              <td>truck</td>
                            </tr>
                            <tr>
                              <th>3</th>
                              <td>4</td>
                              <td>deer</td>
                            </tr>
                            <tr>
                              <th>4</th>
                              <td>5</td>
                              <td>automobile</td>
                            </tr>
                          </tbody>
                        </table>
                      </div>
                    </div>

                    <p>
                      You only need the second column (<code>label</code>) from
                      the <code>labels_main</code> data frame, which can be
                      accessed using the Pandas <code>.iloc</code> function,
                      once you have the second column just convert it into a
                      list using <code>.tolist()</code>.
                    </p>
                    <pre><code class="lang-python">labels = labels_main.iloc[:,1].tolist()
</code></pre>
                    <p>
                      Next, you need to create a dictionary that will map your
                      categorical string into an integer value. Then you will
                      use list comprehension and apply the mapping on the
                      <code>labels</code> list that you created above.
                    </p>
                    <p>
                      Finally, you will convert these integer values into
                      one-hot encoding values using the
                      <code>to_categorical</code> function.
                    </p>
                    <pre><code class="lang-python">conversion = {'airplane':0,'automobile':1,'bird':2,'cat':3, 'deer':4, 'dog':5, 'frog':6,\
              'horse':7, 'ship':8, 'truck':9}
</code></pre>
                    <pre><code class="lang-python">num_labels = []
</code></pre>
                    <pre><code class="lang-python">num_labels.append([conversion[item] for item in labels])
</code></pre>
                    <pre><code class="lang-python">num_labels = np.array(num_labels)
</code></pre>
                    <pre><code class="lang-python">num_labels
</code></pre>
                    <pre><code>array([[6, 9, 9, ..., 9, 1, 1]])
</code></pre>
                    <pre><code class="lang-python">from keras.utils import to_categorical
</code></pre>
                    <pre><code>Using TensorFlow backend.
</code></pre>
                    <pre><code class="lang-python">label_one = to_categorical(num_labels)
</code></pre>
                    <pre><code class="lang-python">label_one = label_one.reshape(-1,10)
</code></pre>
                    <pre><code class="lang-python">label_one.shape
</code></pre>
                    <pre><code>(50000, 10)
</code></pre>
                    <p>
                      Now you will read the images from the
                      <code>train</code> folder by looping one-by-one using
                      <code>OpenCV</code> and store them in a list, and finally,
                      you will convert that list into a NumPy array. The shape
                      of your final output should be (50000, 32, 32, 3).
                    </p>
                    <pre><code class="lang-python">import cv2
</code></pre>
                    <pre><code class="lang-python">for i in data:
    img = cv2.imread(i)
    if img is not None:
        imgs.append(img)
</code></pre>
                    <pre><code class="lang-python">train_imgs = np.array(imgs)
train_imgs.shape
</code></pre>
                    <pre><code>(50000, 32, 32, 3)
</code></pre>
                    <p>
                      Let's visualize a couple of images from the training
                      dataset. Note that the class labels and image semantics
                      should be in sync, which should also act as a validation
                      that the data preprocessing was done correctly.
                    </p>
                    <pre><code class="lang-python">plt.figure(figsize=[5,5])

# Display the first image in training data
plt.subplot(121)
curr_img = np.reshape(train_imgs[0], (32,32,3))
curr_lbl = labels_main.iloc[0,1]
plt.imshow(curr_img)
plt.title("(Label: " + str(curr_lbl) + ")")

# Display the second image in training data
plt.subplot(122)
curr_img = np.reshape(train_imgs[1], (32,32,3))
curr_lbl = labels_main.iloc[1,1]
plt.imshow(curr_img)
plt.title("(Label: " + str(curr_lbl) + ")")
</code></pre>
                    <pre><code>Text(0.5, 1.0, '(Label: truck)')
</code></pre>
                    <center>
                      <img
                        src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1591576245/output_82_1_q6eans.png"
                      />
                    </center>

                    <p>As a final step, you would:</p>
                    <ul>
                      <li>
                        Normalize your images between 0 and 1 before you feed
                        them into the convolution neural network.
                      </li>
                      <li>
                        Split the 50,000 images into training &amp; testing
                        images with a 20% split, which means the model will be
                        trained on 40,000 images and tested on 10,000 images.
                      </li>
                    </ul>
                    <pre><code class="lang-python">train_images = train_imgs / np.max(train_imgs)
</code></pre>
                    <pre><code class="lang-python">np.max(train_images), np.min(train_images)
</code></pre>
                    <pre><code>(1.0, 0.0)
</code></pre>
                    <pre><code class="lang-python">from sklearn.model_selection import train_test_split
</code></pre>
                    <pre><code class="lang-python">X_train, X_test, y_train, y_test = train_test_split(train_images, label_one, test_size=0.2, random_state=42)
</code></pre>
                    <pre><code class="lang-python">train_X = X_train.reshape(-1, 32, 32, 3)
test_X = X_test.reshape(-1, 32, 32, 3)
</code></pre>
                    <p>
                      Now, you are all set to feed the data into the Convolution
                      Neural Network you created and trained above. But you
                      would have to make slight modifications before you can
                      start training the model and finally test it. This
                      modification would be a good exercise for you to learn and
                      understand how the dimensions of the parameters as well as
                      the overall architecture changes when your input &amp;
                      output is varied.
                    </p>
                    <h1
                      id="go-further-and-master-deep-learning-with-tensorflow-"
                    >
                      Go Further and Master Deep Learning with TensorFlow!
                    </h1>
                    <p>
                      This tutorial was a good start to understanding how
                      TensorFlow works underneath the hood, along with an
                      implementation of convolutional neural networks in Python.
                    </p>
                    <p>
                      If you were able to follow along easily, well done! Try
                      doing some experiments with the same model architecture
                      but using different types of public datasets available.
                      You could also try playing with different weight
                      initializers, maybe deepen the network architecture,
                      change the learning rate, etc. and see how your network
                      performs by changing these parameters. But try changing
                      them one at a time only. Then, you will get more intuition
                      about these parameters and will not get confused; that's
                      what is called Ablation Study!
                    </p>
                    <p>
                      There is still a lot to cover, so why not take DataCamp’s
                      <a
                        href="https://www.datacamp.com/courses/deep-learning-in-python"
                        >Deep Learning in Python course</a
                      >? Also make sure to check out the
                      <a href="https://www.tensorflow.org/"
                        >TensorFlow documentation</a
                      >, if you haven’t done so already. You will find more
                      examples and information on all functions, arguments, more
                      layers, etc. It will undoubtedly be an indispensable
                      resource when you’re learning how to work with neural
                      networks in Python!
                    </p>
                  </div>
                  <link
                    href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/solarized-dark.min.css"
                    rel="stylesheet"
                  />
                  <link
                    href="https://fonts.googleapis.com/css?family=Lora"
                    rel="stylesheet"
                  />
                  <link
                    href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,400i,700,700i"
                    rel="stylesheet"
                  />
                </div>
                <div class="jsx-1851940463 social__bottom mobileOnly">
                  <div class="jsx-1851940463 voteAndSocial">
                    <div class="jsx-1851940463">
                      <div class="jsx-1727309017 Upvote">
                        <div class="jsx-1727309017">
                          <div class="jsx-1727309017 normal">
                            <span class="jsx-1727309017 icon"
                              ><svg
                                xmlns="http://www.w3.org/2000/svg"
                                width="12"
                                height="12"
                                viewBox="0 0 12 12"
                              >
                                <path d="M1 10L6 0l5 10z"></path></svg></span
                            ><span class="jsx-1727309017 count">22</span>
                          </div>
                          <div class="jsx-1727309017 voted">
                            <span class="jsx-1727309017 icon"
                              ><svg
                                xmlns="http://www.w3.org/2000/svg"
                                width="12"
                                height="12"
                                viewBox="0 0 12 12"
                              >
                                <path d="M1 10L6 0l5 10z"></path></svg></span
                            ><span class="jsx-1727309017 count">22</span>
                          </div>
                        </div>
                      </div>
                      <a href="#comments" class="jsx-765279523 CommentCounter"
                        ><span class="jsx-765279523 icon"
                          ><svg
                            xmlns="http://www.w3.org/2000/svg"
                            viewBox="0 0 18 18"
                          >
                            <path
                              d="M12.595 13.364c.01-.111.02-.197.028-.251.058-.405.372-.702.74-.702h1.257c1.035-.002 1.875-.934 1.88-2.082V4.764c-.001-1.155-.842-2.092-1.878-2.094H3.38c-1.034.002-1.873.931-1.88 2.076v5.565c.001 1.156.842 2.092 1.878 2.094h6.626c.292 0 .557.189.68.484.408.977 1.07 1.576 1.94 1.85a6.004 6.004 0 0 1-.03-1.375zm1.51 1.119c.048.314.136.521.235.606.566.487.258 1.497-.458 1.497-1.87 0-3.423-.785-4.33-2.51H3.376C1.513 14.07.004 12.39 0 10.311V4.74C.014 2.673 1.52 1.004 3.378 1h11.245c1.864.004 3.373 1.686 3.377 3.763v5.57c-.01 2.07-1.518 3.744-3.378 3.748h-.551c.004.138.016.273.035.402zm-8.423-5.81a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229zm3.268 0a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229zm3.318 0a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229z"
                            ></path></svg></span
                        ><span class="jsx-765279523 count">0</span></a
                      >
                    </div>
                    <div class="jsx-2027482828 Social">
                      <div class="jsx-2027482828 icons">
                        <a
                          href="https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/cnn-tensorflow-python"
                          target="_blank"
                          rel="noopener noreferrer"
                          class="jsx-2027482828 icon"
                          ><svg
                            height="12"
                            xmlns="http://www.w3.org/2000/svg"
                            viewBox="0 0 11.73 22.58"
                          >
                            <path
                              d="M7.61 22.58v-10.3h3.46l.52-4h-4V5.7c0-1.16.32-2 2-2h2.13V.16A28.47 28.47 0 0 0 8.63 0C5.56 0 3.47 1.87 3.47 5.31v3H0v4h3.47v10.3h4.14z"
                            ></path></svg></a
                        ><a
                          href="https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/cnn-tensorflow-python"
                          target="_blank"
                          rel="noopener noreferrer"
                          class="jsx-2027482828 icon centerIcon"
                          ><svg
                            height="10"
                            xmlns="http://www.w3.org/2000/svg"
                            viewBox="0 0 20.42 16.67"
                          >
                            <path
                              d="M10 5.18c0-.28-.06-.53-.07-.78a4 4 0 0 1 .73-2.57A4.08 4.08 0 0 1 13.93 0 4 4 0 0 1 17 1.15a.43.43 0 0 0 .46.12 8.68 8.68 0 0 0 2.2-.84l.2-.1a4.36 4.36 0 0 1-1.75 2.28A9 9 0 0 0 20.42 2l-.21.3a3.83 3.83 0 0 1-.23.3A8.45 8.45 0 0 1 18.5 4a.28.28 0 0 0-.13.27A12 12 0 0 1 17 10.18a11.8 11.8 0 0 1-3.37 4.11 11.17 11.17 0 0 1-4.39 2.06 12.53 12.53 0 0 1-4.44.22 11.87 11.87 0 0 1-4.74-1.73L0 14.79a8.6 8.6 0 0 0 6.16-1.74 4.28 4.28 0 0 1-3.91-2.91h.95a6.18 6.18 0 0 0 .89-.12A4.2 4.2 0 0 1 .8 5.88a4 4 0 0 0 1.81.49 4.23 4.23 0 0 1-1.78-3A4.07 4.07 0 0 1 1.38.79 12.06 12.06 0 0 0 10 5.18z"
                              id="iOjKBC.tif"
                            ></path></svg></a
                        ><a
                          href="https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/cnn-tensorflow-python"
                          target="_blank"
                          rel="noopener noreferrer"
                          class="jsx-2027482828 icon"
                          ><svg
                            height="10"
                            xmlns="http://www.w3.org/2000/svg"
                            viewBox="0 0 16.99 17"
                          >
                            <path
                              d="M3.85 17H.34V5.67h3.51zM2.07 4.18a2.09 2.09 0 1 1 2.08-2.09 2.08 2.08 0 0 1-2.08 2.09zM17 17h-3.5v-5.95c0-1.63-.62-2.54-1.91-2.54s-2.14.95-2.14 2.54V17H6.09V5.67h3.36v1.52a4 4 0 0 1 3.42-1.87c2.4 0 4.12 1.47 4.12 4.5V17z"
                            ></path></svg
                        ></a>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </main>
          <div class="jsx-2326038540 SidebarSocial">
            <div class="jsx-2326038540 rss">
              <a href="../rss.xml" class="jsx-2326038540"
                ><svg
                  height="13"
                  xmlns="http://www.w3.org/2000/svg"
                  viewBox="0 0 18 18"
                >
                  <circle cx="3.08" cy="14.92" r="3.08"></circle>
                  <path
                    d="M16.46 18a1.54 1.54 0 0 1-1.54-1.54c0-8.25-5.13-13.38-13.38-13.38A1.59 1.59 0 0 1 .46 1.15 1.72 1.72 0 0 1 1.54 0a16.45 16.45 0 0 1 12 4.51c3 2.95 4.51 7.08 4.51 12A1.54 1.54 0 0 1 16.46 18z"
                  ></path>
                  <path
                    d="M10.63 18a1.54 1.54 0 0 1-1.54-1.54c0-5-2.54-7.54-7.54-7.54a1.54 1.54 0 0 1 0-3.08c6.75 0 10.63 3.87 10.63 10.62A1.54 1.54 0 0 1 10.63 18z"
                  ></path></svg
                >Subscribe to RSS</a
              >
            </div>
            <div class="jsx-2326038540 icons">
              <a
                href="https://www.facebook.com/pages/DataCamp/726282547396228"
                target="_blank"
                rel="noopener noreferrer"
                class="jsx-2326038540 icon"
                ><svg
                  height="16"
                  xmlns="http://www.w3.org/2000/svg"
                  viewBox="0 0 11.73 22.58"
                >
                  <path
                    d="M7.61 22.58v-10.3h3.46l.52-4h-4V5.7c0-1.16.32-2 2-2h2.13V.16A28.47 28.47 0 0 0 8.63 0C5.56 0 3.47 1.87 3.47 5.31v3H0v4h3.47v10.3h4.14z"
                  ></path></svg></a
              ><a
                href="https://twitter.com/datacamp"
                target="_blank"
                rel="noopener noreferrer"
                class="jsx-2326038540 icon"
                ><svg
                  height="13"
                  xmlns="http://www.w3.org/2000/svg"
                  viewBox="0 0 20.42 16.67"
                >
                  <path
                    d="M10 5.18c0-.28-.06-.53-.07-.78a4 4 0 0 1 .73-2.57A4.08 4.08 0 0 1 13.93 0 4 4 0 0 1 17 1.15a.43.43 0 0 0 .46.12 8.68 8.68 0 0 0 2.2-.84l.2-.1a4.36 4.36 0 0 1-1.75 2.28A9 9 0 0 0 20.42 2l-.21.3a3.83 3.83 0 0 1-.23.3A8.45 8.45 0 0 1 18.5 4a.28.28 0 0 0-.13.27A12 12 0 0 1 17 10.18a11.8 11.8 0 0 1-3.37 4.11 11.17 11.17 0 0 1-4.39 2.06 12.53 12.53 0 0 1-4.44.22 11.87 11.87 0 0 1-4.74-1.73L0 14.79a8.6 8.6 0 0 0 6.16-1.74 4.28 4.28 0 0 1-3.91-2.91h.95a6.18 6.18 0 0 0 .89-.12A4.2 4.2 0 0 1 .8 5.88a4 4 0 0 0 1.81.49 4.23 4.23 0 0 1-1.78-3A4.07 4.07 0 0 1 1.38.79 12.06 12.06 0 0 0 10 5.18z"
                    id="iOjKBC.tif"
                  ></path></svg></a
              ><a
                href="https://www.linkedin.com/company/datacampinc"
                target="_blank"
                rel="noopener noreferrer"
                class="jsx-2326038540 icon"
                ><svg
                  height="12"
                  xmlns="http://www.w3.org/2000/svg"
                  viewBox="0 0 16.99 17"
                >
                  <path
                    d="M3.85 17H.34V5.67h3.51zM2.07 4.18a2.09 2.09 0 1 1 2.08-2.09 2.08 2.08 0 0 1-2.08 2.09zM17 17h-3.5v-5.95c0-1.63-.62-2.54-1.91-2.54s-2.14.95-2.14 2.54V17H6.09V5.67h3.36v1.52a4 4 0 0 1 3.42-1.87c2.4 0 4.12 1.47 4.12 4.5V17z"
                  ></path></svg></a
              ><a
                href="https://www.youtube.com/channel/UC79Gv3mYp6zKiSwYemEik9A"
                target="_blank"
                rel="noopener noreferrer"
                class="jsx-2326038540 icon"
                ><svg
                  height="12"
                  xmlns="http://www.w3.org/2000/svg"
                  viewBox="0 0 25.19 17.73"
                >
                  <path
                    d="M24.21 1.52C23.3.44 21.62 0 18.42 0H6.78C3.5 0 1.79.47.88 1.62S0 4.4 0 6.68V11c0 4.43 1 6.68 6.78 6.68h11.64c2.78 0 4.32-.39 5.32-1.34S25.2 13.76 25.2 11V6.68c-.01-2.41-.08-4.07-.99-5.16zm-8 7.94l-5.29 2.76a.81.81 0 0 1-1.19-.72V6a.81.81 0 0 1 1.19-.72L16.17 8a.81.81 0 0 1 0 1.44z"
                  ></path></svg
              ></a>
            </div>
            <div class="jsx-2326038540 menu">
              <a
                href="https://www.datacamp.com/about"
                class="jsx-2326038540 menuItem"
                >About</a
              ><a
                href="https://www.datacamp.com/terms-of-use"
                class="jsx-2326038540 menuItem"
                >Terms</a
              ><a
                href="https://www.datacamp.com/privacy-policy"
                class="jsx-2326038540 menuItem"
                >Privacy</a
              >
            </div>
          </div>
        </div>
        <link
          href="https://fonts.googleapis.com/css?family=Lato:300,400i,400,700"
          rel="stylesheet"
        />
        <div class="Analytics">
          <script>
            (function (p, l, o, w, i, n, g) {
              if (!p[i]) {
                p.GlobalSnowplowNamespace = p.GlobalSnowplowNamespace || [];
                p.GlobalSnowplowNamespace.push(i);
                p[i] = function () {
                  (p[i].q = p[i].q || []).push(arguments);
                };
                p[i].q = p[i].q || [];
                n = l.createElement(o);
                g = l.getElementsByTagName(o)[0];
                n.async = 1;
                n.src = w;
                g.parentNode.insertBefore(n, g);
              }
            })(
              window,
              document,
              "script",
              "http://cdn.datacamp.com/sp/2.10.2.js",
              "snowplow"
            );

            var options = {
              appId: "community",
              platform: "web",
              post: true,
              discoverRootDomain: true,
              contexts: {
                webPage: true,
                performanceTiming: true,
              },
              forceSecureTracker: true,
              postPath: "/spevent",
            };

            window.snowplow("newTracker", "co", "www.datacamp.com", options);
            window.snowplow("enableActivityTracking", 10, 10);
            window.snowplow("enableLinkClickTracking");
          </script>
          <script data-cfasync="false">
            (function (W, i, s, e, P, o, p) {
              W["WisePopsObject"] = P;
              (W[P] =
                W[P] ||
                function () {
                  (W[P].q = W[P].q || []).push(arguments);
                }),
                (W[P].l = 1 * new Date());
              (o = i.createElement(s)), (p = i.getElementsByTagName(s)[0]);
              o.async = 1;
              o.src = e;
              p.parentNode.insertBefore(o, p);
            })(
              window,
              document,
              "script",
              "http://loader.wisepops.com/get-loader.js?v=1&amp;site=VswVJn7o4J",
              "wisepops"
            );
          </script>
        </div>
      </div>
    </div>
    <script id="__NEXT_DATA__" type="application/json">
      {
        "props": {
          "pageProps": {
            "isServer": true,
            "store": {},
            "initialState": {
              "adminContent": {
                "isFetching": false,
                "isFetched": false,
                "statusMessage": "",
                "content": {},
                "form": {
                  "isSaving": false,
                  "isSucceeded": false,
                  "statusMessage": "",
                  "isAdminFormModalOpen": false,
                  "previewSlug": ""
                },
                "delete": {
                  "isDeleting": false,
                  "isSucceeded": false,
                  "statusMessage": "",
                  "isDeleteAdminContentModalOpen": false
                },
                "update": {
                  "isApprovingArticle": false,
                  "isSucceeded": false,
                  "statusMessage": "",
                  "isApproveArticleModalOpen": false
                }
              },
              "adminList": {
                "isFetched": false,
                "isFetching": false,
                "statusMessage": ""
              },
              "auth": {
                "isAuthModalOpen": false,
                "isLogin": false,
                "isAuthorized": false,
                "isSubscriber": false,
                "user": {},
                "loginTitle": "",
                "signUpTitle": ""
              },
              "clientConfig": {
                "isDevelopment": false,
                "absoluteUrl": "https://www.datacamp.com",
                "isNewsActive": true,
                "ALGOLIA_APP_ID": "7H5IORUQLD",
                "ALGOLIA_API_KEY": "ae4f7a78b4914ef37d11dc177ad2eb13",
                "ALGOLIA_API_INDEX": "community_prod",
                "DC_COMMUNITY_AUTHOR_APP_URL": "https://community-author-app.new.datacamp.com/",
                "DC_LIGHT_URL": "https://cdn.datacamp.com/datacamp-light-latest.min.js",
                "ANALYTICS_SNOWPLOW_ENDPOINT": "www.datacamp.com",
                "COOKIE_BANNER_URL": "https://compliance.datacamp.com",
                "WISEPOPS": "VswVJn7o4J",
                "CHAT_SUBSCRIBER_REDIRECT": "https://www.datacamp.com/slack/join",
                "CHAT_NONSUBSCRIBER_LINK": "https://www.datacamp.com/pricing",
                "CHAT_SUBSCRIBE_TEAM": "https://www.datacamp.com/groups/business"
              },
              "comment": {
                "isFetching": false,
                "isFetched": false,
                "statusMessage": "",
                "comments": [],
                "commentsTotal": 0,
                "form": {
                  "isSaving": false,
                  "isSucceeded": false,
                  "statusMessage": "",
                  "id": "new",
                  "parentId": null,
                  "commentText": ""
                },
                "delete": {
                  "isDeleting": false,
                  "isSucceeded": false,
                  "statusMessage": "",
                  "isDeleteCommentModalOpen": false
                },
                "isBottomBarOpen": true,
                "bottomBarView": "bar"
              },
              "content": {
                "content": {
                  "id": 24062,
                  "externalId": null,
                  "type": "Tutorial",
                  "status": "published",
                  "authorId": "adityasharma101993",
                  "title": "Convolutional Neural Networks with TensorFlow",
                  "slug": "cnn-tensorflow-python",
                  "previewSlug": null,
                  "description": "In this tutorial, you'll learn how to construct and implement Convolutional Neural Networks (CNNs) in Python with the TensorFlow framework.",
                  "articles": [],
                  "courses": [],
                  "redirectSlug": null,
                  "contentHtml": "\u003ccenter\u003e\n\u003ca target=“_blank” href=\"https://www.datacamp.com/learn/python/\"\u003e\u003cimg src=\"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1583330891/python_2_wyjjwk.png\" /\u003e\u003c/a\u003e\n\u003c/center\u003e\u003cbr\u003e\n\n\u003cp\u003eTensorFlow is a popular deep learning framework. In this tutorial, you will learn the basics of this Python library and understand how to implement these deep, feed-forward artificial neural networks with it.\u003c/p\u003e\n\n\u003cp\u003eTo be precise, you\u0026#39;ll be introduced to the following topics in today\u0026#39;s tutorial:\u003c/p\u003e\n\n\u003cp\u003e- You\u0026#39;ll be first introduced to \u003ca href=\"#tensors\"\u003etensors\u003c/a\u003e and how they differ from matrices; Once you understand what tensors are then, you\u0026#39;ll be introduced to the \u003ca href=\"#Tensorflow\"\u003eTensorflow\u003c/a\u003e Framework, within this you will also see that how even a single line of code is implemented via a \u003ca href=\"#computational\"\u003ecomputational graph\u003c/a\u003e in TensorFlow, then you will learn about some of the package\u0026#39;s concepts that play a major role in you to do deep learning like \u003ca href=\"#constants\"\u003econstants, variables, and placeholders\u003c/a\u003e.\u003c/p\u003e\n\n\n\u003cp\u003e- Then, you\u0026#39;ll be headed to the most interesting part of this tutorial. That is the implementation of the \u003ca href=\"#cnn\"\u003eConvolutional Neural Network\u003c/a\u003e: first, you will try to understand the data. You\u0026#39;ll use Python and its libraries to load, explore, and analyze your data. You\u0026#39;ll also preprocess your data: you’ll learn how to visualize your images as a matrix, reshape your data and rescale the images between 0 and 1 if required.\u003c/p\u003e\n\n\n\u003cp\u003e- With all of this done, you are ready to \u003ca href=\"#construct\"\u003econstruct the deep neural network model\u003c/a\u003e. You\u0026#39;ll start by defining the network parameters, then learn how to create wrappers to increase the simplicity of your code, define weights and biases, model the network, define loss and optimizer nodes. Once you have all this in place, you are ready for \u003ca href=\"#training\"\u003etraining and testing your model\u003c/a\u003e.\u003c/p\u003e\n\n\n\u003cp\u003e- Finally, you will learn to work with your \u003ca href=\"#own\"\u003eown dataset\u003c/a\u003e. In this section, you would download the CIFAR-10 dataset from Kaggle, load the images and labels using Python modules like \u003ccode\u003eglob\u003c/code\u003e \u0026amp; \u003ccode\u003epandas\u003c/code\u003e. You will read the images using \u003ccode\u003eOpenCV\u003c/code\u003e, one-hot the class labels, visualize the images with labels, normalize the images, and finally split the dataset into train and test set.\u003c/p\u003e\n\n\u003ch2 id=\"tensors\"\u003eTensors\u003c/h2\u003e\n\n\u003ca id='tensors'\u003e\u003c/a\u003e\n\n\u003cp\u003eIn layman\u0026#39;s terms, a tensor is a way of representing the data in deep learning. A tensor can be a 1-dimensional, a 2-dimensional, a 3-dimensional array, etc. You can think of a tensor as a multidimensional array. In machine learning and deep learning, you have datasets that are high dimensional, in which each dimension represents a different feature of that dataset.\u003c/p\u003e\n\n\u003cp\u003eConsider the following example of a dog versus cat classification problem, where the dataset you\u0026#39;re working with has multiple varieties of both cats and dogs images. Now, in order to correctly classify a dog or a cat when given an image, the network has to learn discriminative features like color, face structure, ears, eyes, the shape of the tail, etc.\u003c/p\u003e\n\n\u003cp\u003eThese features are incorporated by the tensors.\u003c/p\u003e\n\n\u003ccenter\u003e\u003cimg src=\"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1591576676/Screen_Shot_2020-06-07_at_8.36.33_PM_yqpppc.png\"\u003e\u003c/center\u003e\n\n\u003cp\u003e\u003cstrong\u003eTip\u003c/strong\u003e: if you want to learn more about tensors, check out DataCamp\u0026#39;s \u003ca href=\"https://www.datacamp.com/community/tutorials/tensorflow-tutorial\"\u003eTensorFlow Tutorial for Beginners\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eBut how are tensors then any different from matrices? You\u0026#39;ll find out in the next section!\u003c/p\u003e\n\u003ch3 id=\"tensors-versus-matrices-differences\"\u003eTensors versus Matrices: Differences\u003c/h3\u003e\n\u003cp\u003eA matrix is a two-dimensional grid of size $n×m$ that contains numbers: you can add and subtract matrices of the same size, multiply one matrix with another as long as the sizes are compatible $((n×m)×(m×p)=n×p)$, and multiply an entire matrix by a constant.\u003c/p\u003e\n\u003cp\u003eA vector is a matrix with just one row or column (but see below).\u003c/p\u003e\n\u003cp\u003eA tensor is often thought of as a generalized matrix. That is, it could be\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003ea 1-D matrix, like a vector, which is actually such a tensor,\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003ea 3-D matrix (something like a cube of numbers),\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003ea 0-D matrix (a single number), or\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003ea higher dimensional structure that is harder to visualize.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe dimension of the tensor is called its rank.\u003c/p\u003e\n\u003cp\u003eAny rank-2 tensor can be represented as a matrix, but not every matrix is a rank-2 tensor. The numerical values of a tensor’s matrix representation depend on what transformation rules have been applied to the entire system.\u003c/p\u003e\n\u003ch2 id=\"tensorflow-constants-variables-and-placeholders\"\u003eTensorFlow: Constants, Variables, and Placeholders\u003c/h2\u003e\n\u003cp\u003e\u003ca id='Tensorflow'\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eTensorFlow is a framework developed by Google on 9th November 2015. It is written in Python, C++, and Cuda. It supports platforms like Linux, Microsoft Windows, macOS, and Android. TensorFlow provides multiple APIs in Python, C++, Java, etc. It is the most widely used API in Python, and you will implement a convolutional neural network using Python API in this tutorial.\u003c/p\u003e\n\u003cp\u003eThe name TensorFlow is derived from the operations, such as adding or multiplying, that artificial neural networks perform on multidimensional data arrays. These arrays are called tensors in this framework, which is slightly different from what you saw earlier.\u003c/p\u003e\n\u003cp\u003eSo why is there a mention of a flow when you\u0026#39;re talking about operations?\u003c/p\u003e\n\u003cp\u003eLet\u0026#39;s consider a simple equation and its diagram, represented as a computational graph. Note: don\u0026#39;t worry if you don\u0026#39;t get this equation straight away, this is just to help you to understand how the flow takes place while using the TensorFlow framework.\u003c/p\u003e\n\u003cp\u003e\u003ca id='computational'\u003e\u003c/a\u003e\nprediction = tf.nn.softmax(tf.matmul(W,x) + b)\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src=\"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1591576676/Screen_Shot_2020-06-07_at_8.36.57_PM_asaftw.png\"\u003e\u003c/center\u003e\n\n\u003cp\u003eIn TensorFlow, every line of code that you write has to go through a computational graph. As in the above figure, you can see that first $W$ and $x$ get multiplied. Then comes b, which is added to the output of $W$ and $x$. After adding the output of $W$ and $x$ with $b$, a softmax function is applied, and the final output is generated.\u003c/p\u003e\n\u003cp\u003e\u003ca id='constants'\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eYou\u0026#39;ll find that when you\u0026#39;re working with TensorFlow, constants, variables, and placeholders come handy to define the input data, class labels, weights, and biases.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eConstant\u003c/strong\u003e takes no input, you use them to store constant values. They produce a constant output that it stores.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003eimport tensorflow as tf\na = tf.constant(2.0)\nb = tf.constant(3.0)\nc = a * b\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere, nodes \u003ccode\u003ea\u003c/code\u003e and \u003ccode\u003eb\u003c/code\u003e are constants that store values \u003ccode\u003e2.0\u003c/code\u003e and \u003ccode\u003e3.0\u003c/code\u003e. Node \u003ccode\u003ec\u003c/code\u003e stores the operation that multiplies the nodes \u003ccode\u003ea\u003c/code\u003e and \u003ccode\u003eb\u003c/code\u003e, respectively. When you initialize a session and run \u003ccode\u003ec\u003c/code\u003e, you\u0026#39;ll see that the output that you get back is \u003ccode\u003e6.0\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003esess = tf.Session()\nsess.run(c)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e6.0\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePlaceholders\u003c/strong\u003e allow you to feed input on the run. Because of this flexibility, placeholders are used, which allows your computational graph to take inputs as parameters. Defining a node as a placeholder assures that node, that it is expected to receive a value later or during runtime. Here, \u0026quot;runtime\u0026quot; means that the input is fed to the placeholder when you run your computational graph.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Creating placeholders\na = tf.placeholder(tf.float32)\nb = tf.placeholder(tf.float32)\n\n# Assigning addition operation w.r.t. a and b to node add\nadd = a + b\n\n# Create session object\nsess = tf.Session()\n\n# Executing add by passing the values [1, 3] [2, 4] for a and b respectively\noutput = sess.run(add, {a: [1,3], b: [2, 4]})\nprint(\u0026#39;Adding a and b:\u0026#39;, output)\nprint(\u0026#39;Datatype:\u0026#39;, output.dtype)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003eAdding a and b: [3. 7.]\nDatatype: float32\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIn this case, you have explicitly provided the data type with \u003ccode\u003etf.float32\u003c/code\u003e. Note that this data type is, therefore, a single-precision, which is stored in 32 bits form. However, in cases where you do not do this, just like in the first example, TensorFlow will infer the type of the constant/variable from the initialized value.\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src=\"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1591576676/Screen_Shot_2020-06-07_at_8.37.08_PM_fjym5c.png\"\u003e\u003c/center\u003e\n\n\u003cp\u003e\u003cstrong\u003eVariables\u003c/strong\u003e allow you to modify the graph such that it can produce new outputs with respect to the same inputs. A variable allows you to add such parameters or nodes to the graph that are trainable. That is, the value can be modified throughout time.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e#Variables are defined by providing their initial value and type\nvariable = tf.Variable([0.9,0.7], dtype = tf.float32)\n\n#variable must be initialized before a graph is used for the first time.\ninit = tf.global_variables_initializer()\nsess.run(init)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eConstants are initialized when you call \u003ccode\u003etf.constant\u003c/code\u003e, and their value can never change. But, variables are not initialized when you call \u003ccode\u003etf.Variable\u003c/code\u003e. To initialize all the variables in TensorFlow, you need to explicitly call the global variable initializer \u003ccode\u003eglobal_variables_initializer()\u003c/code\u003e, which initializes all the existing variables in your TensorFlow code, as you can see in the above code chunk.\u003c/p\u003e\n\u003cp\u003eVariables survive across multiple executions of a graph, unlike normal tensors that are only instantiated when a graph is run and are immediately deleted afterward.\u003c/p\u003e\n\u003cp\u003eIn this section, you have seen that placeholders are used for holding the input data and class labels, whereas variables are used for weights and biases. Don\u0026#39;t worry if you have still not been able to develop proper intuition about how a computational graph works or for what placeholders and variables typically used for in deep learning. You will address all these topics later on in this tutorial.\u003c/p\u003e\n\u003ch2 id=\"convolutional-neural-network-cnn-in-tensorflow\"\u003eConvolutional Neural Network (CNN) in TensorFlow\u003c/h2\u003e\n\u003cp\u003e\u003ca id='cnn'\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"fashion-mnist-dataset\"\u003eFashion-MNIST Dataset\u003c/h3\u003e\n\u003cp\u003eBefore you go ahead and load in the data, it\u0026#39;s good to take a look at what you\u0026#39;ll exactly be working with! The \u003ca href=\"https://arxiv.org/abs/1708.07747\"\u003eFashion-MNIST\u003c/a\u003e dataset contains Zalando\u0026#39;s article images, with 28x28 grayscale images of 65,000 fashion products from 10 categories, and 6,500 images per category. The training set has 55,000 images, and the test set has 10,000 images. You can double-check this later when you have loaded in your data! ;)\u003c/p\u003e\n\u003cp\u003eFashion-MNIST is similar to the MNIST dataset that you might already know, which you use to classify handwritten digits. That means that the image dimensions, training, and test splits are similar.\u003c/p\u003e\n\u003cp\u003eTip: if you want to learn how to implement a Multi-Layer Perceptron (MLP) for classification tasks with this latter dataset, \u003ca href=\"https://www.datacamp.com/community/tutorials/deep-learning-python\"\u003ego to this tutorial\u003c/a\u003e, or if you want to learn about convolutional neural networks and its implementation in a Keras framework, check out \u003ca href=\"https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python\"\u003ethis tutorial\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eYou can find the Fashion-MNIST dataset \u003ca href=\"https://github.com/zalandoresearch/fashion-mnist\"\u003ehere\u003c/a\u003e. Unlike the Keras or Scikit-Learn packages, TensorFlow has no predefined module to load the Fashion MNIST dataset, though it has an MNIST dataset by default. To load the data, you first need to download the data from the above link and then structure the data in a particular folder format, as shown below, to be able to work with it. Otherwise, Tensorflow will download and use the original MNIST.\u003c/p\u003e\n\u003ch3 id=\"load-the-data\"\u003eLoad the data\u003c/h3\u003e\n\u003cp\u003eYou first start with importing all the required modules like NumPy, matplotlib, and, most importantly, Tensorflow.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Import libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n%matplotlib inline\nimport os\n# os.environ[\u0026quot;CUDA_VISIBLE_DEVICES\u0026quot;]=\u0026quot;0\u0026quot; #for training on gpu\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter importing all the modules, you will now learn how to load data in TensorFlow, which should be pretty straightforward. The only thing that you should take into account is the \u003ccode\u003eone_hot=True\u003c/code\u003e argument, which you\u0026#39;ll also find in the line of code below: it converts the categorical class labels to binary vectors.\u003c/p\u003e\n\u003cp\u003eIn one-hot encoding, you convert the categorical data into a vector of numbers. You do this because machine learning algorithms can\u0026#39;t work with categorical data directly. Instead, you generate one boolean column for each category or class. Only one of these columns could take on the value 1 for each sample. That explains the term \u0026quot;one-hot encoding\u0026quot;.\u003c/p\u003e\n\u003cp\u003eBut what does such a one-hot encoded data column look like?\u003c/p\u003e\n\u003cp\u003eFor your problem statement, the one-hot encoding will be a row vector, and for each image, it will have a dimension of 1 x 10. It\u0026#39;s important to note here that the vector consists of all zeros except for the class that it represents. There, you\u0026#39;ll find a 1. For example, the ankle boot image that you plotted above has a label of 9, so for all the ankle boot images, the one-hot encoding vector would be [0 0 0 0 0 0 0 0 0 1].\u003c/p\u003e\n\u003cp\u003eNow that all of this is clear, it\u0026#39;s time to import the data!\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003edata = input_data.read_data_sets(\u0026#39;data/fashion\u0026#39;,one_hot=True,\\\n                                 source_url=\u0026#39;http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\u0026#39;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003eWARNING:tensorflow:From /Users/adityasharma/Library/Python/3.7/lib/python/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.\u0026lt;locals\u0026gt;.wrap.\u0026lt;locals\u0026gt;.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use urllib or similar directly.\nSuccessfully downloaded train-images-idx3-ubyte.gz 26421880 bytes.\nExtracting data/fashion1/train-images-idx3-ubyte.gz\nSuccessfully downloaded train-labels-idx1-ubyte.gz 29515 bytes.\nExtracting data/fashion1/train-labels-idx1-ubyte.gz\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 4422102 bytes.\nExtracting data/fashion1/t10k-images-idx3-ubyte.gz\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 5148 bytes.\nExtracting data/fashion1/t10k-labels-idx1-ubyte.gz\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: If you have trouble loading the \u003ccode\u003eFashion-MNIST\u003c/code\u003e dataset with the above method, kindly refer to \u003ca href=\"https://github.com/zalandoresearch/fashion-mnist\"\u003ethis\u003c/a\u003e repository, which shows a handful of ways in which you can load your dataset.\u003c/p\u003e\n\u003cp\u003eOnce you have the training and testing data loaded, you\u0026#39;re all set to analyze the data to get some intuition about the dataset that you are going to work with for this tutorial!\u003c/p\u003e\n\u003ch3 id=\"analyze-the-data\"\u003eAnalyze the Data\u003c/h3\u003e\n\u003cp\u003eBefore you start any heavy lifting, it\u0026#39;s always a good idea to check out what the images in the dataset look like. First, you can take a programmatical approach and check out their dimensions. Also, take into account that if you want to explore your images, these have already been rescaled between 0 and 1. That means that you would not need to rescale the image pixels again!\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Shapes of training set\nprint(\u0026quot;Training set (images) shape: {shape}\u0026quot;.format(shape=data.train.images.shape))\nprint(\u0026quot;Training set (labels) shape: {shape}\u0026quot;.format(shape=data.train.labels.shape))\n\n# Shapes of test set\nprint(\u0026quot;Test set (images) shape: {shape}\u0026quot;.format(shape=data.test.images.shape))\nprint(\u0026quot;Test set (labels) shape: {shape}\u0026quot;.format(shape=data.test.labels.shape))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003eTraining set (images) shape: (55000, 784)\nTraining set (labels) shape: (55000, 10)\nTest set (images) shape: (10000, 784)\nTest set (labels) shape: (10000, 10)\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eFrom the above output, you can see that the training data has a shape of 55000 x 784: there are 55,000 training samples each of the 784-dimensional vector. Similarly, the test data has a shape of 10000 x 784, since there are 10,000 testing samples.\u003c/p\u003e\n\u003cp\u003eThe 784-dimensional vector is nothing but a 28 x 28-dimensional matrix. That\u0026#39;s why you will be reshaping each training and testing sample from a 784-dimensional vector to a 28 x 28 x 1-dimensional matrix in order to feed the samples into the CNN model.\u003c/p\u003e\n\u003cp\u003eFor simplicity, let\u0026#39;s create a dictionary that will have class names with their corresponding categorical class labels.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Create dictionary of target classes\nlabel_dict = {\n 0: \u0026#39;T-shirt/top\u0026#39;,\n 1: \u0026#39;Trouser\u0026#39;,\n 2: \u0026#39;Pullover\u0026#39;,\n 3: \u0026#39;Dress\u0026#39;,\n 4: \u0026#39;Coat\u0026#39;,\n 5: \u0026#39;Sandal\u0026#39;,\n 6: \u0026#39;Shirt\u0026#39;,\n 7: \u0026#39;Sneaker\u0026#39;,\n 8: \u0026#39;Bag\u0026#39;,\n 9: \u0026#39;Ankle boot\u0026#39;,\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAlso, let\u0026#39;s take a look at a couple of images in the dataset:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003eplt.figure(figsize=[5,5])\n\n# Display the first image in training data\nplt.subplot(121)\ncurr_img = np.reshape(data.train.images[0], (28,28))\ncurr_lbl = np.argmax(data.train.labels[0,:])\nplt.imshow(curr_img, cmap=\u0026#39;gray\u0026#39;)\nplt.title(\u0026quot;(Label: \u0026quot; + str(label_dict[curr_lbl]) + \u0026quot;)\u0026quot;)\n\n# Display the first image in testing data\nplt.subplot(122)\ncurr_img = np.reshape(data.test.images[0], (28,28))\ncurr_lbl = np.argmax(data.test.labels[0,:])\nplt.imshow(curr_img, cmap=\u0026#39;gray\u0026#39;)\nplt.title(\u0026quot;(Label: \u0026quot; + str(label_dict[curr_lbl]) + \u0026quot;)\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003eText(0.5, 1.0, \u0026#39;(Label: Ankle boot)\u0026#39;)\n\u003c/code\u003e\u003c/pre\u003e\u003ccenter\u003e\u003cimg src=\"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1591576245/output_22_1_ylcf7u.png\" /\u003e\u003c/center\u003e\n\n\n\u003cp\u003eThe output of the above two plots is one of the sample images from both training and testing data, and these images are assigned a class label of 4 (Coat) and 9 (Ankle boot). Similarly, other fashion products will have different labels, but similar products will have the same labels. This means that all the 6,500 ankle boot images will have a class label of 9.\u003c/p\u003e\n\u003ch3 id=\"data-preprocessing\"\u003eData Preprocessing\u003c/h3\u003e\n\u003cp\u003eThe images are of size 28 x 28 (or a 784-dimensional vector).\u003c/p\u003e\n\u003cp\u003eThe images are already rescaled between 0 and 1, so you don\u0026#39;t need to rescale them again, but to be sure, let\u0026#39;s visualize an image from the training dataset as a matrix. Along with that, let\u0026#39;s also print the maximum and minimum value of the matrix.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003edata.train.images[0][500:]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003earray([0.40784317, 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.03921569, 0.9568628 ,\n       0.8588236 , 0.9803922 , 0.80392164, 0.7803922 , 0.8196079 ,\n       0.79215693, 0.8196079 , 0.82745105, 0.7411765 , 0.83921576,\n       0.8078432 , 0.8235295 , 0.7843138 , 0.8313726 , 0.6039216 ,\n       0.94117653, 0.81568635, 0.8588236 , 0.54901963, 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.08235294, 1.        , 0.8705883 , 0.9333334 ,\n       0.72156864, 0.8235295 , 0.75294125, 0.8078432 , 0.8196079 ,\n       0.8235295 , 0.7411765 , 0.8352942 , 0.82745105, 0.8196079 ,\n       0.75294125, 0.8941177 , 0.60784316, 0.8862746 , 0.9333334 ,\n       0.9450981 , 0.6509804 , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.14509805,\n       0.9607844 , 0.8862746 , 0.9450981 , 0.5882353 , 0.7725491 ,\n       0.7411765 , 0.8000001 , 0.8196079 , 0.8235295 , 0.7176471 ,\n       0.8352942 , 0.8352942 , 0.78823537, 0.72156864, 0.8431373 ,\n       0.57254905, 0.8470589 , 0.92549026, 0.882353  , 0.6039216 ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.227451  , 0.93725497, 0.89019614,\n       1.        , 0.61960787, 0.7568628 , 0.76470596, 0.8000001 ,\n       0.8196079 , 0.8352942 , 0.7058824 , 0.8117648 , 0.85098046,\n       0.7803922 , 0.7607844 , 0.82745105, 0.61960787, 0.8588236 ,\n       0.92549026, 0.8470589 , 0.5921569 , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.26666668, 0.91372555, 0.8862746 , 0.95294124, 0.54509807,\n       0.7843138 , 0.7568628 , 0.80392164, 0.8235295 , 0.81568635,\n       0.7058824 , 0.80392164, 0.8313726 , 0.7960785 , 0.7686275 ,\n       0.8470589 , 0.6156863 , 0.7019608 , 1.        , 0.8470589 ,\n       0.60784316, 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.31764707, 0.882353  ,\n       0.87843144, 0.82745105, 0.5411765 , 0.8588236 , 0.7254902 ,\n       0.78823537, 0.8352942 , 0.8117648 , 0.7725491 , 0.8862746 ,\n       0.8313726 , 0.7843138 , 0.74509805, 0.8431373 , 0.7176471 ,\n       0.3529412 , 1.        , 0.82745105, 0.5764706 , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.35686275, 0.8235295 , 0.90196085, 0.61960787,\n       0.44705886, 0.80392164, 0.73333335, 0.81568635, 0.8196079 ,\n       0.8078432 , 0.7568628 , 0.8235295 , 0.82745105, 0.8000001 ,\n       0.76470596, 0.8000001 , 0.70980394, 0.09019608, 1.        ,\n       0.8352942 , 0.61960787, 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.34117648,\n       0.80392164, 0.909804  , 0.427451  , 0.6431373 , 1.        ,\n       0.83921576, 0.87843144, 0.8705883 , 0.8235295 , 0.7725491 ,\n       0.83921576, 0.882353  , 0.8705883 , 0.82745105, 0.86274517,\n       0.85098046, 0.        , 0.9176471 , 0.8470589 , 0.6627451 ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.36078432, 0.8352942 , 0.909804  ,\n       0.57254905, 0.01960784, 0.5254902 , 0.5921569 , 0.63529414,\n       0.6666667 , 0.7176471 , 0.7137255 , 0.6431373 , 0.6509804 ,\n       0.69803923, 0.63529414, 0.6117647 , 0.38431376, 0.        ,\n       0.94117653, 0.882353  , 0.8235295 , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.16862746, 0.6431373 , 0.8078432 , 0.5529412 , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.49803925, 0.4901961 ,\n       0.29803923, 0.        , 0.        , 0.        ], dtype=float32)\n\u003c/code\u003e\u003c/pre\u003e\u003cpre\u003e\u003ccode class=\"lang-python\"\u003enp.max(data.train.images[0])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e1.0\n\u003c/code\u003e\u003c/pre\u003e\u003cpre\u003e\u003ccode class=\"lang-python\"\u003enp.min(data.train.images[0])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e0.0\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eLet us reshape the images so that it\u0026#39;s of size 28 x 28 x 1, and feed this as an input to the network.\u003c/p\u003e\n\u003cp\u003eThe reason you need to reshape your data is that Tensorflow expects a certain input shape for its Deep Learning Model, i.e., in this case, a \u003ccode\u003eConvolution Neural Network\u003c/code\u003e, specifically:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e(\u0026lt;number of images\u0026gt;, \u0026lt;image x_dim\u0026gt;, \u0026lt;image y_dim\u0026gt;, \u0026lt;number of channels\u0026gt;)\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eThe dataset class used here apparently yields a flattened (list-like) shape for these images, so the reshape command puts the data structure into a type that the TF class can work with.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Reshape training and testing image\ntrain_X = data.train.images.reshape(-1, 28, 28, 1)\ntest_X = data.test.images.reshape(-1,28,28,1)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003etrain_X.shape, test_X.shape\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e((55000, 28, 28, 1), (10000, 28, 28, 1))\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eYou need not reshape the labels since they already have the correct dimensions, but let us put the training and testing labels in separate variables and also print their respective shapes just to be on the safer side.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003etrain_y = data.train.labels\ntest_y = data.test.labels\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003etrain_y.shape, test_y.shape\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e((55000, 10), (10000, 10))\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"the-deep-neural-network\"\u003eThe Deep Neural Network\u003c/h2\u003e\n\u003cp\u003e\u003ca id='construct'\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eYou\u0026#39;ll use three convolutional layers:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe first layer will have 32-3 x 3 filters,\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003eThe second layer will have 64-3 x 3 filters and\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003eThe third layer will have 128-3 x 3 filters.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn addition, there are three max-pooling layers, each of the size 2 x 2.\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src=\"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1591576676/Screen_Shot_2020-06-07_at_8.37.21_PM_kytyha.png\"\u003e\u003c/center\u003e\n\n\u003cp\u003eYou start with defining the training iterations \u003ccode\u003etraining_iters\u003c/code\u003e, the learning rate \u003ccode\u003elearning_rate\u003c/code\u003e, and the batch size \u003ccode\u003ebatch_size\u003c/code\u003e. Keep in mind that all these are hyperparameters and that these don\u0026#39;t have fixed values, as these differ for every problem statement.\u003c/p\u003e\n\u003cp\u003eNevertheless, here\u0026#39;s what you usually can expect:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTraining iterations indicate the number of times you train your network,\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003eIt is a good practice to use a learning rate of 1e-3, the learning rate is a factor that is multiplied with the weights based on which the weights get updated, and this indeed helps in reducing the cost/loss/cross-entropy and ultimately in converging or reaching the local optima. The learning rate should neither be too high or too low it should be a balanced rate.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003eThe batch size means that your training images will be divided into a fixed batch size, and at every batch, it will take a fixed number of images and train them. It\u0026#39;s recommended to use a batch size in the power of 2. Since the number of the physical processor is often a power of 2, using several virtual processors different from a power of 2 leads to poor performance. Also, taking a very large batch size can lead to memory errors, so you have to make sure that the machine you run your code on has sufficient RAM to handle specified batch size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003etraining_iters = 10\nlearning_rate = 0.001\nbatch_size = 128\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"network-parameters\"\u003eNetwork Parameters\u003c/h2\u003e\n\u003cp\u003eNext, you need to define the network parameters. Firstly, you define the number of inputs. This is 784 since the image is initially loaded as a 784-dimensional vector. Later, you will see that how you will reshape the 784-dimensional vector to a 28 x 28 x 1 matrix. Secondly, you\u0026#39;ll also define the number of classes, which is nothing else than the number of class labels.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# MNIST data input (img shape: 28*28)\nn_input = 28\n\n# MNIST total classes (0-9 digits)\nn_classes = 10\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow is the time to use those placeholders, about which you read previously in this tutorial. You will define an input placeholder \u003ccode\u003ex\u003c/code\u003e, which will have a dimension of \u003ccode\u003eNone x 784\u003c/code\u003e and the output placeholder with a dimension of \u003ccode\u003eNone x 10\u003c/code\u003e. To reiterate, placeholders allow you to do operations and build your computation graph without feeding in data.\u003c/p\u003e\n\u003cp\u003eSimilarly, y will hold the label of the training images in the form matrix, which will be a \u003ccode\u003eNone*10 matrix\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThe row dimension is \u003ccode\u003eNone\u003c/code\u003e. That\u0026#39;s because you have defined \u003ccode\u003ebatch_size\u003c/code\u003e, which tells placeholders that they will receive this dimension when you feed in the data to them. Since you set the batch size to 128, this will be the row dimension of the placeholders.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e#both placeholders are of type float\nx = tf.placeholder(\u0026quot;float\u0026quot;, [None, 28,28,1])\ny = tf.placeholder(\u0026quot;float\u0026quot;, [None, n_classes])\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"creating-wrappers-for-simplicity\"\u003eCreating wrappers for simplicity\u003c/h3\u003e\n\u003cp\u003eIn your network architecture model, you will have multiple convolution and max-pooling layers. In such cases, it\u0026#39;s always a better idea to define convolution and max-pooling functions, so that you can call them as many times you want to use them in your network.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIn the \u003ccode\u003econv2d()\u003c/code\u003e function, you pass 4 arguments: input \u003ccode\u003ex\u003c/code\u003e, weights \u003ccode\u003eW\u003c/code\u003e, bias \u003ccode\u003eb\u003c/code\u003e, and \u003ccode\u003estrides\u003c/code\u003e. This last argument is by default set to 1, but you can always play with it to see how the network performs. The first and last stride must always be 1 because the first is for the image-number, and the last is for the input-channel (since the image is a gray-scale image which has only one channel). After applying the convolution, you will add bias and apply an activation function called Rectified Linear Unit (ReLU).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003eThe max-pooling function is simple: it has the input \u003ccode\u003ex\u003c/code\u003e and a kernel size \u003ccode\u003ek\u003c/code\u003e, which is set to be 2. This means that the max-pooling filter will be a square matrix with dimensions 2 x 2, and the stride by which the filter will move in is also 2.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou will be padding equal to the same which ensures that while performing the convolution operations, the boundary pixels of the image are not left out, so padding equal to same will basically add zeros at the boundaries of the input and allow the convolution filter to access the boundary pixels as well.\u003c/p\u003e\n\u003cp\u003eSimilarly, max-pooling operation padding equal to the same will add zeros. Later, when you define the weights and the biases, you will notice that input of size 28 x 28 is downsampled to 4 x 4 after applying three max-pooling layers.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003edef conv2d(x, W, b, strides=1):\n    # Conv2D wrapper, with bias and relu activation\n    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=\u0026#39;SAME\u0026#39;)\n    x = tf.nn.bias_add(x, b)\n    return tf.nn.relu(x)\n\ndef maxpool2d(x, k=2):\n    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],padding=\u0026#39;SAME\u0026#39;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter you have defined the \u003ccode\u003econv2d\u003c/code\u003e and \u003ccode\u003emaxpool2d\u003c/code\u003e wrappers, now you can define your weights and biases variables. So, let\u0026#39;s get started!\u003c/p\u003e\n\u003cp\u003eBut first, let\u0026#39;s understand each weight and bias parameter step by step. You will create two dictionaries, one for weight and the second for the bias parameter.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf you can recall from the above figure that the first convolution layer has 32-3x3 filters, so the first key (\u003ccode\u003ewc1\u003c/code\u003e) in the weight dictionary has an argument \u003ccode\u003eshape\u003c/code\u003e that takes a tuple with 4 values: the first and second are the filter size, while the third is the number of channels in the input image and the last represents the number of convolution filters you want in the first convolution layer. The first key in the \u003ccode\u003ebiases\u003c/code\u003e dictionary, \u003ccode\u003ebc1\u003c/code\u003e, will have 32 bias parameters.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003eSimilarly, the second key (\u003ccode\u003ewc2\u003c/code\u003e) of the weight dictionary has a \u003ccode\u003eshape\u003c/code\u003e parameter that will take a tuple with 4 values: the first and second again refer to the filter size, and the third represents the number of channels from the previous output. Since you pass 32 convolution filters on the input image, you will have 32 channels as an output from the first convolution layer operation. The last represents the number of filters you want in the second convolution filter. Note that the second key in the \u003ccode\u003ebiases\u003c/code\u003e dictionary, \u003ccode\u003ebc2\u003c/code\u003e, will have 64 parameters.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou will do the same for the third convolution layer.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNow, it\u0026#39;s important to understand the fourth key (\u003ccode\u003ewd1\u003c/code\u003e). After applying 3 convolution and max-pooling operations, you are downsampling the input image from 28 x 28 x 1 to 4 x 4 x 1, and now you need to flatten this downsampled output to feed this as input to the fully connected layer. That\u0026#39;s why you do the multiplication operation $44128$, which is the output of the previous layer or number of channels that are outputted by the convolution layer 3. The second element of the tuple that you pass to \u003ccode\u003eshape\u003c/code\u003e has number of neurons that you want in the fully connected layer. Similarly, in the \u003ccode\u003ebiases\u003c/code\u003e dictionary, the fourth key \u003ccode\u003ebd1\u003c/code\u003e has 128 parameters.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou will follow the same logic for the last fully connected layer, in which the number of neurons will be equivalent to the number of classes.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003eweights = {\n    \u0026#39;wc1\u0026#39;: tf.get_variable(\u0026#39;W0\u0026#39;, shape=(3,3,1,32), initializer=tf.contrib.layers.xavier_initializer()),\n    \u0026#39;wc2\u0026#39;: tf.get_variable(\u0026#39;W1\u0026#39;, shape=(3,3,32,64), initializer=tf.contrib.layers.xavier_initializer()),\n    \u0026#39;wc3\u0026#39;: tf.get_variable(\u0026#39;W2\u0026#39;, shape=(3,3,64,128), initializer=tf.contrib.layers.xavier_initializer()),\n    \u0026#39;wd1\u0026#39;: tf.get_variable(\u0026#39;W3\u0026#39;, shape=(4*4*128,128), initializer=tf.contrib.layers.xavier_initializer()),\n    \u0026#39;out\u0026#39;: tf.get_variable(\u0026#39;W6\u0026#39;, shape=(128,n_classes), initializer=tf.contrib.layers.xavier_initializer()),\n}\nbiases = {\n    \u0026#39;bc1\u0026#39;: tf.get_variable(\u0026#39;B0\u0026#39;, shape=(32), initializer=tf.contrib.layers.xavier_initializer()),\n    \u0026#39;bc2\u0026#39;: tf.get_variable(\u0026#39;B1\u0026#39;, shape=(64), initializer=tf.contrib.layers.xavier_initializer()),\n    \u0026#39;bc3\u0026#39;: tf.get_variable(\u0026#39;B2\u0026#39;, shape=(128), initializer=tf.contrib.layers.xavier_initializer()),\n    \u0026#39;bd1\u0026#39;: tf.get_variable(\u0026#39;B3\u0026#39;, shape=(128), initializer=tf.contrib.layers.xavier_initializer()),\n    \u0026#39;out\u0026#39;: tf.get_variable(\u0026#39;B4\u0026#39;, shape=(10), initializer=tf.contrib.layers.xavier_initializer()),\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow, it\u0026#39;s time to define the network architecture! Unfortunately, this is not as simple as you do it in the Keras framework!\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003econv_net()\u003c/code\u003e function takes 3 arguments as an input: the input \u003ccode\u003ex\u003c/code\u003e and the \u003ccode\u003eweights\u003c/code\u003e and \u003ccode\u003ebiases\u003c/code\u003e dictionaries. Again, let\u0026#39;s go through the construction of the network step by step:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFirstly, you reshape the 784-dimensional input vector to a 28 x 28 x 1 matrix. As you had seen earlier, the images are loaded as a 784-dimensional vector, but you will feed the input to your model as a matrix of size 28 x 28 x 1. The \u003ccode\u003e-1\u003c/code\u003e in the \u003ccode\u003ereshape()\u003c/code\u003e function means that it will infer the first dimension on its own, but the rest of the dimensions are fixed, that is, 28 x 28 x 1.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003eNext, as shown in the figure of the architecture of the model, you will define \u003ccode\u003econv1\u003c/code\u003e, which takes input as an image, weights \u003ccode\u003ewc1\u003c/code\u003e, and biases \u003ccode\u003ebc1\u003c/code\u003e. Next, you apply max-pooling on the output of \u003ccode\u003econv1\u003c/code\u003e, and you will perform a process analogous to this until \u003ccode\u003econv3\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003eSince your task is to classify, given an image, it belongs to which class label. So, after you pass through all the convolution and max-pooling layers, you will flatten the output of \u003ccode\u003econv3\u003c/code\u003e. Next, you\u0026#39;ll connect the flattened \u003ccode\u003econv3\u003c/code\u003e neurons with each and every neuron in the next layer. Then you will apply activation function on the output of the fully connected layer \u003ccode\u003efc1\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFinally, in the last layer, you will have 10 neurons since you have to classify 10 labels. That means that you will connect all the neurons of fc1 in the output layer with 10 neurons in the last layer.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003edef conv_net(x, weights, biases):  \n\n    # here we call the conv2d function we had defined above and pass the input image x, weights wc1 and bias bc1.\n    conv1 = conv2d(x, weights[\u0026#39;wc1\u0026#39;], biases[\u0026#39;bc1\u0026#39;])\n    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 14*14 matrix.\n    conv1 = maxpool2d(conv1, k=2)\n\n    # Convolution Layer\n    # here we call the conv2d function we had defined above and pass the input image x, weights wc2 and bias bc2.\n    conv2 = conv2d(conv1, weights[\u0026#39;wc2\u0026#39;], biases[\u0026#39;bc2\u0026#39;])\n    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 7*7 matrix.\n    conv2 = maxpool2d(conv2, k=2)\n\n    conv3 = conv2d(conv2, weights[\u0026#39;wc3\u0026#39;], biases[\u0026#39;bc3\u0026#39;])\n    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 4*4.\n    conv3 = maxpool2d(conv3, k=2)\n\n\n    # Fully connected layer\n    # Reshape conv2 output to fit fully connected layer input\n    fc1 = tf.reshape(conv3, [-1, weights[\u0026#39;wd1\u0026#39;].get_shape().as_list()[0]])\n    fc1 = tf.add(tf.matmul(fc1, weights[\u0026#39;wd1\u0026#39;]), biases[\u0026#39;bd1\u0026#39;])\n    fc1 = tf.nn.relu(fc1)\n    # Output, class prediction\n    # finally we multiply the fully connected layer with the weights and add a bias term.\n    out = tf.add(tf.matmul(fc1, weights[\u0026#39;out\u0026#39;]), biases[\u0026#39;out\u0026#39;])\n    return out\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"loss-and-optimizer-nodes\"\u003eLoss and Optimizer Nodes\u003c/h3\u003e\n\u003cp\u003eYou will start with constructing a model and call the \u003ccode\u003econv_net()\u003c/code\u003e function by passing in input \u003ccode\u003ex\u003c/code\u003e, \u003ccode\u003eweights\u003c/code\u003e, and \u003ccode\u003ebiases\u003c/code\u003e. Since this is a multi-class classification problem, you will use softmax activation on the output layer. This will give you probabilities for each class label. The loss function you use is cross-entropy.\u003c/p\u003e\n\u003cp\u003eThe reason you use cross-entropy as a loss function is that the cross-entropy function\u0026#39;s value is always positive, and tends toward zero as the neuron gets better at computing the desired output, y, for all training inputs, x. These are both properties you would intuitively expect for a cost function. It avoids the problem of learning to slow down, which means that if the weights and biases are initialized in a wrong fashion, it helps in recovering faster and does not hamper much of the training phase.\u003c/p\u003e\n\u003cp\u003eIn TensorFlow, you define both the activation and the cross-entropy loss functions in one line. You pass two parameters, which are the predicted output and the ground truth label \u003ccode\u003ey\u003c/code\u003e. You will then take the mean (\u003ccode\u003ereduce_mean\u003c/code\u003e), which will compute the mean loss over all instances in a single batch and not the average over all the batches since you will be training your model in a mini-batch fashion.\u003c/p\u003e\n\u003cp\u003eNext, you define one of the most popular optimization algorithms: Adam optimizer. You can read more about the optimizer from \u003ca href=\"https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\"\u003ehere\u003c/a\u003e, and you specify the learning rate by explicitly stating how to minimize the cost you had calculated in the previous step.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003epred = conv_net(x, weights, biases)\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003eWARNING:tensorflow:From \u0026lt;ipython-input-23-989f812044df\u0026gt;:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"evaluate-model-node\"\u003eEvaluate Model Node\u003c/h3\u003e\n\u003cp\u003eTo test your model, let\u0026#39;s define two more nodes: correct_prediction and accuracy. It will evaluate your model after every training iteration, which will help you keep track of your model\u0026#39;s performance. After every iteration, the model is tested on the 10,000 testing images, which will not be seen in the training phase.\u003c/p\u003e\n\u003cp\u003eYou can always save the graph and run the testing part later as well. But for now, you will test within the session.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e#Here, you check whether the index of the maximum value of the predicted image is equal to the actual labeled image. And both will be a column vector.\ncorrect_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n\n#calculate accuracy across all the given images and average them out.\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eRemember\u003c/strong\u003e that your weights and biases are variables and that you have to initialize them before you can make use of them. So let\u0026#39;s do that with the following line of code:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Initializing the variables\ninit = tf.global_variables_initializer()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca id='training'\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"training-and-testing-the-model\"\u003eTraining and Testing the Model\u003c/h3\u003e\n\u003cp\u003eWhen you train and test your model in TensorFlow, you go through the following steps:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eYou start by launching the graph. This is a class that runs all the TensorFlow operations and launches the graph in a session. All the operations have to be within the indentation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003eThen, you run the session, which will execute the initialized variables in the previous step and evaluate the tensor.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eNext, you define a for loop that runs for the number of training iterations you had specified in the beginning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eRight after that, you\u0026#39;ll initiate a second for loop, which is for the number of batches that you will have based on the batch size you chose, so you divide the total number of images by the batch size.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003eYou will then input the images based on the batch size you pass in \u003ccode\u003ebatch_x\u003c/code\u003e and their respective labels in \u003ccode\u003ebatch_y\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003eNow is the most important step. Just like you ran the initializer after creating the graph, now you feed the placeholders \u003ccode\u003ex\u003c/code\u003e and \u003ccode\u003ey\u003c/code\u003e the actual data in a dictionary and run the session by passing the cost and the accuracy that you had defined earlier. It returns the loss (cost) and accuracy.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003eYou can print the loss and training accuracy after each epoch (training iteration) is completed.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003eAfter each training iteration is completed, you run only the accuracy by passing all of the 10000 test images and labels. This will give you an idea of how accurately your model is performing while it is training.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIt\u0026#39;s usually recommended to do the testing once your model is trained completely and validate only while it is in the training phase after each epoch. However, let\u0026#39;s stick with this approach for now.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003ewith tf.Session() as sess:\n    sess.run(init)\n    train_loss = []\n    test_loss = []\n    train_accuracy = []\n    test_accuracy = []\n    summary_writer = tf.summary.FileWriter(\u0026#39;./Output\u0026#39;, sess.graph)\n    for i in range(training_iters):\n        for batch in range(len(train_X)//batch_size):\n            batch_x = train_X[batch*batch_size:min((batch+1)*batch_size,len(train_X))]\n            batch_y = train_y[batch*batch_size:min((batch+1)*batch_size,len(train_y))]    \n            # Run optimization op (backprop).\n                # Calculate batch loss and accuracy\n            opt = sess.run(optimizer, feed_dict={x: batch_x,\n                                                              y: batch_y})\n            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n                                                              y: batch_y})\n        print(\u0026quot;Iter \u0026quot; + str(i) + \u0026quot;, Loss= \u0026quot; + \\\n                      \u0026quot;{:.6f}\u0026quot;.format(loss) + \u0026quot;, Training Accuracy= \u0026quot; + \\\n                      \u0026quot;{:.5f}\u0026quot;.format(acc))\n        print(\u0026quot;Optimization Finished!\u0026quot;)\n\n        # Calculate accuracy for all 10000 mnist test images\n        test_acc,valid_loss = sess.run([accuracy,cost], feed_dict={x: test_X,y : test_y})\n        train_loss.append(loss)\n        test_loss.append(valid_loss)\n        train_accuracy.append(acc)\n        test_accuracy.append(test_acc)\n        print(\u0026quot;Testing Accuracy:\u0026quot;,\u0026quot;{:.5f}\u0026quot;.format(test_acc))\n    summary_writer.close()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003eIter 0, Loss= 0.383201, Training Accuracy= 0.84375\nOptimization Finished!\nTesting Accuracy: 0.83500\nIter 1, Loss= 0.205107, Training Accuracy= 0.92969\nOptimization Finished!\nTesting Accuracy: 0.87630\nIter 2, Loss= 0.163720, Training Accuracy= 0.96094\nOptimization Finished!\nTesting Accuracy: 0.88950\nIter 3, Loss= 0.135824, Training Accuracy= 0.96875\nOptimization Finished!\nTesting Accuracy: 0.89450\nIter 4, Loss= 0.120255, Training Accuracy= 0.97656\nOptimization Finished!\nTesting Accuracy: 0.90190\nIter 5, Loss= 0.116372, Training Accuracy= 0.97656\nOptimization Finished!\nTesting Accuracy: 0.90210\nIter 6, Loss= 0.114322, Training Accuracy= 0.95312\nOptimization Finished!\nTesting Accuracy: 0.90260\nIter 7, Loss= 0.095541, Training Accuracy= 0.97656\nOptimization Finished!\nTesting Accuracy: 0.90110\nIter 8, Loss= 0.094024, Training Accuracy= 0.96875\nOptimization Finished!\nTesting Accuracy: 0.90060\nIter 9, Loss= 0.079477, Training Accuracy= 0.98438\nOptimization Finished!\nTesting Accuracy: 0.90130\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe test accuracy looks impressive. It turns out that your classifier does better than the benchmark that was reported \u003ca href=\"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\"\u003ehere\u003c/a\u003e, which is an SVM classifier with a mean accuracy of 0.897.  Also, the model does well compared to some of the deep learning models mentioned on the \u003ca href=\"https://github.com/zalandoresearch/fashion-mnist\"\u003eGitHub\u003c/a\u003e profile of the creators of the fashion-MNIST dataset.\u003c/p\u003e\n\u003cp\u003eHowever, you saw that the model was overfitting since the training accuracy is more than the testing accuracy. Are these results all that good?\u003c/p\u003e\n\u003cp\u003eLet\u0026#39;s put your model evaluation into perspective and plot the accuracy and loss plots between training and validation data:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003eplt.plot(range(len(train_loss)), train_loss, \u0026#39;b\u0026#39;, label=\u0026#39;Training loss\u0026#39;)\nplt.plot(range(len(train_loss)), test_loss, \u0026#39;r\u0026#39;, label=\u0026#39;Test loss\u0026#39;)\nplt.title(\u0026#39;Training and Test loss\u0026#39;)\nplt.xlabel(\u0026#39;Epochs \u0026#39;,fontsize=16)\nplt.ylabel(\u0026#39;Loss\u0026#39;,fontsize=16)\nplt.legend()\nplt.figure()\nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ccenter\u003e\u003cimg src=\"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1591576245/output_54_0_k2wmy0.png\" /\u003e\u003c/center\u003e\n\n\n\n\u003cpre\u003e\u003ccode\u003e\u0026lt;Figure size 432x288 with 0 Axes\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\u003cpre\u003e\u003ccode class=\"lang-python\"\u003eplt.plot(range(len(train_loss)), train_accuracy, \u0026#39;b\u0026#39;, label=\u0026#39;Training Accuracy\u0026#39;)\nplt.plot(range(len(train_loss)), test_accuracy, \u0026#39;r\u0026#39;, label=\u0026#39;Test Accuracy\u0026#39;)\nplt.title(\u0026#39;Training and Test Accuracy\u0026#39;)\nplt.xlabel(\u0026#39;Epochs \u0026#39;,fontsize=16)\nplt.ylabel(\u0026#39;Loss\u0026#39;,fontsize=16)\nplt.legend()\nplt.figure()\nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ccenter\u003e\u003cimg src=\"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1591576245/output_55_0_bwinbp.png\" /\u003e\u003c/center\u003e\n\n\n\n\u003cpre\u003e\u003ccode\u003e\u0026lt;Figure size 432x288 with 0 Axes\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eFrom the above two plots, you can see that the test accuracy almost became stagnant after 8 epochs and rarely increased at certain epochs. In the beginning, the testing accuracy was linearly increasing with loss, but then it did not increase much.\u003c/p\u003e\n\u003cp\u003eThe testing loss shows that this is the sign of overfitting. Similar to training accuracy, it linearly decreased, but after 5 epochs, it started to increase. This means that the model tried to memorize the data and succeeded.\u003c/p\u003e\n\u003cp\u003eThis was it for this tutorial, but there is a task for you all:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eYour task is to reduce the overfitting of the above model by introducing the dropout technique. For simplicity, you may like to follow along with the tutorial \u003ca href=\"https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python#explore\"\u003eConvolutional Neural Networks in Python\u003c/a\u003e with Keras, even though it is in keras. However, still, the accuracy and loss heuristics are pretty much the same. So, following along with this tutorial will help you to add dropout layers in your current model since both of the tutorials have exactly similar architecture.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003eSecondly, try to improve the testing accuracy by deepening the network a bit, adding learning rate decay for faster convergence, or trying to play with the optimizer and so on!\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca id='own'\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"processing-your-own-data\"\u003eProcessing your own Data\u003c/h3\u003e\n\u003cp\u003eYou would be working with the \u003ccode\u003eCIFAR-10\u003c/code\u003e data, which consists of 60,000 32x32 color images in 10 classes, with 6000 images per class. There are 50,000 training images and 10,000 test images in the official data.\u003c/p\u003e\n\u003cp\u003eThe dataset can be downloaded from the \u003ca href=\"https://www.kaggle.com/c/cifar-10/data\"\u003ekaggle\u003c/a\u003e website. It would be in \u003ccode\u003e.7z\u003c/code\u003e format, which you would need to uncompress, and finally, you will have the \u003ccode\u003e.png\u003c/code\u003e image files in the folder.\u003c/p\u003e\n\u003cp\u003eIf you are using a Macbook, you can install \u003ccode\u003ep7zip\u003c/code\u003e using \u003ccode\u003ebrew install p7zip\u003c/code\u003e, and once its installed, run \u003ccode\u003e7z x train.7z\u003c/code\u003e. This will create a \u003ccode\u003etrain\u003c/code\u003e folder which will have 50,000 \u003ccode\u003e.png\u003c/code\u003e images.\u003c/p\u003e\n\u003cp\u003eThe data was split in train/test from the original dataset, hence, you can download the files accordingly. For now, you will only download the \u003ccode\u003etrain.7z\u003c/code\u003e folder.\u003c/p\u003e\n\u003cp\u003eThe dataset consists of following classes all being mutually exclusive:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eairplane\u003c/li\u003e\n\u003cli\u003eautomobile\u003c/li\u003e\n\u003cli\u003ebird\u003c/li\u003e\n\u003cli\u003ecat\u003c/li\u003e\n\u003cli\u003edeer\u003c/li\u003e\n\u003cli\u003edog\u003c/li\u003e\n\u003cli\u003efrog\u003c/li\u003e\n\u003cli\u003ehorse\u003c/li\u003e\n\u003cli\u003eship\u003c/li\u003e\n\u003cli\u003etruck\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe first step is to load the \u003ccode\u003etrain\u003c/code\u003e folder using Python\u0026#39;s built-in \u003ccode\u003eglob\u003c/code\u003e module and then read the \u003ccode\u003elabels.csv\u003c/code\u003e using the \u003ccode\u003ePandas\u003c/code\u003e library.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003eimport glob\nimport pandas as pd\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003eimgs = []\nlabel = []\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003edata = glob.glob(\u0026#39;train/*\u0026#39;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003elen(data)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e50000\n\u003c/code\u003e\u003c/pre\u003e\u003cpre\u003e\u003ccode class=\"lang-python\"\u003elabels_main = pd.read_csv(\u0026#39;trainLabels.csv\u0026#39;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003elabels_main.head(5)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003cdiv class=\"output_wrapper\"\u003e\u003ctable border=\"1\" class=\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style=\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003eid\u003c/th\u003e\n      \u003cth\u003elabel\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e0\u003c/th\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003efrog\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1\u003c/th\u003e\n      \u003ctd\u003e2\u003c/td\u003e\n      \u003ctd\u003etruck\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2\u003c/th\u003e\n      \u003ctd\u003e3\u003c/td\u003e\n      \u003ctd\u003etruck\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e3\u003c/th\u003e\n      \u003ctd\u003e4\u003c/td\u003e\n      \u003ctd\u003edeer\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e4\u003c/th\u003e\n      \u003ctd\u003e5\u003c/td\u003e\n      \u003ctd\u003eautomobile\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\u003c/div\u003e\n\u003c/div\u003e\n\n\n\n\u003cp\u003eYou only need the second column (\u003ccode\u003elabel\u003c/code\u003e) from the \u003ccode\u003elabels_main\u003c/code\u003e data frame, which can be accessed using the Pandas \u003ccode\u003e.iloc\u003c/code\u003e function, once you have the second column just convert it into a list using \u003ccode\u003e.tolist()\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003elabels = labels_main.iloc[:,1].tolist()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNext, you need to create a dictionary that will map your categorical string into an integer value. Then you will use list comprehension and apply the mapping on the \u003ccode\u003elabels\u003c/code\u003e list that you created above.\u003c/p\u003e\n\u003cp\u003eFinally, you will convert these integer values into one-hot encoding values using the \u003ccode\u003eto_categorical\u003c/code\u003e function.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003econversion = {\u0026#39;airplane\u0026#39;:0,\u0026#39;automobile\u0026#39;:1,\u0026#39;bird\u0026#39;:2,\u0026#39;cat\u0026#39;:3, \u0026#39;deer\u0026#39;:4, \u0026#39;dog\u0026#39;:5, \u0026#39;frog\u0026#39;:6,\\\n              \u0026#39;horse\u0026#39;:7, \u0026#39;ship\u0026#39;:8, \u0026#39;truck\u0026#39;:9}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003enum_labels = []\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003enum_labels.append([conversion[item] for item in labels])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003enum_labels = np.array(num_labels)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003enum_labels\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003earray([[6, 9, 9, ..., 9, 1, 1]])\n\u003c/code\u003e\u003c/pre\u003e\u003cpre\u003e\u003ccode class=\"lang-python\"\u003efrom keras.utils import to_categorical\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003eUsing TensorFlow backend.\n\u003c/code\u003e\u003c/pre\u003e\u003cpre\u003e\u003ccode class=\"lang-python\"\u003elabel_one = to_categorical(num_labels)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003elabel_one = label_one.reshape(-1,10)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003elabel_one.shape\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e(50000, 10)\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNow you will read the images from the \u003ccode\u003etrain\u003c/code\u003e folder by looping one-by-one using \u003ccode\u003eOpenCV\u003c/code\u003e and store them in a list, and finally, you will convert that list into a NumPy array. The shape of your final output should be (50000, 32, 32, 3).  \u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003eimport cv2\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003efor i in data:\n    img = cv2.imread(i)\n    if img is not None:\n        imgs.append(img)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003etrain_imgs = np.array(imgs)\ntrain_imgs.shape\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e(50000, 32, 32, 3)\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eLet\u0026#39;s visualize a couple of images from the training dataset. Note that the class labels and image semantics should be in sync, which should also act as a validation that the data preprocessing was done correctly.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003eplt.figure(figsize=[5,5])\n\n# Display the first image in training data\nplt.subplot(121)\ncurr_img = np.reshape(train_imgs[0], (32,32,3))\ncurr_lbl = labels_main.iloc[0,1]\nplt.imshow(curr_img)\nplt.title(\u0026quot;(Label: \u0026quot; + str(curr_lbl) + \u0026quot;)\u0026quot;)\n\n# Display the second image in training data\nplt.subplot(122)\ncurr_img = np.reshape(train_imgs[1], (32,32,3))\ncurr_lbl = labels_main.iloc[1,1]\nplt.imshow(curr_img)\nplt.title(\u0026quot;(Label: \u0026quot; + str(curr_lbl) + \u0026quot;)\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003eText(0.5, 1.0, \u0026#39;(Label: truck)\u0026#39;)\n\u003c/code\u003e\u003c/pre\u003e\u003ccenter\u003e\u003cimg src=\"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1591576245/output_82_1_q6eans.png\" /\u003e\u003c/center\u003e\n\n\n\u003cp\u003eAs a final step, you would:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNormalize your images between 0 and 1 before you feed them into the convolution neural network.\u003c/li\u003e\n\u003cli\u003eSplit the 50,000 images into training \u0026amp; testing images with a 20% split, which means the model will be trained on 40,000 images and tested on 10,000 images.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003etrain_images = train_imgs / np.max(train_imgs)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003enp.max(train_images), np.min(train_images)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e(1.0, 0.0)\n\u003c/code\u003e\u003c/pre\u003e\u003cpre\u003e\u003ccode class=\"lang-python\"\u003efrom sklearn.model_selection import train_test_split\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003eX_train, X_test, y_train, y_test = train_test_split(train_images, label_one, test_size=0.2, random_state=42)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003etrain_X = X_train.reshape(-1, 32, 32, 3)\ntest_X = X_test.reshape(-1, 32, 32, 3)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow, you are all set to feed the data into the Convolution Neural Network you created and trained above. But you would have to make slight modifications before you can start training the model and finally test it. This modification would be a good exercise for you to learn and understand how the dimensions of the parameters as well as the overall architecture changes when your input \u0026amp; output is varied.\u003c/p\u003e\n\u003ch1 id=\"go-further-and-master-deep-learning-with-tensorflow-\"\u003eGo Further and Master Deep Learning with TensorFlow!\u003c/h1\u003e\n\u003cp\u003eThis tutorial was a good start to understanding how TensorFlow works underneath the hood, along with an implementation of convolutional neural networks in Python.\u003c/p\u003e\n\u003cp\u003eIf you were able to follow along easily, well done! Try doing some experiments with the same model architecture but using different types of public datasets available. You could also try playing with different weight initializers, maybe deepen the network architecture, change the learning rate, etc. and see how your network performs by changing these parameters. But try changing them one at a time only. Then, you will get more intuition about these parameters and will not get confused; that\u0026#39;s what is called Ablation Study!\u003c/p\u003e\n\u003cp\u003eThere is still a lot to cover, so why not take DataCamp’s \u003ca href=\"https://www.datacamp.com/courses/deep-learning-in-python\"\u003eDeep Learning in Python course\u003c/a\u003e? Also make sure to check out the \u003ca href=\"https://www.tensorflow.org/\"\u003eTensorFlow documentation\u003c/a\u003e, if you haven’t done so already. You will find more examples and information on all functions, arguments, more layers, etc. It will undoubtedly be an indispensable resource when you’re learning how to work with neural networks in Python!\u003c/p\u003e\n",
                  "contentUrl": "https://www.datacamp.com/community/tutorials/cnn-tensorflow-python",
                  "userContentUrl": null,
                  "illustrationUrl": null,
                  "seoTitle": "(Tutorial) Convolutional Neural Networks with TensorFlow",
                  "seoMetaDescription": "In this tutorial, you'll learn how to construct and implement Convolutional Neural Networks (CNNs) in Python with the TensorFlow framework.",
                  "seoKeyword": "python, tensorflow, cnn, neural network",
                  "mustRead": true,
                  "programmingLanguage": null,
                  "submissionDate": "2020-06-08T00:33:44.588Z",
                  "publishDate": "2020-06-08T16:00:00.000Z",
                  "episode": null,
                  "isLatest": null,
                  "externalUrl": null,
                  "transcriptUrl": null,
                  "guests": [],
                  "links": null,
                  "isSpam": false,
                  "xp": 0,
                  "flaggingUsers": [],
                  "isDisabled": false,
                  "connectedInternalContentId": null,
                  "createdAt": "2020-06-08T00:33:44.587Z",
                  "updatedAt": "2020-06-08T21:19:45.038Z",
                  "upvoting": {
                    "voteCount": 22,
                    "voted": false
                  },
                  "tags": ["python", "neural networks", "tensorflow"],
                  "author": {
                    "id": 701074,
                    "slug": "adityasharma101993",
                    "avatarUrlSquare": "https://assets.datacamp.com/users/avatars/000/701/074/square/29872933_10210392357919457_2437555204118650647_o.jpg?1544687015",
                    "fullName": "Aditya Sharma",
                    "nameFromEmail": "adityasharma101993",
                    "isAdmin": false
                  },
                  "recommendedArticles": []
                },
                "isFetched": true,
                "isFetching": false,
                "statusMessage": ""
              },
              "form": {},
              "list": {
                "isFetched": false,
                "isFetching": false,
                "statusMessage": ""
              },
              "menu": {
                "isSidebarMenuOpen": false
              },
              "notifications": {
                "isFetched": false,
                "isFetching": false,
                "isReadFetched": false,
                "isReadFetching": false,
                "statusMessage": "",
                "readStatusMessage": "",
                "Notifications": [],
                "NotificationsTotal": 0,
                "unReadCount": 0
              },
              "preview": {
                "isFetching": false,
                "isFetched": false,
                "statusMessage": "",
                "content": {}
              },
              "recommendCS": {
                "isPosting": false,
                "isPosted": false,
                "statusMessage": "",
                "isModalOpen": false,
                "currentStep": "form"
              },
              "spam": {
                "isFlagging": false,
                "isSucceeded": false,
                "statusMessage": "",
                "isSpamModalOpen": false,
                "isUnSpamModalOpen": false
              },
              "tag": {
                "isRequesting": false,
                "isSucceeded": false,
                "statusMessage": "",
                "isDeleteTagModalOpen": false
              },
              "tagList": {
                "isFetched": false,
                "isFetching": false,
                "statusMessage": "",
                "list": [],
                "total": 0
              },
              "tagSearch": {
                "isFetching": false,
                "isFetched": false,
                "statusMessage": "",
                "content": {}
              },
              "user": {
                "isFetching": false,
                "isFetched": false,
                "statusMessage": "",
                "unBan": {
                  "isUnBanning": false,
                  "isSucceeded": false,
                  "statusMessage": "",
                  "isUnBanUserModalOpen": false
                },
                "ban": {
                  "isBanning": false,
                  "isSucceeded": false,
                  "statusMessage": "",
                  "isBanUserModalOpen": false
                }
              },
              "submitArticle": {
                "isPosting": false,
                "isPosted": false,
                "statusMessage": "",
                "timer": 0,
                "articleSlug": "",
                "isModalOpen": false,
                "currentStep": "form",
                "slug": "",
                "externalUrl": ""
              },
              "rss": {
                "isCreating": false,
                "isSucceeded": false,
                "statusMessage": ""
              },
              "rssFeedList": {
                "isFetched": false,
                "isFetching": false,
                "statusMessage": "",
                "list": [],
                "disconnectModal": {
                  "isFetched": true,
                  "isFetching": false,
                  "isOpen": false,
                  "rssFeedIdToDisconnect": null,
                  "statusMessage": ""
                }
              },
              "setAsHomePage": {
                "isSetAsHomePageModalOpen": false
              },
              "analytics": {}
            },
            "initialProps": {
              "asPath": "/community/tutorials/cnn-tensorflow-python"
            }
          }
        },
        "page": "/community/tutorial",
        "query": {
          "slug": "cnn-tensorflow-python"
        },
        "buildId": "hfz2jJVqP9kStsLDnKa6g",
        "assetPrefix": "/community",
        "isFallback": false,
        "customServer": true,
        "gip": true
      }
    </script>
    <script
      nomodule=""
      src="../_next/static/chunks/polyfills-ce007b75707b1f3589a8.js"
    ></script>
    <script
      src="../_next/static/chunks/main-f0b5624c3014889b3762.js"
      async=""
    ></script>
    <script
      src="../_next/static/chunks/webpack-8da9cabe58c6d2aa3bdd.js"
      async=""
    ></script>
    <script
      src="../_next/static/chunks/framework.ad7002b3a551c278f5f0.js"
      async=""
    ></script>
    <script
      src="../_next/static/chunks/commons.aeba1b501ee5096455a7.js"
      async=""
    ></script>
    <script
      src="../_next/static/chunks/pages/_app-87c1170f38154f9c4d0d.js"
      async=""
    ></script>
    <script
      src="../_next/static/chunks/ac46388c.70f445eb5557aa9dec72.js"
      async=""
    ></script>
    <script
      src="../_next/static/chunks/68f504b0.bba56c4f391a1db0815a.js"
      async=""
    ></script>
    <script
      src="../_next/static/chunks/78537f27.8279c7505c61cb8fea60.js"
      async=""
    ></script>
    <script
      src="../_next/static/chunks/adf1b103b565b7865a0f98ea967b51b3e3660c18.095266edfcdd9f646efd.js"
      async=""
    ></script>
    <script
      src="../_next/static/chunks/8ffc5858bf71a15f52361e7319c3ad5ceec7c325.576033b2f3878dbe8c10.js"
      async=""
    ></script>
    <script
      src="../_next/static/chunks/1ea862844a52e07ea30f2afd84cc366b3979d6b4.e27afa73a93d521c36e1.js"
      async=""
    ></script>
    <script
      src="../_next/static/chunks/de2bd6607f0d09174f6b5f6ccd214f4f3c8d6fb9.e2051ed48ad4791c2328.js"
      async=""
    ></script>
    <script
      src="../_next/static/chunks/0f6d78b899a1ada8a069577d0a0f2d60f5d91117.4019acfb783e806a75cb.js"
      async=""
    ></script>
    <script
      src="../_next/static/chunks/pages/community/tutorial-ab49573adb58f7e29045.js"
      async=""
    ></script>
    <script
      src="../_next/static/hfz2jJVqP9kStsLDnKa6g/_buildManifest.js"
      async=""
    ></script>
    <script
      src="../_next/static/hfz2jJVqP9kStsLDnKa6g/_ssgManifest.js"
      async=""
    ></script>
    <script type="text/javascript">
      (function () {
        window["__CF$cv$params"] = {
          r: "623ad3c84814e865",
          m: "c893bdea924399f6a35a05a7ad981243c35e757d-1613683595-1800-AYWFnlczJvIsnu6rQ71cCe1mU9wRm/vm7wRMxBSX4gWOFRCPd+CLfrYS2R3aFQwJ46z3PtBbxtBKnb/CGY7J8DgvjtbICrQ4MyR/g2kupkyQAlCQ0GYruYugiquAEoPq0XTVwWSfm6Od0VIWoVZWu41gHkTS/Edf7Cy7Yepy0eV3",
          s: [0x039e207548, 0xe89d623a54],
        };
      })();
    </script>
  </body>
  <!-- Mirrored from www.datacamp.com/community/tutorials/cnn-tensorflow-python by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 18 Feb 2021 21:27:23 GMT -->
</html>
